
<!---- Page 1 ---------------------------------------------------------------------------------------------------------------------------------->
Tell us about your PDF experience.
Azure AI Document Intelligence
documentation
Azure AI Document Intelligence is a cloud-based Azure AI service that uses machine-
learning models to automate your data processing in applications and workflows.
Document Intelligence is essential for enhancing data-driven strategies and enriching
document search capabilities.
About Azure AI Document Intelligence
o
OVERVIEW
What is Azure AI Document Intelligence?
Document Intelligence FAQ
WHAT'S NEW
What's new in Azure AI Document Intelligence?
Document Intelligence models
2
TRAINING
Read
Layout
Financial Services and Legal
US Tax
Personal identification
Custom field extraction
Custom classification
Add-on capabilities
Query field extraction
Document Intelligence Studio


<!---- Page 2 ---------------------------------------------------------------------------------------------------------------------------------->
®
OVERVIEW
Studio overview
Studio concepts
Document Intelligence concepts
{} CONCEPT
NEW
Retrieval-Augmented Generation (RAG)
NEW
Batch document analysis
Accuracy and confidence scores
Use the analyzeDocument response
Custom model overview
Responsible AI
O
REFERENCE
Transparency notes
Characteristics and limitations
Guidance for integration and responsible use
Data privacy, compliance, and security
How-to-guides
HOW-TO GUIDE
Create a Document Intelligence resource
Create SAS tokens for storage containers
Create and use managed identities
Build a custom extraction model
Build a custom classification model


<!---- Page 3 ---------------------------------------------------------------------------------------------------------------------------------->
Compose custom models
Install and run Document Intelligence containers


<!---- Page 4 ---------------------------------------------------------------------------------------------------------------------------------->
What is Azure AI Document Intelligence?
Article · 02/06/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Azure AI Document Intelligence is a cloud-based Azure AI service that enables you to build intelligent document processing solutions.
Massive amounts of data, spanning a wide variety of data types, are stored in forms and documents. Document Intelligence enables you to
effectively manage the velocity at which data is collected and processed and is key to improved operations, informed data-driven decisions,
and enlightened innovation.
For information on region access, see Azure AI Services Product Availability by Region .
I
Document analysis models
Prebuilt models
Custom models |
Document analysis models
Document analysis (general extraction) models enable text extraction from forms and documents and return structured business-ready
content ready for your organization's action, use, or development.
Read | Extract printed and handwritten text.
Layout | Extract text, tables, and document structure.
Prebuilt models
Prebuilt models enable you to add intelligent document processing to your apps and flows without having to train and build your own
models.
Financial Services and Legal
Bank Statement | Extract account information and details from bank statements.
Check | Extract relevant information from checks.
Contract | Extract agreement and party details.
Credit card | Extract payment card information.
Invoice | Extract customer and vendor details.
Pay Stub | Extract pay stub details.
Receipt | Extract sales transaction details.
US Tax
Unified US tax | Extract from any US tax forms supported.
US Tax W-2 | Extract taxable compensation details.
US Tax 1098 | Extract 1098 variation details.
US Tax 1099 | Extract 1099 variation details.
US Tax 1040 | Extract 1040 variation details.
US Mortgage
US mortgage 1003 | Extract loan application details.


<!---- Page 5 ---------------------------------------------------------------------------------------------------------------------------------->
US mortgage 1004 | Extract information from appraisal.
US mortgage 1005 | Extract information from validation of employment.
US mortgage 1008 | Extract loan transmittal details.
US mortgage disclosure | Extract final closing loan terms.
Personal Identification
Health Insurance card | Extract insurance coverage details.
Identity | Extract verification details.
Marriage certificate | Extract certified marriage information.
Custom models
Custom models are trained using your labeled datasets to extract distinct data from forms and documents, specific to your use cases.
Standalone custom models can be combined to create composed models.
Document field extraction models
V Document field extraction models are trained to extract labeled fields from documents.
Custom neural | Extract data from mixed-type documents.
Custom template | Extract data from static layouts.
Custom composed | Extract data using a collection of models.
Custom classification models
Custom classifiers identify document types before invoking an extraction model.
Custom classifier | Identify designated document types (classes) before invoking an extraction model.
Add-on capabilities
Document Intelligence supports optional features that can be enabled and disabled depending on the document extraction scenario:
· ocr.highResolution
· ocr.formula
· ocr.font
· ocr.barcode
. Read model support for searchable PDF
· Searchable PDF
· queryFields
· keyValuePairs
Analysis features
" Expand table


<!---- Page 6 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Content
Extraction
Query
fields
Paragraphs
Paragraph
Roles
Selection
Marks
Tables
Key-
Value
Pairs
Languages
Barcodes
Document
Analysis
Formulas*
prebuilt-read
✓
✓
O
O
O
prebuilt-layout
✓
✓
✓
✓
✓
✓
O
O
O
O
prebuilt-contract
✓
✓
✓
✓
✓
O
O
✓
O
prebuilt-
healthInsuranceCard.us
✓
✓
O
O
✓
O
prebuilt-idDocument
✓
✓
O
O
✓
O
prebuilt-invoice
✓
✓
✓
✓
O
O
O
✓
O
prebuilt-receipt
✓
✓
O
O
✓
O
prebuilt-
marriageCertificate.us
✓
✓
✓
O
O
✓
O
prebuilt-creditCard
✓
✓
O
O
✓
O
prebuilt-check.us
✓
✓
O
O
✓
O
prebuilt-payStub.us
✓
✓
O
O
✓
O
prebuilt-bankStatement
✓
✓
O
O
✓
O
prebuilt-mortgage.us.1003
✓
✓
✓
O
O
✓
O
prebuilt-mortgage.us.1004
✓
✓
✓
O
O
✓
O
prebuilt-mortgage.us.1005
✓
✓
✓
O
O
✓
O
prebuilt-mortgage.us.1008
✓
✓
✓
O
O
✓
O
prebuilt-
mortgage.us.closingDisclosure
✓
✓
✓
O
O
✓
O
prebuilt-tax.us
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.w2
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.w4
✓
✓
O
O
✓
O
prebuilt-tax.us.1040 (various)
✓
✓
✓
O
O
✓
O
prebuilt-tax.us. 1095A
✓
✓
O
O
✓
O
prebuilt-tax.us. 1095C
✓
✓
O
O
✓
O
prebuilt-tax.us.1098
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.1098E
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.1098T
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.1099 (various)
✓
✓
✓
O
O
✓
O
prebuilt-tax.us. 1099SSA
✓
✓
O
O
✓
O
{ customModelName }
✓
✓
✓
✓
✓
✓
O
O
✓
O
V - Enabled
O - Optional
* - Premium features incur extra costs
Models and development options
You can use Document Intelligence to automate document processing in applications and workflows, enhance data-driven strategies, and
enrich document search capabilities. Use the links in the table to learn more about each model and browse development options.
Read

| Model ID | Content Extraction | Query fields | Paragraphs | Paragraph Roles | Selection Marks | Tables | Key- Value Pairs | Languages | Barcodes | Document Analysis | Formulas* |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| prebuilt-read | :selected: ✓ | :unselected: | ✓ :selected: | :unselected: |  |  |  | O :unselected: | O :unselected: |  | O :unselected: |
| prebuilt-layout | ✓ :selected: | ✓ :selected: | ✓ :selected: | ✓ :selected: | ✓ :selected: | ✓ :selected: | O :unselected: | O :unselected: | O :unselected: |  | O :unselected: |
| prebuilt-contract | :selected: ✓ | ✓ :selected: | ✓ :selected: | ✓ :selected: | ✓ :selected: |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt- healthInsuranceCard.us | ✓ :selected: | ✓ :selected: |  |  |  |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-idDocument | :selected: ✓ | :selected: ✓ | :unselected: | :unselected: |  |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-invoice | :selected: ✓ | :selected: ✓ |  |  | ✓ :selected: | :selected: ✓ | O :unselected: | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-receipt | :selected: ✓ | :selected: ✓ |  |  |  |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt- marriageCertificate.us | :selected: ✓ | :selected: ✓ |  |  | ✓ :selected: |  |  | O :unselected: | :unselected: O | ✓ :selected: | O :unselected: |
| prebuilt-creditCard | :selected: ✓ | :selected: ✓ |  |  |  |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-check.us | :selected: ✓ | :selected: ✓ |  |  |  |  |  | O :unselected: | O :unselected: | :selected: ✓ | O :unselected: |
| prebuilt-payStub.us | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-bankStatement | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-mortgage.us.1003 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | O :unselected: | :unselected: O | ✓ :selected: | O :unselected: |
| prebuilt-mortgage.us.1004 | :selected: ✓ | ✓ :selected: |  |  | :selected: ✓ |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-mortgage.us.1005 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | O :unselected: |
| prebuilt-mortgage.us.1008 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt- mortgage.us.closingDisclosure | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.w2 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.w4 | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1040 (various) | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us. 1095A | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | O :unselected: | :selected: ✓ | :unselected: O |
| prebuilt-tax.us. 1095C | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1098 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1098E | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1098T | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1099 (various) | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us. 1099SSA | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | ✓ :selected: | :unselected: O |
| { customModelName } | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |


<!---- Page 7 ---------------------------------------------------------------------------------------------------------------------------------->
Analyze
I
V
All pages
Range
£
V
Content
Result
Code
&
JSON V
₡
1
While healthcare is still in the early stages of its Al journey, we
are seeing pharmaceutical and other life sciences organizations
making major investments in Al and related technologies."
Content
While healthcare is still in the early
stages of its Al journey, we are
seeing pharmaceutical and other life
sciences organizations making major
investments in Al and related
technologies." TOM LAWRY |
National Director for Al, Health and
Life Sciences | Microsoft
ed",
"2023-02-21T19:27:23Z",
TOM LAWRY | National Director for AL. Health and Life Sciences | Micronsht
ne": "2023-02-21T19:27:25Z",
As pharmaceutical and other life sciences organizations invest
in and deploy advanced technologies, they are beginning to see
benefits in diverse areas across their organizations. Companies
are looking to incorporate automation and continuing smart
factory investments to reduce costs in drug discovery, research
and development, and manufacturing and supply chain
management. Many life sciences organizations are also choosing
to stay with more virtual approaches in the "new normal" -
particularly in clinical trials and sales and marketing areas.
Tandem was able to create and deploy this innovation by
leveraging the Al and machine learning capabilities of the
intelligent cloud. As Al and other technologies continue to
advance, potential use cases will multiply. "Speed to value is
going to continue to accelerate." said Lawry
022-08-31",
uilt-read",
": "utf16CodeUnit",
e healthcare is still in the early stages of its A
Clinical trial sponsors are continually seeking to make clinical
trials faster and to improve the experience for patients and
physicians. The COVID-19 pandemic has accelerated the
adoption of decentralized clinical trials, with an increase in
trial activities conducted remotely and in participants' homes.
In a Mckinsey survey.' up to 98 percent of patients reported
satisfaction with telemedicine. In the same report, 72 percent of
physicians surveyed reported similar or better experiences with
In addition to enhancing the patient experience.
pharmaceutical and other life sciences companies can
leverage advanced technologies to improve relationships with
providers. For example, COVID-19 is driving changes in the
way companies interact with clinicians. Prior to COVID-19.
75 percent of physicians preferred in-person sales visits from
medtech reps; likewise, 77 percent of physicians preferred in-
person sales visits from pharma reps."
Polygon
257, 54, 826, 56, 826, 167, 257, 166
11
{
Enhancing the patient
and provider experience
12
"pageNumber": 1,
13
"angle": 0,
"width": 915,
14
Since the advent of COVID-19. however, physician
preferences are moving toward virtual visits. Only 53 percent
of physicians now express a preference for in-person visits
from medtech reps and only 40 percent prefer in-person visits
from pharma reps." That puts the onus on pharmaceutical and
life sciences organizations to deliver valuable and engaging
virtual visits to providers.
15
"height": 1190,
"unit": "pixel",
"words": [
16
17
18
{
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-read
. Extract text from documents.
· Digitizing any document.
. Document Intelligence Studio EZ
· Data extraction
· Compliance and auditing.
· REST API
· Processing handwritten notes before translation.
· C# SDK
· Python SDK
· Java SDK
· JavaScript
Return to model types
Layout
Analyze
1
V
All pages
Range
&
V
Content
Result
Code
NEWS TODAY
Latest news and bulletin updates
Role
title
Content
NEWS TODAY Latest news and
bulletin updates
The scoop of the day
The latest updates
"succeeded",
ateTime": "2023-02-21T19:39: 32Z",
Polygon
139, 9, 608, 8, 608, 89, 139, 90
pepe your point. When you dirk Celine
tedDateTime": "2023-02-21T19:39: 34Z",
5
"analyzeResult": {
Hidro ther bert fin your document
6
"apiVersion": "2022-08-31",
7
"modelId": "prebuilt-layout",
8
"stringIndexType": "utf16CodeUnit",
Click: bonnet and then choose the element:
9
"content": "Tuesday, Sep 20, YYYY\nNEWS TODAY Latest new
"pages": [
Themts and sgồm than hưip lượng your
10
Minam Nilusen
The scoop of the day
The liest updates to get you through the day
11
{
12
"pageNumber": 1,
Themen and mayles she lup knep forse
13
"angle": 0,
-
prove your pouz. When you sick Ouint
14
"width": 756,
15
"height": 1066,
16
"unit": "pixel",
cover poet, and text box design the
17
"words": [
0
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-
. Extract text and layout information from
· Document indexing and retrieval by
· Document Intelligence
layout
documents.
structure.
Studio
Data extraction
. Financial and medical report analysis.
. REST API
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
Invoice

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt-read | . Extract text from documents. | · Digitizing any document. | . Document Intelligence Studio EZ |
|  | · Data extraction | · Compliance and auditing. | · REST API |
|  |  | · Processing handwritten notes before translation. | · C# SDK · Python SDK · Java SDK · JavaScript |

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- | . Extract text and layout information from | · Document indexing and retrieval by | · Document Intelligence |
| layout | documents. | structure. | Studio |
|  | Data extraction | . Financial and medical report analysis. | . REST API · C# SDK · Python SDK · Java SDK . JavaScript |


<!---- Page 8 ---------------------------------------------------------------------------------------------------------------------------------->
Analyze
I
V
All pages
Range
8
Fields
Content
Result
Code
Prebuilt invoice
Key-Value pairs
INVOICE:
INV-100
4
92.40%
INVOICE DATE:
90.80%
11/15/2019
DUE DATE:
90.70%
12/15/2019
CUSTOMER NAME:
88.90%
MICROSOFT CORPORATION
SERVICE PERIOD:
87.40%
10/14/2019 - 11/14/2019
CUSTOMER ID:
90.70%
CID-12345
0
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-invoice
· Extract key information from invoices.
. Data and field extraction
· Accounts payable processing.
. Automated tax recording and reporting.
· Document Intelligence Studiolz
· REST API
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
Receipt
Analyze
I
V
All pages
Range
8
V
Fields
Result
Code
Contoso
DocType: receipt.hotel
Contoso inn
0000 14001 AVe NE
Redmond, WA 98052
Tolaphone
ArrivalDate
#1
99.40%
2021-03-27
Alex Morgen
Room: 515
5600 148th Ave NE
Redmond, WA 98052
Room Type: STQT
Number of Guests: 1
Rate. $127.00
Currency
99.50%
Clurk. MVD
USD
Conluso
Active
Time. 05.02PM
Depart
Time. 02.52PM
Folio Number: 12345
DepartureDate
#1
99.30%
DATE
DESCRIPTION
CHARGES
CREDITS
2021-03-28
LUCIE
Items (6)
#1
-
-
MerchantAddress
#1
98.70%
5600 148th Ave NE, Redmond, WA 98052
Total
104.92
HouseNumber
5600
Balance
0.00
USD
0
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-
. Extract key information from receipts.
· Expense management.
. Document Intelligence
receipt
. Data and field extraction
· Consumer behavior data analysis.
Studio
. Receipt model v3.0 supports processing of single-page hotel
. Customer loyalty program.
· REST API
receipts.
· Merchandise return processing.
· C# SDK
· Automated tax recording and
· Python SDK
reporting.
· Java SDK
. JavaScript
CONTOSO LTD.
INVOICE
Contoso Headquarters
123 456" 50
New York, NY, 10001
CvK vais
HVORCE-DATE 11/15/2017
2 12 FAFE7/45
CREATEFER IN MIND PAES
Microsoft Corp
123 Other St.
Redmond WA, 98052
CHE-
WILTOSONL FMENKLE
23 Bill St,
edmond WA 9805
........
MECTOSOIT DETIVRY
123 Ship St,
Redmond WA. 9805
RE: INNAN
Microsoft Services
23 Service St,
edmond WA. 98.05
5100 00
MINAWER
30000
CHIPS
510000
590100
561000
THANK YOU FOR YOUR BUSINESS!
KEMIT TO
Contoso Billing
123 Remit St
New York, NY. 10001
SALESPERSON
CO- VERV HTE
REQUISITIONER
SHIPPED VIA
F.O.B. POINT
TERMS
DERECE
E
DATE
ITEM CODE
DESCRIPTION
QTY
UM
PRICE
TAX
AMOUNT
3/4/2021
A123
Consulting Services
2
hours
$30.00
$6.00
$60.00
3/5/2021
B456
Document Fee
3
$10.00
$3.00
$30.00
3/6/2021
C789
Printing Fee
10
pages
$1.00
$1.00
$10.00

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt-invoice | · Extract key information from invoices. . Data and field extraction | · Accounts payable processing. . Automated tax recording and reporting. | · Document Intelligence Studiolz · REST API · C# SDK · Python SDK · Java SDK . JavaScript |

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- | . Extract key information from receipts. | · Expense management. | . Document Intelligence |
| receipt | . Data and field extraction | · Consumer behavior data analysis. | Studio |
|  | . Receipt model v3.0 supports processing of single-page hotel | . Customer loyalty program. | · REST API |
|  | receipts. | · Merchandise return processing. | · C# SDK |
|  |  | · Automated tax recording and | · Python SDK |
|  |  | reporting. | · Java SDK |
|  |  |  | . JavaScript |

|  | SALESPERSON | CO- VERV HTE | REQUISITIONER | SHIPPED VIA | F.O.B. POINT | TERMS |
| --- | --- | --- | --- | --- | --- | --- |
|  |  | DERECE |  |  |  |  |
|  |  |  |  |  |  |  |

| E | DATE | ITEM CODE | DESCRIPTION | QTY | UM | PRICE | TAX | AMOUNT |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | 3/4/2021 | A123 | Consulting Services | 2 | hours | $30.00 | $6.00 | $60.00 |
|  | 3/5/2021 | B456 | Document Fee | 3 |  | $10.00 | $3.00 | $30.00 |
|  | 3/6/2021 | C789 | Printing Fee | 10 | pages | $1.00 | $1.00 | $10.00 |


<!---- Page 9 ---------------------------------------------------------------------------------------------------------------------------------->
Return to model types
Identity (ID)
Analyze
I
V
All pages
Range
8
Fields
Result
Code
DocType: idDocument.passport
MALAYSIA
Pasport /
Passport
Jenis / Type
B
Kod Negara / Country Code
MYS
No. Pasport / Passport No.
R00000000
Nama / Name
AFIFAI HAIRUDDIN
Warganegara / Nationality
MALAYSIA
No. Pengenalan / Identity No.
751102076384
Tarikh Lahir / Date of Birth
02 NOV 1975
Tempat Lahir / Place of Birth
SELANGOR
Jantina / Sex
P-F
Tinggi / Height
160 cm
Tarikh Dikeluarkan / Date of Issue
11 MAR 2017
Tarikh Tamat / Date of Expiry
10 MAR 2022
Pejabat Pengeluar / Issuing Office
KUALA LUMPUR
Pekerjaan / Profession
GOVERNMENT
OFFICER
P<MYSHAIRUDDIN << AFIFAH <<<<<<<<<<<<<<<<<<<<<<
R000000009MYS7511024F2203104751102076384 << 04
CountryRegion
#1
99.00%
MYS
DateOfBirth
#1
99.00%
1975-11-02
DateOfExpiration
#1
2022-03-10
99.00%
DateOfIssue
#1
99.00%
2017-03-11
DocumentNumber
#1
99.00%
R00000000
DocumentType
#1
99.00%
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-
. Extract key information from passports and ID cards.
· Know your customer (KYC) financial services
· Document Intelligence
idDocument
. Document types
guidelines compliance.
Studio [
. Extract endorsements, restrictions, and vehicle
. Medical account management.
. REST API
classifications from US driver's licenses.
. Identity checkpoints and gateways.
· Hotel registration.
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
Check
Run analysis
Analyze options
Fields
Result
Code
Drag & drop file
here or
Browse for files
or
Fetch from URL
PayerAddress
#1
123 Main St, Redmond, WA 98052
99.50%
HouseNumber
123
Contoso Bank
Road
Contoso Ltd.
123 Main St, Redmond, WA 98052
No. 370654
98-2
125
Main St
Date: June 20, 2024
PostalCode
Pay
To The
Order Of
22 Century Insurance
John Doe
98052
check_0731.pdf
$
*** 123,456.00
Citv
Sample
NumberAmount
One Hundred Twenty-Three Thousand Four Hundred Fifty-Six And 00/100 Dollars
annywalke
Content
$ 123,456.00
1
Memo: Fees & Charges
Value
123456
Confidence
99.50%
Authorized Signature
StreetAddress
check.pdf
123 Main St
370654⑈
8125000024⑆
84950⑉55432⑈
PayerName
#1
59.40%
Contoso Ltd.
PayTo
#1
99.50%
22nd Century Insurance John Doe
WordAmount
₦1
99.50%
<
1
of 1 >
Q Q @ Q
123456
[ Expand table

| :selected: CountryRegion #1 | 99.00% |
| --- | --- |
| MYS |  |
| :selected: DateOfBirth #1 | 99.00% |
| 1975-11-02 :unselected: |  |
| :selected: DateOfExpiration #1 2022-03-10 :unselected: | 99.00% |
|  |  |
| :selected: DateOfIssue #1 | 99.00% |
| 2017-03-11 |  |
| :selected: DocumentNumber #1 | 99.00% |
| R00000000 |  |
| :selected: DocumentType #1 | 99.00% |

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- | . Extract key information from passports and ID cards. | · Know your customer (KYC) financial services | · Document Intelligence |
| idDocument | . Document types | guidelines compliance. | Studio [ |
|  | . Extract endorsements, restrictions, and vehicle | . Medical account management. | . REST API |
|  | classifications from US driver's licenses. | . Identity checkpoints and gateways. · Hotel registration. | · C# SDK |
|  |  |  | · Python SDK · Java SDK . JavaScript |


<!---- Page 10 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Description
Automation use cases
Development options
prebuilt-check
· Extract key information from checks.
. Data and field extraction
· Credit management.
. Document Intelligence Studio [
· Automated lender management.
· REST API
· C# SDK
· Python SDK
· Java SDK
· JavaScript
Return to model types
Pay stub
EARNINGS
HOURS
RATE
CURRENT
YTD
Regular Hours
56.00
24.00
$
1,344.00
$
5,000.00
Overtime
2.00
36.00
$
72.00
$
500.00
Sick Pay
8.00
24.00
$
192.00
$
500.00
Vacation Pay
8.00
24.00
$
192.00
$
500.00
Holiday Pay
8.00
24.00
$
192.00
$
500.00
82.00
S
1,992.00
$
7,000.00
DEDUCTIONS
RATE
CURRENT
YTD
Aftertax Health Ins
$
120.00
$
500.00
Aftertax Dental Ins
$
35.00
$
400.00
Aftertax 401k
$
78.00
$
200.00
Child Support
$
155.00
$
200.00
Garnishment
$
45.00
$
500.00
S
433.00
$
1,800.00
CONTOSO LTD
EARNINGS STATEMENT
Pay Date
1/12/2024
Pay Start
7/1/2024
Pay End
1/7/2024
Carl Andersor
203 Denver Blvd
Seattle, WA 98040
SSN:
997-60-1247
DOB: 01-09-1997
NET PAY
CURRENT
YTD
S
1,246.61
S
3,785.78
TAXES
RATE
CURRENT
YTD
Federal Income Tax
S
100.00
$
500.00
Social Security
0.0620
$
123.50
$
475.22
Medicare
0.0145
S
28.88
$
199.00
State Income Tax
$
50.00
$
200.00
Local Income Tax
$
10.00
$
40.00
3
I
312
S
S
1,414.22
Direct Deposited to Account of:
Carl Anderson
Routing Number:
485066128
Account Number:
8300567112
Amount:
$
1,246.61
CONTOSO LTD
123 Main Street
Redmond, WA 98052
1/12/2024
1001
CurrentPeriodGrossPay
#1
99.50%
1992
CurrentPeriodNetPay
#1
99.50%
1246.61
CurrentPeriodTaxes
#1
99.50%
312.39
EmployeeAddress
#1
99.50%
203 Denver Blvd Seattle, WA 98040
HouseNumber
203
Road
Denver Blvd
PostalCode
98040
City
Seattle
State
WA
StreetAddress
203 Denver Blvd
" Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-paystub
· Extract key information from pay stubs.
· Data and field extraction
· Employee payroll detail verification.
· Fraud detection for employment.
· Automated tax processing.
. Document Intelligence Studio EZ
· REST API
· C# SDK
· Python SDK
· Java SDK
· JavaScript
Return to model types
Bank statement

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt-check | · Extract key information from checks. . Data and field extraction | · Credit management. | . Document Intelligence Studio [ |
|  |  | · Automated lender management. | · REST API · C# SDK · Python SDK · Java SDK · JavaScript |

| EARNINGS | HOURS | RATE | CURRENT | YTD |
| --- | --- | --- | --- | --- |
| Regular Hours | 56.00 | 24.00 | $ 1,344.00 | $ 5,000.00 |
| Overtime | 2.00 | 36.00 | $ 72.00 | $ 500.00 |
| Sick Pay | 8.00 | 24.00 | $ 192.00 | $ 500.00 |
| Vacation Pay | 8.00 | 24.00 | $ 192.00 | $ 500.00 |
| Holiday Pay | 8.00 | 24.00 | $ 192.00 | $ 500.00 |
|  | 82.00 |  | S 1,992.00 | $ 7,000.00 |

| DEDUCTIONS | RATE | CURRENT | YTD |
| --- | --- | --- | --- |
| Aftertax Health Ins |  | $ 120.00 | $ 500.00 |
| Aftertax Dental Ins |  | $ 35.00 | $ 400.00 |
| Aftertax 401k |  | $ 78.00 | $ 200.00 |
| Child Support |  | $ 155.00 | $ 200.00 |
| Garnishment |  | $ 45.00 | $ 500.00 |
|  |  | S 433.00 | $ 1,800.00 |

| TAXES | RATE | CURRENT | YTD |
| --- | --- | --- | --- |
| Federal Income Tax |  | S 100.00 | $ 500.00 |
| Social Security | 0.0620 | $ 123.50 | $ 475.22 |
| Medicare | 0.0145 | S 28.88 | $ 199.00 |
| State Income Tax |  | $ 50.00 | $ 200.00 |
| Local Income Tax |  | $ 10.00 | $ 40.00 |
|  |  | 3 I 312 S | S 1,414.22 |

| Direct Deposited to Account of: Carl Anderson | Routing Number: 485066128 | Account Number: 8300567112 | Amount: $ 1,246.61 |
| --- | --- | --- | --- |
| CONTOSO LTD 123 Main Street Redmond, WA 98052 |  | 1/12/2024 | 1001 |
|  |  |  |  |

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt-paystub | · Extract key information from pay stubs. · Data and field extraction | · Employee payroll detail verification. · Fraud detection for employment. · Automated tax processing. | . Document Intelligence Studio EZ · REST API · C# SDK · Python SDK · Java SDK · JavaScript :selected: :selected: :selected: :selected: :selected: :selected: |


<!---- Page 11 ---------------------------------------------------------------------------------------------------------------------------------->
CONTOSO BANK
DocType: bankStatement.us.layout
Contoso Bank
1513 Grove Street
Nowy York NY 1001
Account Number
168552031585
AccountHolderAddress
#1
99.50%
2096 Godfrey Road New York, NY 10021
Lela R. Duvall
2096 Godfrey Road
New York, NY 10021
Account Type
Simple Checking
Statement Period
Nov 1, 2023
HouseNumber
2096
Nov 30, 2023
Road
Godfrey Road
Account Summary
PostalCode
10021
City
New York
State
NY
Withdrawals / Debits
StreetAddress
2096 Godfrey Road
Transactions
Content
11/02/2023 Online Payment -200.00
IderName
#1
99.50%
Service Fees
Date
Description
Amount
Lela R. Duvall
11/02/2023
International Transaction Fee
5110.00
Total Service Fees
$ 10.00
AccountNumber
#1
99.50%
Page 1 of 1
168552031585
Classified as Microsoft Confidential
AccountType
#1
99.50%
Beginning Balance on Nov 1, 2023
$ 3.000.00
Deposits / Credits
+ 2,500.00
Withdrawals / Debits
- 1,200.00
Service Fees
- 10.00
Ending Balance on Nov 30, 2023
$ 4,290.00
Deposits / Credits
Date
Description
Amount
11/01/2023
Deposi
2.000.00
11/29/2023
Deposi
500.00
Total Deposits / Credits
$ 2,500.00
Date
Description
Amount
1/12/2026
Online Payment
20001
1111/18/2023
Online Transfer To xxxxxxxx1379
1,000.00
Total Withdrawals / Debits
-$ 1,200.00
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-bankStatement
. Extract key information from bank statements.
· Data and field extraction
· Tax Processing use cases.
. Document Intelligence Studio
. REST API
· C# SDK
. Automated accounting management.
· Credit-debit management.
· Loan documentation processing.
· Python SDK
. Java SDK
. JavaScript
Return to model types
Health insurance card
m Run analysis
I
Analyze options
V
REMERA
BLUE CROSS
Microsoft
Member
Medical Network HERITAGE
ANGEL BROWN
Premera Dental
YES
YES
Prefix Identification # Suffix
Premera Vision
ABC 123456789
01
Group # 1000000
HEALTH SAVINGS PLAN
Rx Group # BCAAXYZ
Shared In and Out of Network
Rx BIN# 987654
Deductible
$1.500
Coinsurance Max
$1,000
BCBS 456
Note: Rx and Medical Cost-Shares are Shared
Rx
0
PPC
<
1
of 1
>
2
¥
Fields
Result
Code
Copays (2)
#1
E
1
Amount
$1,500
Benefit
Deductible
2
Amount
$1,000
Benefit
Coinsurance Max
GroupNumber
#1
99.50%
1000000
IdNumber
#1
E
A
Number
99.50%
123456789
Prefix
99.50%
ABC
Insurer
#1
99.50%
PREMERA BLUE CROSS
0
Expand table

| Beginning Balance on Nov 1, 2023 | $ 3.000.00 |
| --- | --- |
| Deposits / Credits | + 2,500.00 |
| Withdrawals / Debits | - 1,200.00 |
| Service Fees | - 10.00 |
| Ending Balance on Nov 30, 2023 | $ 4,290.00 |

| Deposits / Credits |  |  |
| --- | --- | --- |
| Date | Description | Amount |
| 11/01/2023 | Deposi | 2.000.00 |
| 11/29/2023 | Deposi | 500.00 |
| Total Deposits / Credits |  | $ 2,500.00 |

| Date | Description | Amount |
| --- | --- | --- |
| 1/12/2026 | Online Payment | 20001 |
| 1111/18/2023 | Online Transfer To xxxxxxxx1379 | 1,000.00 |
| Total Withdrawals / Debits |  | -$ 1,200.00 |

| Model ID :unselected: | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt-bankStatement | . Extract key information from bank statements. · Data and field extraction | · Tax Processing use cases. | . Document Intelligence Studio . REST API · C# SDK |
|  |  | . Automated accounting management. · Credit-debit management. |  |
|  |  | · Loan documentation processing. | · Python SDK . Java SDK . JavaScript |

| Fields | Result | Code |  |
| --- | --- | --- | --- |
| :selected: Copays (2) #1 |  | E |  |
| 1 Amount :unselected: |  |  |  |
| :unselected: $1,500 |  |  |  |
| :unselected: Benefit |  |  |  |
| :unselected: | Deductible |  |  |
| 2 Amount :unselected: |  |  |  |
| :unselected: $1,000 |  |  |  |
| :unselected: Benefit |  |  |  |
| :unselected: | Coinsurance Max |  |  |
| :selected: | GroupNumber | #1 | 99.50% |
| :unselected: 1000000 |  |  |  |
| :selected: IdNumber | #1 | E | A :selected: |
| :unselected: Number |  | 99.50% |  |
| :unselected: 123456789 |  | :unselected: |  |
| :unselected: Prefix |  | :unselected: 99.50% |  |
| :unselected: ABC |  |  |  |
| :selected: Insurer | #1 | 99.50% |  |
| :unselected: PREMERA BLUE CROSS |  |  |  |


<!---- Page 12 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Description
Automation use cases
Development options
prebuilt-
. Extract key information from US health insurance
. Coverage and eligibility
. Document Intelligence
healthInsuranceCard.us
cards.
verification.
Studio z
. Data and field extraction
· Predictive modeling.
· REST API
· Value-based analytics.
· C# SDK
· Python SDK
· Java SDK
· JavaScript
Return to model types
Contract model
Analyze
V
All pages
Range
V
Fields
Content
Result
Code
DocType: contract
EffectiveDate
#1
99.99%
22: 2184412416 61422412/1
This well Hosting Agreement is entered as of this KEDETT DU LUNDICHAUME
15 day of October, 2022
ExecutionDate
#1
99.99%
15 day of October, 2022
This agreement shall vold and nulty any and all previous agreements to this
date between Contoso and AdventureWorks.
There shall be no additional fees of any kind paid to Contoio, other than
those stated within this agreement for software usage and/or bandwidth usage.
AdventureWorks agrees to pay Contoso 50.01 (one cent) per access up to
800.000 accesses thereafter payment shall be so bos jone-half cent) per access.
AdventureWorks shall send this amount to Contono by no later than
Wednesday for accesses uted from the previous week (Monday thru Sunday).
Jurisdictions #1
0
Clause
100.00%
0
Contoso must provide a person(s) to correct any technical problems [Server
being down or inaccessible) 24 hours per day. 7 days per week. This personts!
must be available by beeper or te pollen
same 24 hour service at the broadcast location.
This Agreement shall be governed by and construed in
accordance with the internal laws of the State of Washington
applicable to agreements made and to be performed entirely
within such state.
0
L
0
This Agreement shall be governed by and construed in accordance wiim ine iridemal
aws of the State of Erparoin applicable to agreements made and to be performed
atively within such szare
Region
100.00%
All parties have read and fully agree to all terms and conditions an set forth
in this Web Hosting Agreement
Washington
Contase Corporation
Title: CTD
Tite: LTD
AMON
Brown
Adventure Works Cyle
ar Axon Smith
Tele: CTO
Aaron Smith
Parties (2)
#1
V
Title
#1
99.98%
WEB HOSTING AGREEMENT
0
Expand table
Model ID
Description
Development options
prebuilt-contract
Extract contract agreement and party details.
. Data and field extraction
. Document Intelligence Studio ['
. REST API
· REST API
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
Credit card model

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- | . Extract key information from US health insurance | . Coverage and eligibility | . Document Intelligence |
| healthInsuranceCard.us | cards. | verification. | Studio z |
|  | . Data and field extraction | · Predictive modeling. | · REST API |
|  |  | · Value-based analytics. | · C# SDK |
|  |  |  | · Python SDK |
|  |  |  | · Java SDK |
|  |  |  | · JavaScript |

| Model ID | Description | Development options |
| --- | --- | --- |
| prebuilt-contract | Extract contract agreement and party details. . Data and field extraction | . Document Intelligence Studio [' |
|  |  | . REST API · REST API · C# SDK · Python SDK · Java SDK |
|  |  | . JavaScript |


<!---- Page 13 ---------------------------------------------------------------------------------------------------------------------------------->
in[ Run analysis
Query fields
Analyze options
V
Fields
Result
Code
DocType: creditCard
CardHolderName
#1
99.50%
ADAM SMITH
Contoso Bank:
CardNumber
#1
99.50%
›))
5412 1234 5656 8888
5412 1234 5656 8888
CardVerificationValue
123
#1
99.50%
VALID
01/28
THRU
ADAM SMITH
mastercard
CustomerServicePhoneNumbers (2)
#1
=
A
1 +1 200-345-6789
2 +1 200-000-8888
For customer servies, cal
1 200-345-678
1 200-000-888
ExpirationDate
#1
99.50%
01/28
IssuingBank
#1
99.50%
123
Contoso Bank
NOT VALID UNLESS SIGNED
Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh
euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.
PaymentNetwork
#1
99.10%
mastercard
0
Expand table
Model ID
Description
Development options
prebuilt-creditCard
Extract contract agreement and party details.
. Data and field extraction
· Document Intelligence Studio 2
· REST API
· REST API
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
Marriage certificate model
Run analysis
Query fields
Analyze options
STATE OF MICHIGAN
CERTIFICATION OF VITAL RECORD
V
Fields
Result
Code
COUNTY OF KENT
STATE OF MICHIGAN
A
Except for signature, spaces left
blank must be completed by typewriter
DocType: marriageCertificate.us
Marriage License
State of Michigan
24245482
STATE FILE NO
91517
IssueDate
#1
99.50%
LOCAL FILE NO
or printed legibly
All of the following appears as of record
and Recorded in Liber 8 of RECORD OF MARRIAGE on page 53 Record Number: 311
March 22, 2017
Luke Eli Harrison
between
and
Myla Kate Calderon
MarriageDate #1
98.60%
FULL NAME OF MALE (FIRST, MIDDLE, LAST]
FULL NAME OF FEMALE (FIRST, MIDDLE, LAST)
LAST NAME BEFORE FIRST MARRIED, IF DIFFERENT
NIKE GN MICHIG
NAME OF MAGISTRATE war service against Germany (PEOR PRINT)
1. MARY HOLLENRAKE, CLERK OF KENT COUNTY DO HEREBY CERTIFY that the foregoing is a true and exact
copy of the original document on file in the office of the County Clerk.
19th March AD. 2017
Many Hollande
MART HOLLINRAKE
COUNTY CLERK
COURT OF KENT COUNTY
MarriagePlace
#1
DATED
March 22, 2017
99.50%
Detroit Wayne MICHIGAN,
[] Expand table
Model ID
Description
Development options
prebuilt-marriageCertificate.us
Extract contract agreement and party details.
. Data and field extraction
· Document Intelligence Studiolz
· REST API
· REST API
· C# SDK

| in[ Run analysis | Query fields | Analyze options | V | Fields | Result | Code |
| --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  | DocType: creditCard |  |  |
|  |  |  |  | :selected: CardHolderName |  | #1 99.50% |
|  |  |  |  | ADAM :unselected: | SMITH |  |
| Contoso Bank: |  |  |  | CardNumber #1 :selected: |  | 99.50% |
|  |  | ›)) |  | 5412 1234 5656 :unselected: |  | 8888 |
| 5412 | 1234 5656 8888 |  |  | :selected: CardVerificationValue 123 :unselected: |  | #1 99.50% |
| VALID 01/28 THRU ADAM SMITH |  | mastercard |  | :selected: |  | CustomerServicePhoneNumbers (2) #1 = A |
|  |  |  |  | 1 +1 200-345-6789 :unselected: |  |  |
|  |  |  |  | 2 +1 :unselected: | 200-000-8888 |  |
| For customer servies, cal 1 | 200-345-678 1 | 200-000-888 |  | :selected: | ExpirationDate #1 | 99.50% |
|  |  |  |  | 01/28 |  |  |
|  |  |  |  | :selected: IssuingBank #1 |  | 99.50% |
|  |  |  |  | Contoso | Bank |  |
| NOT VALID UNLESS SIGNED Lorem ipsum dolor sit amet, consectetuer adipiscing elit, euismod tincidunt ut laoreet dolore magna aliquam erat |  | 123 |  | :selected: PaymentNetwork #1 99.10% mastercard |  |  |
|  |  | sed diam nonummy nibh |  |  |  |  |
|  |  | volutpat. |  |  |  |  |
|  |  |  |  |  |  |  |

| Model ID | Description | Development options |
| --- | --- | --- |
| prebuilt-creditCard | Extract contract agreement and party details. . Data and field extraction | · Document Intelligence Studio 2 |
|  |  | · REST API |
|  |  | · REST API |
|  |  | · C# SDK |
|  |  | · Python SDK |
|  |  | · Java SDK |
|  |  | . JavaScript |

| Model ID | Description | Development options |
| --- | --- | --- |
| prebuilt-marriageCertificate.us | Extract contract agreement and party details. . Data and field extraction | · Document Intelligence Studiolz · REST API · REST API · C# SDK |


<!---- Page 14 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Description
Development options
· Python SDK
· Java SDK
. JavaScript
US mortgage 1003 form
Ti Run analysis
Query fields
Analyze options
.
To be completed by the Lender
Lender Loan No./Universal Loan Identifier
SE5FPEMA22H4/8-408:4123/7-281
Agency Case No
Uniform Residential Loan Application
Verify and complete the information on this application. If you are applying for this loan with others, each additional Borrower must provide
information as directed by your Lender.
Section 1: Borrower Information. This section asks about your personal information and your income from
employment and other sources, such as retirement, that you want considered to qualify for this loan.
1a. Personal Information
Name (First, Middle, Lost, Suffix)
Social Security Number
557 - 99 - 7283
Gwen Stacy
Alternate Names - List any names by which you are known or any names
under which credit was previously received (First, Middle, Last, Suffix)
(or Individual Taxpayer identification Number)
Date of Birth
Citizenship
OUS Citizen
(mm/dd/yyyy)
04 7 07 7 1080
·
Permanent Resident Alier
.
Non-Permanent Resident Alier
Type of Credit
ar applying fo individual credit
List Name(s) of Other Borrower(s) Applying for this Loan
(First, Middle, Lost, Suffix) - Use a separator between names
Carmen Learnsi
O
arapplying fo joint credit. Total Number of Borrowers:
Each Borrower intends to apply for joint credit. Your initials:
Marital Status
O
Marner
Dependents (not listed by another Borrower)
Number 2
Ages 10, 11
Contact Information
.
O
Unmarried
Home Phone
409 746 -
Separatec
Cell Phone
( 831 ) 728 - 4762
(Single, Divorced, Widowed, Civil Union, Domestic Partnership, Registered
Reciprocal Beneficiary Relationship)
Work Phone
)
Ext.
Email gwen19890407@outlook.com
Current Address
Stre
1634 W Glenoaks Blvd
Unit
City Glendale
State CA
ZIP 91201
Country United States
How Long at Current Address?
Years
Months Housing
Ne primary
housing expense
O OWI
Ren IS
/month)
If at Current Address for LESS than 2 years, list Former Address
Street 3533 Rosa View
Does not apply
Unit #
5
City Richmondport
State CA
ZIP94804
Country United States
How Long at Former Address?
5 Years
6 Months Housing
No primary housing expense
Own
Rent ($
1,600.00
/month)
Mailing Address - if different from Current Address
Street
Does not apply
Unit #
City
State
ZIP
Country
Fields
Result
Code
DocType: mortgage.us.1003
AgencyCaseNumber
#1
99.50%
115894
Borrower
#1
A
BirthDate
04 / 07 /1989
99.50%
CellPhoneNumber
90.20%
( 831 ) 728 - 4766
NumberOfBorrowers
99.50%
2
CurrentAddress
99.50%
1634 W Glenoaks Blvd Glendale CA 91201 United States
HouseNumber
1634
Road
W Glenoaks Blvd
0
0
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-
mortgage.us.1003
. Extract key information from 1003 loan
. Fannie Mae and Freddie Mac documentation
. Document Intelligence
applications.
Data and field extraction
requirements.
Studio z
. REST API
. C# SDK
· Python SDK
· Java SDK
· JavaScript
Return to model types
US mortgage 1004 form

| Model ID Description | Development options |
| --- | --- |
|  | · Python SDK · Java SDK . JavaScript |
|  |  |

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- mortgage.us.1003 | . Extract key information from 1003 loan | . Fannie Mae and Freddie Mac documentation | . Document Intelligence |
|  | applications. Data and field extraction | requirements. | Studio z . REST API . C# SDK · Python SDK · Java SDK |
|  |  |  | · JavaScript |


<!---- Page 15 ---------------------------------------------------------------------------------------------------------------------------------->
Uniform Residential Appraisal Report
File # 4LTARZ221
DocType: mortgage.us.1004
The purpose of this summary appramal sport is to provide the lender/csent with an accurate, and adoquilily supported, opinion of the market value of the subject property
County San diego
Nghborhood Name ST. Andrew"s Luthran Church
RE Tamme
Mxp Reference 472300
Census Tract 0174
Appraiser
#6
Occupant 0 imes == -
Special Aonessmerts $ 952
U
PUD
HOA
Property Rights Appraised D = semin | Firmen | Ed for
Be mbra property cumuriby offered lor kile or tas i been ofered for kile in the twelve months prior to the efectivo dite of this apprais(? [x] Hes m No
inport data source(a) used, ofunng pricm(s), and don(n)
Multiple Listing Service, 5498,605, 06/17/2023
[x] dd I did not analyze the contract for sale for the subject purchase transaction. Exglans the results of the analysis of the contract for suite or why the andlyan wia nok
performed
Contract
#1
INTHACI
The analysis of the contract for sale indicates that the purchase price of 5430. 608 aligon closely with recant conparable saims in the area.
Contract Price
Date of Contrac
Is the property soler the owner of public record
ContractDate
99.50%
[ Ses report the lotil dollar amount and describe the flame to be pokud
Note: Race and the racial composition of the neighborhood are not appraisal factors.
2023-06-17
Neighborhood Characteriwdes
One-Unit How
g Brenda
One-Unit Housing
Present Land Uue %
Location
PRICE
AGE
One-Unit
S
Amins supply
$ (000)
0
Markoting Time
S
1
ContractPrice
99.50%
ocate of within the city tosits of San santos city and enconpassed by the city tosits of all sides
Neighborhood Description
1:300,000 Pred. 15-25
[Other
0
498605
Omugagnis ingrigtige Residents of the neighborhood pratarit consist of siggie-agps individuais and families with school-aged cuigren
Market Conditions (including support for the above conclutiona)
wcmit sales activity fares activity in the sengbortone has been robust with Bowes typical spending tues days on the market compared to surrounding areas.
IsPropertySellerOwnerOfPublicRecord
99.50%
View ara
specie Zoning Classification C+2
Zoning Descrption Neighborhood comercial
Zoning Compliance [X Legal |[1] Legal Nonconforming (Grandfathered Une) |[ No Zoring " Ilegal (describe)
Yes
In the highest and best use of the subject property an improved (or as proposed per plans and specifications] the present use? |X Vie |No ! No describe
Milities Public Other (describe)
A
D
Public Other (describe)
Of sin Improvements-Type
Public Private
LE
Dx
5
Gav
U
.
FMA Arsa
EMA Special Flood Hacard Apus| |
Santary Sower
A
Alkry
O
FEMA Flood Zone SFHA
FFMA Map
FEMA Map Dat
Improvements
#1
make a? (x) Yes No INa describe
Ano there any adverse side conditions or external factors jessemerita, oncrouchmenta, environmental conditions, land uses, ofe )? (x) Yon = No It Yes, describe
Neighboring property encroaches on subject.
General Description
Foundation
Exterior Description
materials/condition
Stone
Interior
materials/condition
Neighborhood
#1
Foundation Walls
Carpet
Exterior Walls
Plaster Frysall
Flashing
Gutters & Downspouts
Buch Floor
Sump Pump
Window Type
Steal
Fiberglass
Towe Built
B
Evidence of
Vinyl
Screens
Storage
X None
PudInfo
#3
Drvoway # of Cars
HWBB | Radur
Amenities
E
way Surface
Drop Star
-
Ofe
ud Electric
Cooling [x] Central Air Conditioning
Fireplace(a) #
x Pabo/Deck
Pool
# of Cars
W of Cam
Floor
-
Carport
Washer Oryer
Other (describe) Coffee Malut
Bathing
Square Foot of Groms Living Area Above Grade
Property includes coffee maker and other appliances.
mished area above prade contains
2
Bedrooms
Reconciliation
#2
Describe the condition of the property (including needed repairs, defencegion, menoustoną, numodeing. ofc ]
C2: Home is in good overall condition, with minor repairs of updates needed to address cosmetic issues. Kitchen and bathroom
lixtares may show slight siges of ape but are functional.
Any there any physical deficiencies or adverse conditions that afect the livability, soundness, or structural integrity of the property?| 2 | if Yon, describe
Piercing ETLes Gaming mint Canape en sanitrilon problemy
SalesComparisonApproach
#2
=
Down the property generaly conform to the neighborhood (funcional utility style, condition, use, construction, etc ? n. Vas ||No ! No describe
#1
Site
Freddie Mac Form 70 March 2005
Page 1 of 6
Fannie Mae Forms 1004 March 2005
<
1
of 7
>
Q
Q
‹Û›
2
Subject
#1
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-
mortgage.us.1004
. Extract key information from
1004 appraisals.
Data and field extraction
· Fannie Mae and Freddie Mac documentation requirements.
. Uniform Residential Appraisal report to help lender/client with
the market value of the subject property.
. Document Intelligence
Studio
. REST API
· C# SDK
· Python SDK
· Java SDK
· JavaScript
Return to model types
US mortgage 1005 form

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- mortgage.us.1004 | . Extract key information from 1004 appraisals. Data and field extraction | · Fannie Mae and Freddie Mac documentation requirements. . Uniform Residential Appraisal report to help lender/client with the market value of the subject property. | . Document Intelligence Studio . REST API · C# SDK · Python SDK · Java SDK · JavaScript |


<!---- Page 16 ---------------------------------------------------------------------------------------------------------------------------------->
4
Fannie Mae
DocType: mortgage.us.1005
Request for Verification of Employment
Privacy Act Notice: This information is to be used by the agency collecting it or its assignees in determining whether you quality as a prospective mongagor under its program.
meseneete mortospor or borrower may be delaved or rejected. The information requested in this form is authorized by The 38, USC, Chapter 37 jr VA); by 12 USC, Section
1701 et. seq. of HUD/FHAr by 42 USC, Section 1482b [ HUD/CPD]; and Tite 42 USC, 1471 et. seq, or 7 USC, 1921 et. seq. (if USDA/FmHA).
Instructions:
Lender - Complete Hors 1 through 7. Have applicant complete tem 8. Forward directly to employer named in tem 1.
pe- Pe cor pele erner Part il or part it as applicable, Complete Part IV and resum directly to lender named in tem 2.
The form is to be transmitted directly to the lender and is not to be transmitted through the applicant or any other party.
ApplicantNameAndAddress
#1
99.50%
Richard Smith 456 Bay Blvd, Sacramento, CA
94203
Part | - Request
1. To (Name and address of employer)
ABC Software Company
123 Main Street, Seattle, WA 9811
Attn- Jane Doe
2. From (Name and address of lender)
John Doe
EmployerNameAndAddress
#1
99.50%
EmployerNameAndAddress
I certify that this verification has been sent directly
ABC Software Company 123 Main Street,
Seattle, WA 98111 Attn: Jane Doe
LenderNameAndAddress
#1
99.50%
John Doe XYZ Mortgage Company 789 Avenue,
New York, NY 10001
PresentEmployment
#1
20. Remarks (If employee was off work for any length of time, please Indicate time period and reason)
Part IV - Authorized Signature . Federal statutes provide severe penaties for any fraud, intentional misrepresentation, or criminal connivance or conspiracy purposed to
influence the issuance of any guaranty or insurance by the VA Secretary, the U.S.D.A., FriHA/PHA Commissioner, or the HUEVOPD Assistant Secretary.
Fannie Mae
Form 1000
July 96
<
1
of 1
>
3. Signature of Lender
John Doe
Content
Value
ABC Software Company 123 Main
Street, Seattle, WA 98111 Attn: Jane
Doe
I have applied for a mortgage loan and staned that
7. Name and Address of Applicant (include employ
Richard Smith
456 Bay Blvd, Sacramento, CA 9420
ABC Software Company 123 Main
Part II - Verification of Present Employer
9. Applicant's Date of Employment
10.
Street, Seattle, WA 98111 Attn: Jane
Doe
MAPASPOTE
12A. Cument Gross Base Pay (Enter Amount ant
S
M
a
Confidence
99.50%
128. Gross Eaming!
15. If paid houry - average hours per week
Type
Year To Date
Past Year 2019
Past Year 2018
Rations
5
Bằng Pay
Thu 2020
$ 150402
: 132507
$
100639
Fight or HAPara
$
16. Date of applicant's next pay increase
03/05/2021
Overtime
2093
$
1698
3007
Clothing
5
Quarters
$
17. Projected amount of next pay increase
5.200
Commissions
764
1254
521
Pro Pay
5
18. Dale of applicant's last pay increase
12/14/2019
Bonus
1636
5.
2233
$ 1261
Overseas or
Combat
5
19. Amount of lent pay increase
2,600
Total
$154,895.00
137,692.00
105,428.00
Vartable Housing
Allowance
$
Part ill - Verification of Previous Employment
21 Dare Hired
23. Salary/Wage at Termination Per (Year) (Month) (Week)
Base
Overtime
Commissions
Bonus
22. Date Terminated
24. Reason for Leaving
25 Position Helt
26. Signature of Employer
Jane Doe
27. Title (Please print or type)
28. Date
Supervisor
11/11/2020
29. Print or type name signed in llom 26
Jane Doe
30. Phone No.
(900) 333-4444
0
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-
mortgage.us.1005
. Extract key information from 1005
validation of employment.
. Data and field extraction
. Fannie Mae and Freddie Mac documentation
requirements.
· Verification of employment document to determine the
qualification as a prospective mortgagor.
. Document Intelligence
Studio
· REST API
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
US mortgage 1008 form
Uniform Underwriting and Transmittal Summary
1. Borrower and Property Information
Borrower Name
Stan Hettnon
Occupancy Status
Primar
Residence
Second Home
nvesimen
Sales Price$
800.000.00
Total # of Borrowers
Property Address
225 Bustleton Pike, Feasterville-Trevose PA 19053
Appraised Value $ 300.000.00
Property
Property Type
Project Classification
Freddie Mac
Fannie Mae
Streamlines Reviev
Established
Ney Proiec
Establishes PUD Project
[ [ Ney] PUD | Projec
Property Rights
Fex Simple
Leasehold
Project
DE Limites Revier | Nevi Conde Project
0Ember Rewe ! Established Condi
Project
Condominiun
PUD
2 16 4-un Proiec
: ET Review | Nev | Conde Project
: Ful Review ! Established Condt Proindi
1 Fannit Mar Review through PER: | Conde Projec
Manufacture Housing
Single Wide
Exomo fron ] Roviny
Reciproca Review
Multiwide
0|FHA-approved Conde Project
! Conde Projec Review| Waived
i FU Review | Co-of Project
Fannit Mat Review through PERE I Co-of Project
Project Name
Alpha Park
Fannie Mae Condo Project Manager™ Project ID# (if any)
IL. Mortgage Information
Loan Type
Amortization Type
Loan Purpose
Lien Position
Conventiona
Fixed-Rate-Monthly Payments
Purchase
Firs Mortgage
DEM
Fixed-Rate-Biweekly
Payment:
Cash-Out Refinance
NA
Balloor
Limiter Cash-Ou Refinance [Fannie]
Amount of Subordinate Financing
$
OUSDARI
ARI (type
Othe (specify
ON Cash-Ou Refinance (Freddie]
Hoy Improvement
(If HELOC, include balance and credit limit)
Second Mortgage
Construction Conversion/Constructior fi Permanent
Note Information
Loan Amount $_00.000.00
Mortgage Originator
Seller
Temporary Buydown
Ves
Note Rate
6.5000
Broke
NG
Loan Term (in months)
= Corresponden
Terms
Broken/Correspondent Name and Company Name:
Mesh Nick
. Underwriting Information
Underwriter's Name
Appraiser's Name/License #
Appraisal Company Name
Abdel Keeling
Claudia Denes k / 102398
Padove Appraisal Service
Stable Monthly Income
Proposed Monthly Payment for the Property
First Mortgage P&I
Subordinate Lien (s) P&I
Homeowner's Insurance
Sunlamartal Pwwartu İnci.ranna
Borrower 1
$
12,956.70
$
2,982.29
Borrower 2
$
$
Borrower 3
Rrumuwar A
$
€
Loan-to-Value Ratios
$
166.67
ITV
€
€
<
1
of 1
>
Q
Q
2
DocType: mortgage.us.1008
Borrower
#1
E
Name
99.50%
Stan Hettinger
NumberOfBorrowers
99.50%
1
Mortgage
#1
A
BrokerOrCorrespondentNameAndCompany
Name
99.50%
Mesh Nicki
LoanAmount
99.50%
400000
LoanTerminMonths
99.50%
240
NoteRatePercentage
99.50%
6.5
0
Expand table

| 3. Signature of Lender | John Doe |  | Content Value | ABC Software Company 123 Main Street, Seattle, WA 98111 Attn: Jane Doe |
| --- | --- | --- | --- | --- |
| I have applied for a mortgage loan and staned that |  |  |  |  |
| 7. Name and Address of Applicant (include employ |  |  |  |  |
| Richard Smith 456 Bay Blvd, | Sacramento, CA 9420 |  |  | ABC Software Company 123 Main |
| Part II - Verification | of Present Employer |  |  |  |
| 9. Applicant's Date of | Employment 10. |  |  | Street, Seattle, WA 98111 Attn: Jane Doe |
| MAPASPOTE |  |  |  |  |
| 12A. Cument Gross | Base Pay (Enter | Amount ant |  |  |
| S |  | M a :unselected: :unselected: | Confidence | 99.50% |

| 128. Gross Eaming! |  |  |  |  |  | 15. If paid houry - average hours per week |
| --- | --- | --- | --- | --- | --- | --- |
| Type | Year To Date | Past Year 2019 | Past Year 2018 | Rations | 5 |  |
| Bằng Pay | Thu 2020 $ 150402 | : 132507 | $ 100639 | Fight or HAPara | $ | 16. Date of applicant's next pay increase 03/05/2021 |
| Overtime | 2093 | $ 1698 | 3007 | Clothing | 5 |  |
|  |  |  |  | Quarters | $ | 17. Projected amount of next pay increase 5.200 |
| Commissions | 764 | 1254 | 521 |  |  |  |
|  |  |  |  | Pro Pay | 5 | 18. Dale of applicant's last pay increase 12/14/2019 |
| Bonus | 1636 | 5. 2233 | $ 1261 | Overseas or Combat | 5 |  |
|  |  |  |  |  |  | 19. Amount of lent pay increase 2,600 |
| Total | $154,895.00 | 137,692.00 | 105,428.00 | Vartable Housing Allowance | $ |  |

| Part ill - Verification of Previous Employment |  |  |  |
| --- | --- | --- | --- |
| 21 Dare Hired | 23. Salary/Wage at Termination Per (Year) (Month) (Week) Base Overtime Commissions Bonus |  |  |
| 22. Date Terminated |  |  |  |
| 24. Reason for Leaving |  | 25 Position Helt |  |

| 26. Signature of Employer | 27. Title (Please print or type) | 28. Date |
| --- | --- | --- |
| Jane Doe | Supervisor | 11/11/2020 |
| 29. Print or type name signed in llom 26 Jane Doe | 30. Phone No. (900) 333-4444 |  |

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- mortgage.us.1005 | . Extract key information from 1005 validation of employment. . Data and field extraction | . Fannie Mae and Freddie Mac documentation requirements. · Verification of employment document to determine the qualification as a prospective mortgagor. | . Document Intelligence Studio · REST API · C# SDK · Python SDK · Java SDK . JavaScript |


<!---- Page 17 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Description
Automation use cases
Development options
prebuilt-
mortgage.us.1008
. Extract key information from Uniform Underwriting
and Transmittal Summary.
. Data and field extraction
· Loan underwriting processing using
summary data.
· Document Intelligence
Studio
. REST API
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
US mortgage disclosure form
Run analysis
Query fields
Analyze options
Fields
Result
Code
DocType: mortgage.us.closingDisclosure
Closing Disclosure
This form is a statement of final loan terms and closing costs. Compare this
document with your Loan Estimate.
Closing
#1
E
A
Closing Information
Transaction Information
Loan Information
Date Issued
1/12/202
Borrower
Gwen Stacy
Loan Term
30 year
Closing Date
4/17/2022
1634 W Glenoaks Blvd, Glendale,
CA 91201
Purpose
Purchase
ClosingDate
99.50%
Disbursement Date
92120077
Product
Fixed Rate
Settlement Agent
Bright Title Co
Seller
Bonita Andersor
4/17/2022
File #
64 568]
920 University Blvd, Rexburg
CA 94954
Loan Type
Conventional
· FHA
Property
1634 W Glenoaks Blv
W
Glendale, CA 91201
51.800.000.00
Lender
Skye AU Bank
Loan ID #
572047875
DisbursementDate
99.50%
Sale Price
MIC#
147188500
Loan Terms
Can this amount increase after closing?
4/21/2022
Loan Amount
$1,440,000
NO
FileNumber
99.50%
Interest Rate
7.033%
NO
64-5681
Monthly Principal & Interest
$9,609.39
NO
See Projected Payments below for your
Estimated Total Monthly Payment
IssueDate
99.50%
Does the loan have these features?
4/12/2022
Prepayment Penalty
NO
PropertyAddress
99.50%
Balloon Payment
NO
1634 W Glenoaks Blvd, Glendale, CA 91201
Projected Payments
HouseNumber
Payment Calculation
Years 1-7
Years 8-30
1634
Principal & Interest
$9,609.39
$9,609.39
Road
Mortgage Insurance
$100.00
-
W Glenoaks Blvd
Estimated Escrow
Amount can increase over time
$1,775.00
$1,775.00
Estimated Total
Monthly Payment
$11,484.39
$11,384.39
PostalCode
91201
This estimate includes
In escrow?
Estimated Taxes, Insurance
& Assessments
$2,525.00
Property Taxes
YES
City
Homeowner's Insurance
YES
Amount can increase over time
See page 4 for details
a month
[X] Other: Homeowner's Association Dues
NO
Glendale
See Escrow Account on page 4 for details. You must pay for other property
costs separately.
0
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-
mortgage.us.closing Disclosure
. Extract key information from Uniform Underwriting
and Transmittal Summary.
. Data and field extraction
· Mortgage loan final details
requirements.
. Document Intelligence
Studio
· REST API
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
US Tax W-2 model

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- mortgage.us.1008 | . Extract key information from Uniform Underwriting and Transmittal Summary. . Data and field extraction | · Loan underwriting processing using summary data. | · Document Intelligence Studio . REST API · C# SDK · Python SDK · Java SDK . JavaScript |

| Run analysis |  | Query fields |  | Analyze options |  |  | Fields | Result | Code |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |  | DocType: |  | mortgage.us.closingDisclosure |  |
| Closing Disclosure |  |  |  | This form is a statement of final loan terms and closing costs. Compare this document with your Loan Estimate. |  |  | :selected: |  |  |  |
| Closing Information |  |  | Transaction Information |  | Loan Information |  | Closing | #1 E | A |  |
| Date Issued | 1/12/202 |  | Borrower | Gwen Stacy | Loan Term | 30 year |  |  |  |  |
| Closing Date | 4/17/2022 |  |  | 1634 W Glenoaks Blvd, Glendale, | Purpose | Purchase | :unselected: | ClosingDate | 99.50% |  |
| Disbursement Date | 92120077 |  |  | CA 91201 | Product | Fixed Rate |  |  |  |  |
| Settlement Agent | Bright Title Co |  | Seller | Bonita Andersor |  |  |  |  |  |  |
| File # | 64 568] |  |  | 920 University Blvd, Rexburg | Loan Type | Conventional · FHA | 4/17/2022 :unselected: |  |  |  |
| Property | 1634 W Glenoaks | Blv |  | CA 94954 |  | W |  |  |  |  |
|  | Glendale, CA | 91201 | Lender | Skye AU Bank | Loan ID # | 572047875 |  |  |  |  |
| Sale Price | 51.800.000.00 |  |  |  | MIC# | 147188500 |  | DisbursementDate | 99.50% |  |
| Loan Terms |  |  |  | Can this amount increase | after closing? |  | 4/21/2022 :unselected: |  |  |  |
| Loan Amount |  | $1,440,000 |  | NO |  |  | :unselected: | FileNumber | 99.50% |  |
| Interest Rate |  | 7.033% |  | NO |  |  | 64-5681 |  |  |  |
| Monthly Principal & Interest |  | $9,609.39 |  | NO |  |  |  |  |  |  |
| See Projected Payments below for your Estimated Total Monthly Payment |  |  |  |  |  |  | IssueDate :unselected: |  | 99.50% |  |
|  |  |  |  | Does the loan have these features? |  |  | 4/12/2022 :unselected: |  |  |  |
| Prepayment Penalty |  |  |  | NO |  |  | :unselected: | PropertyAddress | 99.50% |  |
| Balloon Payment |  |  |  | NO |  |  |  |  |  |  |
|  |  |  |  |  |  |  | 1634 | W Glenoaks | Blvd, Glendale, CA 91201 |  |
| Projected Payments |  |  |  |  |  |  |  | HouseNumber |  |  |
| Payment Calculation |  | Years |  | 1-7 | Years 8-30 |  | 1634 |  |  |  |
| Principal & Interest |  |  |  | $9,609.39 | $9,609.39 |  |  |  |  |  |
| Mortgage Insurance |  |  |  | $100.00 | - |  | Road |  |  |  |
| Estimated Escrow Amount can increase over time |  |  |  | $1,775.00 | $1,775.00 |  | W | Glenoaks Blvd |  |  |
| Estimated Total Monthly Payment |  |  |  | $11,484.39 | $11,384.39 |  |  | PostalCode |  |  |
|  |  |  |  | This estimate includes |  | In escrow? | 91201 |  |  |  |
| Estimated Taxes, Insurance |  |  |  | Property Taxes :selected: |  | YES |  |  |  |  |
| & Assessments |  | $2,525.00 |  | Homeowner's Insurance :selected: |  | YES | City |  |  |  |
| Amount can increase over time |  | a month |  | [X] Other: Homeowner's Association | Dues | NO |  |  |  |  |
| See page 4 for details |  |  |  | See Escrow Account on page 4 for costs separately. | details. You must pay for other property |  | Glendale |  |  |  |

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- mortgage.us.closing Disclosure | . Extract key information from Uniform Underwriting and Transmittal Summary. . Data and field extraction | · Mortgage loan final details requirements. | . Document Intelligence Studio · REST API · C# SDK · Python SDK · Java SDK . JavaScript |


<!---- Page 18 ---------------------------------------------------------------------------------------------------------------------------------->
8
Analyze
V
All pages
Range
Copy 2 -- To Be Filed with Employee's Stato
City, or Local Income Tax Return.
OMB NO. 1545-0008
a. Employee's Soc Sec No
Wages, tips, other comp.
Federal income tax withheld
123-45-6789
37160.56
3894.54
3 Social security wages
A Social security tay withheld
b. Employer ID number (EIN)
37160.56
2303.95
98-7654321
S Medicare wages and tips
37160.56
6 Medicare tar withheld
538.83
Employer's name, address and ZIP code
CONTOSO LTD
123 MICROSOFT WAY
REDMOND, WA 98765
d Control Number
000086242
Employee's name, address, and ZIP code
ANGEL BROWN
4567 MAIN STREET
BUFFALO, WA 12345
7 Social security tips
Allocated tips
302.30
874.20
9
10 Dependent care benefits
11 Nonqualified plans
9873.20
653.21
128 Code See inst. for box 12
DI
5939.68
13 Statutory employee
14 Other
12b Code
B
5432.00
X
DISINS
170.85
Retirement plan
X
12c Code
0
876.30
Third-party sick pay
X
12d Code
0
123.30
PA
87654321
37160.56
1135.69
WA
12345678
15 State Employer's state ID number
16 State wages, tips, etc.
9631.20
17 State income tax
1032.30
18 Local wages, tips, etc.
19 Local Income tax
20 Locality name
37160.56
51.00
Cmberland Viy/Mddl
37160.56
594.54
E.Pennsboro/E.Pnns
Fields
Result
Code
AllocatedTips
#1
99.90%
874.2
ControlNumber
#1
99.90%
000086242
DependentCareBenefits
#1
99.90%
9873.2
Employee
#1
E
Address
99.90%
4567 MAIN STREET BUFFALO, WA 12345
HouseNumber
4567
Road
MAIN STREET
PostalCode
12345
0
Expand table
Model ID
Description
Automation use cases
Development options
prebuilt-
. Extract key information from IRS US W2 tax forms (year
. Automated tax document
· Document Intelligence
tax.us.w2
2018-2021).
management.
Studio
●
. Mortgage loan application
· REST API
processing.
· C# SDK
· Python SDK
· Java SDK
· JavaScript
Return to model types
US tax 1098 (and variations) forms
i
Analyze
I
V
All pages
Range
e
V
Fields
Result
Code
8181
VOID
CORRECTED
Address
8181 VOID
RECIPIENT'S/LENDER'S name, street address, city or town, state or
province, country, ZIP or foreign postal code, and telephone no.
OMB No. 1545-1380
Form
1098
(Rev. January 2022)
For calendar year
20
Mortgage
Interest
Statement
HouseNumber
8181
1 Mortgage interest received from payer(s)/borrower(s)
S
Copy A
For
Internal Revenue
Service Center
Road
VOID
RECIPIENT'S/LENDER'S TIN
PAYER'S/BORROWER'S TIN
2 Outstanding mortgage
principal
S
3 Mortgage origination date
StreetAddress
8181 VOID
4 Refund of overpaid
interest
5 Mortgage insurance
premiums
$
File with Form 1096.
PAYER'S/BORROWER'S name
$
MortgageInsurancePremium
#1
99.59%
6 Points paid on purchase of principal residence
Si
7
If address of property securing mortgage is the same
as PAYER'S/BORROWER'S address, check the box, or enter
the address or description in box 8.
For Privacy Act
and Paperwork
Reduction Act
Notice, see the
current General
Instructions for
Certain
Information
Returns.
$
Street address (including apt. no.)
MortgageInterest
#1
99.99%
City or town, state or province, country, and ZIP or foreign postal code
8 Address or description of property securing mortgage (see
instructions)
$
9 Number of properties securing the
mortgage
OutstandingMortgagePrincipal
#1
99.95%
10 Other
$
11 Mortgage
acquisition date
Account number (see instructions)
OverpaidInterestRefund
#1
98.60%
$
Form 1098 (Rev. 1-2022)
Cat. No. 14402K
Do Not Cut or Separate Forms on This Page
www.irs.gov/Form1098
Department of the Treasury - Internal Revenue Service
Do Not Cut or Separate Forms on This Page
PointsPaid
#1
99.98%
Expand table
Model ID
Description
Development options
prebuilt-tax.us.1098{variation}
. Extract key information from 1098-form variations.
. Document Intelligence Studio
. REST API
· C# SDK
· Python SDK
●

| Copy 2 -- To Be Filed with Employee's Stato City, or Local Income Tax Return. |  |  | OMB NO. 1545-0008 |  |
| --- | --- | --- | --- | --- |
| a. Employee's Soc Sec No | Wages, tips, other comp. |  | Federal income tax withheld |  |
| 123-45-6789 |  | 37160.56 |  | 3894.54 |
|  | 3 Social security wages |  | A Social security tay withheld |  |
| b. Employer ID number (EIN) |  | 37160.56 |  | 2303.95 |
| 98-7654321 | S Medicare wages and tips | 37160.56 | 6 Medicare tar withheld | 538.83 |
| Employer's name, address and ZIP code CONTOSO LTD 123 MICROSOFT WAY REDMOND, WA 98765 |  |  |  |  |
|  |  |  |  |  |
|  |  |  |  |  |
| d Control Number |  |  |  |  |
| 000086242 |  |  |  |  |
| Employee's name, address, and ZIP code |  |  |  |  |
| ANGEL BROWN 4567 MAIN STREET BUFFALO, WA 12345 |  |  |  |  |
| 7 Social security tips | Allocated tips 302.30 | 874.20 | 9 |  |
| 10 Dependent care benefits 11 Nonqualified plans 9873.20 |  | 653.21 | 128 Code See inst. for box 12 |  |
|  |  |  | DI | 5939.68 |
| 13 Statutory employee | 14 Other |  | 12b Code |  |
| X :selected: |  |  | B | 5432.00 |
| Retirement plan X | DISINS 170.85 |  | 12c Code 0 |  |
|  |  |  |  | 876.30 |
| Third-party sick pay X |  |  | 12d Code 0 | 123.30 |
| PA 87654321 |  | 37160.56 |  | 1135.69 |
| WA 12345678 15 State Employer's state ID number | 16 State wages, | 9631.20 tips, etc. | 17 State income tax | 1032.30 |
| 18 Local wages, tips, etc. | 19 Local Income tax |  | 20 Locality name |  |
| 37160.56 |  | 51.00 | Cmberland Viy/Mddl |  |
|  | 37160.56 | 594.54 | E.Pennsboro/E.Pnns |  |

| Model ID | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| prebuilt- | . Extract key information from IRS US W2 tax forms (year | . Automated tax document | · Document Intelligence |
| tax.us.w2 | 2018-2021). | management. | Studio |
|  | ● | . Mortgage loan application | · REST API |
|  |  | processing. | · C# SDK · Python SDK |
|  |  |  | · Java SDK |
|  |  |  | · JavaScript |


<!---- Page 19 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Description
Development options
· Java SDK
· JavaScript
Return to model types
US tax 1099 (and variations) forms
Qui Run analysis
Query fields
Analyze options
Fields
Result
Code
Document 1 V
7171
VOID
CORRECTED
DocType: tax.us.1099NEC
PAYER'S name, street address, city or town, state or province, country, ZIP
or foreign postal code, and telephone no.
OMB No. 1545-0116
Form 1099-NEC
(Rev. January 2022)
For calendar year
20
Nonemployee
Compensation
#2
Payer
=
PhoneNumber
7171
85.50%
PAYER'S TIN
RECIPIENT'S TIN
1 Nonemployee compensation
$
Copy A
For Internal Revenue
Service Center
RECIPIENT'S name
2 Payer made direct sales totaling $5,000 or more of
consumer products to recipient for resale
File with Form 1096.
Box2
#2
For Privacy Act and
Paperwork Reduction Act
Notice, see the current
General Instructions for
Certain Information
Returns.
TaxYear
20
99.90%
3
Street address (including apt. no.)
false
4 Federal Income tax withheld
City or town, state or province, country, and ZIP or foreign postal code
$
#2
99.90%
5 State tax withheld
6 State/Payer's state no.
2nd TIN not.
7 State income
$
Account number (see instructions)
$
$
$
Form 1099-NEC (Rev. 1-2022)
Cat. No. 72590N
www.irs.gov/Form1099NEC
Department of the Treasury - Internal Revenue Service
Do Not Cut or Separate Forms on This Page - Do Not Cut or Separate Forms on This Page
+
0
Expand table
Model ID
Description
Development options
prebuilt-tax.us.1099{ variation}
. Extract information from 1099-form variations.
●
· Document Intelligence Studio
· REST API
· C# SDK
· Python SDK
· Java SDK
. JavaScript
Return to model types
US tax 1040 (and variations) forms
Og Run analysis
Query fields
Analyze options
Fields
Result
Code
1040
Department of the Treasury-Internal Revenue Service
U.S. Individual Income Tax Return
2023
OMB No. 1545-0074
IRS Use Only-Do not write or staple in this space.
See separate instructions
For the year Jan. 1-Dec. 31, 2023, or other tax year beginning
January 1st
2023, ending
December 31st
20 23
Your first name and middle initial
Pascale
Your social security number
12 34 5 6 7 8 9
DocType: tax.us.1040.2022
Last name
Weydert
I joint retum, spouse's first name and middle initial
Johr
Last name
Weyder
Spouse's social security number
9 8 76 6 4321
TaxYear
#1
99.80%
Home address (number and street). if you have a P.O. box, see instructions.
Apt. no.
Presidential Election Campaign
Check here if you, or your
spouse if fing jointy, want $3
to go to this lund, Checking a
box below will not change
You
Spouse
123 Fremont ave
54
City, town, or post office. If you have a foreign address, also complete spaces below.
Settle
2023
State
ZIP code
WA
98102
Foreign country name
Foreign province/state/county
Foreign postal code
Filing Status
Check only
one box.
Married filing jointly (even if only one had income)
Married filing separately (MFS)
Single
Head of household (HOH)
Taxpayer
#1
E
Qualifying surviving spouse (OSS)
If you checked the MFS box, enter the name of your spouse. If you checked the HOH or QSS box, enter the child's name if the
qualifying person is a child but not your dependent:
SSN
81.40%
Digital
Assets
At any time during 2023, did you: (a) receive (as a reward, award, or payment for property or services); or (b) sel,
exchange, or otherwise dispose of a digital asset (or a financial interest in a digital asset)? (See instructions.)
Someone can claim:
You as a dependent
Your spouse as a dependent
Yes
No
123456789
Standard
Deduction
Spouse itemizes on a separate return or you were a dual-status alien
LastName
99.90%
Weydert
FirstNameAndInitials
99.90%
Pascale
Income
1a
Total amount from Form(s) W-2, box 1 (see instructions)
1a
H47/ 05-400
DAL MA
Age/Blindness You
Were born before January 2, 1959
Are blind
Spouse:
Was bom before January 2, 1959
s blind
Dependents (see instructions):
(2) Social security
number
(3) Relationship
to you
(4) Check the box If qualifies for (see instructions):
Child tươi credin
Credit for other dependents
If more
(1) First name
Last name
than four
dependents
Elton Wayder
53
Sometructions
see instructions
and check
here
0
Expand table
Model ID
Description
Development options
prebuilt-tax.us.1040{variation }
. Extract information from 1040-form variations.
●
. Document Intelligence Studio
· REST API
· C# SDK

| Model ID | Description | Development options |
| --- | --- | --- |
| prebuilt-tax.us.1099{ variation} | . Extract information from 1099-form variations. ● | · Document Intelligence Studio |
|  |  | · REST API · C# SDK · Python SDK · Java SDK |
|  |  | . JavaScript |

| Age/Blindness You :unselected: :unselected: :unselected: :selected: Were born before January 2, 1959 Are blind Spouse: Was bom before January 2, 1959 s blind |  |  |  |  |  |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Dependents (see instructions): |  |  | (2) Social security number | (3) Relationship to you | (4) Check the box If |  |  | qualifies for (see instructions): |  |  |
| If more | (1) First name | Last name |  |  |  | Child | tươi credin | Credit for | other | dependents |
| than four | Elton Wayder |  | 53 | :selected: |  |  | :selected: |  | :unselected: |  |
| dependents Sometructions |  |  |  |  |  |  | :unselected: |  | :unselected: |  |
| see instructions and check here :unselected: |  |  |  |  |  |  | :unselected: :unselected: |  | :unselected: :unselected: |  |

| Model ID | Description | Development options |
| --- | --- | --- |
| prebuilt-tax.us.1040{variation } | . Extract information from 1040-form variations. ● | . Document Intelligence Studio · REST API · C# SDK |


<!---- Page 20 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Description
Development options
· Python SDK
· Java SDK
· JavaScript
Unified US tax forms
0
Expand table
Model ID
Description
Development options
prebuilt-tax.us
. Extract information from any of the supported US tax forms.
. Document Intelligence Studio [
· REST API
· C# SDK
· Python SDK
· Java SDK
· JavaScript
Custom model overview
Label data
Train
Region
Lò
&
V
+
Contoso, Ltd.
Project Statement
=
Receipt No
1
:
Drag and drop
document here
or
Browse for a file
Dole: 6/20
-
2468
=
Sold To
2
Fabrikam Residences
Crane Delivery [hvedeam
B
Vides Dettery (On Demand)
Q
4
ilve Dettrer
-
9 Dete
-
--
=
ID #
3
1197531
54.800
sales Tax
Tolal
5450
54.350
=
Live Delivery?
4
:
Invoice_1.pdf
unselected
Thank you for your business
E
=
Online Delivery?
5
:
selected
Invoice_2.pdf
=
Video Delivery?
6
,
selected
0
Expand table
About
Description
Automation use cases
Development
options
Custom
model
Extracts information from forms and documents into structured data
based on a model created from a set of representative training
document sets.
Extract distinct data from forms and
documents specific to your business and
use cases.
· Document
Intelligence Studio [
· REST API
· C# SDK
· Java SDK
. JavaScript SDK
· Python SDK
Return to custom model types
Custom neural

| Model ID | Description | Development options |
| --- | --- | --- |
| prebuilt-tax.us | . Extract information from any of the supported US tax forms. | . Document Intelligence Studio [ · REST API · C# SDK · Python SDK · Java SDK |
|  |  | · JavaScript |

| About | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| Custom model | Extracts information from forms and documents into structured data based on a model created from a set of representative training document sets. | Extract distinct data from forms and documents specific to your business and use cases. | · Document Intelligence Studio [ · REST API · C# SDK · Java SDK . JavaScript SDK · Python SDK |


<!---- Page 21 ---------------------------------------------------------------------------------------------------------------------------------->
HOUSE RENTAL AGREEMENT
This House Rental Agreement ("Agreement," "rental agreement," or "lease") is entered
into between Dpay CLO (Landlord) and Exum VETor mo TarTy Byrne ma Gari Camerei
(Tenants). If more than one person is named as Tenant they shall be jointly and severally liable
and responsible under the terms of this Agreement. This lease Agreement involves a residential
house, yard, and related facilities located at [ Bellewird Dr. Saf Lake City fald ES15 (the
"premises"). The date of this Agreement is [january [5] [12]]
1.
Landlord rents to Tenant, unfurnished, the premises on a month to month basis,
terminable by either party at the end of any calendar month on at least 30 days notice to the other
party. Tenant shall be entitled to possession of the premises and rent shall commence on April %}
[IZAT Tenant shall not assign, sublease, or allow anyone other than persons permitted under this
lease to at any time be in possession of any portion of the premises. Landlord will provide five (5)
keys to Tenant; each key fits all outside door locks. Landlord will also provide two keys to the
garage, and one remote garage door opener. All keys and the remote door opener will be returned
to Landlord at the end of the tenancy.
2.
Tenant agrees to pay the stipulated monthly rent in advance on the first day of each
calendar month without offset or reduction. All rent is payable to Landlord at the address of
Landlord,
A
Pone
Road
Ity
4998
or at such other place as Landlord
O
Note
To train a custom neural model, set the buildMode property to neural. For more information, see Training a neural model
[] Expand table
About
Description
Automation use cases
Development
options
Custom
Neural
model
The custom neural model is used to extract labeled data from structured
(surveys, questionnaires), semi-structured (invoices, purchase orders), and
unstructured documents (contracts, letters).
Extract text data, checkboxes, and
tabular fields from structured and
unstructured documents.
Document
Intelligence
Studio
· REST API
· C# SDK
· Java SDK
. JavaScript SDK
· Python SDK
Return to custom model types
Custom template
Form
W-9
(Ruv. October 2018)
Dupaartmunt of the Thuaszary
Intimnal Revenue Service
Request for Taxpayer
Identification Number and Certification
Give Form to the
requester. Do not
send to the IRS.
>> Go to www.irs.gov/FormW9 for instructions and the latest information.
1 Name (as shown on your income tax return). Name is required on this in; do not leave tvs ine blank.
Joh 199:
2 laziness namaidisregarded entity name, it cimarent from above
Arcte: Ing
3 Check appropriate box for federal tax cumsitestion of the person whose name is entered on line 1. Check only one of the
Toiowng seven Boxes.
4 Examptions (podes apply only to
cortas entitkos, not kuividuals: 500
Instructions on page 2):
Individual/sole proprietor or
C Corporation
Corporation
Partemhp
Trust/astana
Print or type.
ungio-member LLC
See Specific Instructions on page 3.
Other (see instructions; }
Exumpit payoo code (if any)
Q
2
Limbad sability company. Enter the tax classification (CC corporation, S.5 corporation, P&Partnersein(>
Note: Check the appropriate box in the line above for the tax classification of the single-momibor owner. Do not check
anemer LLC that is not disregarded tiem the super tering, tadura y sapesse monte LLC is
is disregarded from the owner should check the appropriate box for the tax classification of es ounde.
0
Exemption from FATCA reporting
code (if any)
D
Aob bant rubbed ani& p9 |15)
& A3:3es unber, street, and ige. or sute no.) See Fistructionn.
DX Tes Address
6 Cily. sión, and ZIP 0000
Requester's name and address (optional)
Jamie Doe
Bellevue WWAT200
7 Ust account numberjaj here (optional)
123456789123. 123456789987
Part I
Taxpayer Identification Number (TIN)
Enter your TIN in the appropriate box. The TIN provided must match the name given on line 1 to avoid
backup withholding. For individuals, this is generally your social security number (SSN). However, for a
residonit allen, sole proprietor, or disregarded entity, sou the instructions for Part I later, For other
entities, It is your employer identification number (EIN), If you do not have a number, see How to get a
TIN, later.
Social security number
0
B
®
-
9
G
-
3
0
&
O
Note: If the account is in more than one name, see the instructions for line 1. Also see What Name and
OF
Employer identification number
O
Note
To train a custom template model, set the buildMode property to template. For more information, see Training a template model
[] Expand table
About
Description
Automation use cases
Development options
Custom
Template
model
The custom template model extracts labeled values
and fields from structured and semi-structured
documents.
Extract key data from highly structured documents with
defined visual templates or common visual layouts,
forms.
· Document
Intelligence Studio 2
· REST API
· C# SDK

| About | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| Custom Neural model | The custom neural model is used to extract labeled data from structured (surveys, questionnaires), semi-structured (invoices, purchase orders), and unstructured documents (contracts, letters). | Extract text data, checkboxes, and tabular fields from structured and unstructured documents. | Document Intelligence Studio · REST API · C# SDK · Java SDK . JavaScript SDK · Python SDK |

| About | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| Custom Template model | The custom template model extracts labeled values and fields from structured and semi-structured documents. | Extract key data from highly structured documents with defined visual templates or common visual layouts, forms. | · Document Intelligence Studio 2 · REST API · C# SDK |


<!---- Page 22 ---------------------------------------------------------------------------------------------------------------------------------->
About
Description
Automation use cases
Development options
· Python SDK
· Java SDK
. JavaScript SDK
Return to custom model types
Custom composed
[ Expand table
About
Description
Automation use cases
Development
options
Composed
custom models
A composed model is created by taking a collection of
custom models and assigning them to a single model built
from your form types.
Useful when you train several models and want to
group them to analyze similar form types like
purchase orders.
· Document
Intelligence Studio E
· REST API
· C# SDK
· Java SDK
. JavaScript SDK
· Python SDK
Return to custom model types
Custom classification model
Label data
+
14
Sort
Filter
17/17 labeled
1
00
Drag and drop similar files
here to create a new
document type and assign
labels
Drag and drop
document here
or
Browse for a file
sample1/car-maint ...
sample 1/car-maint ...
sample 1/car-maint ...
sample 1/car-maint ..
sample1/car-maint ..
M
m
car-maint
:
sample1/car-maint/Comme ...
sample1/car-maint/Comme ...
sample1/car-maint/Comme ...
sample1/car-maint/Comme ...
sample1/car-maint/Comme ...
sample1/cc-auth/C ...
sample 1/cc-auth/C.
sample 1/cc-auth/C ...
sample 1/cc-auth/C ...
sample 1/cc-auth/C ...
sample1/cc-auth/C ...
MI
cc-auth
:
sample1/cc-auth/CCAuth-1 ....
sample1/cc-auth/CCAuth-2 ...
sample1/cc-auth/CCAuth-3 ..
sample1/cc-auth/CCAuth-4 .... X
sample1/cc-auth/CCAuth-5. X
sample1/cc-auth/CCAuth-6 .... X
sample1/cc-auth/CCAuth-7 .... X
sample1/cc-auth/C ...
sample1/deed-of-t ...
sample1/deed-of-t ...
sample1/deed-of-t ...
sample1/deed-of-t ...
sample1/deed-of-t.
0
Expand table
About
Description
Automation use cases
Development
options
Composed
classification model
Custom classification models combine layout and language
features to detect, identify, and classify documents within an
input file.
· A loan application packaged containing
application form, payslip, and, bank
statement.
. A collection of scanned invoices.
Document
Intelligence Studio [2
· REST API
Return to custom model types

| About | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
|  |  |  | · Python SDK · Java SDK . JavaScript SDK |

| About | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| Composed custom models | A composed model is created by taking a collection of custom models and assigning them to a single model built from your form types. | Useful when you train several models and want to group them to analyze similar form types like purchase orders. | · Document Intelligence Studio E · REST API · C# SDK · Java SDK . JavaScript SDK · Python SDK |

| About | Description | Automation use cases | Development options |
| --- | --- | --- | --- |
| Composed classification model | Custom classification models combine layout and language features to detect, identify, and classify documents within an input file. | · A loan application packaged containing application form, payslip, and, bank statement. . A collection of scanned invoices. | Document Intelligence Studio [2 · REST API |


<!---- Page 23 ---------------------------------------------------------------------------------------------------------------------------------->
Data privacy and security
As with all AI services, developers using the Document Intelligence service should be aware of Microsoft policies on customer data. See our
Data, privacy, and security for Document Intelligence page.
Next steps
. Choose a Document Intelligence model.
. Try processing your own forms and documents with the Document Intelligence Studio [.
· Complete a Document Intelligence quickstart and get started creating a document processing app in the development language of
your choice.
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 24 ---------------------------------------------------------------------------------------------------------------------------------->
What's new in Azure AI Document
Intelligence
Article · 03/10/2025
This content applies to:
v4.0 (GA)
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Document Intelligence service is updated on an ongoing basis. Bookmark this page to
stay up to date with release notes, feature enhancements, and our newest
documentation.
1 Important
Preview API versions are retired once the GA API is released. The 2023-02-28-
preview API version is retiring. If you're still using the preview API or the associated
SDK versions, update your code to target the latest API version 2024-11-30 (GA) .
December 2024
Document Intelligence v4.0 programming language SDKs are now generally available
(GA)!
The latest client libraries default to the 2024-11-30 REST API (GA) version of the service.
For more information, see client libraries for the following supported programming
languages:
NEW
.NET (C#)
NEW
Java
NEW
JavaScript
NEW
Python
November 2024
Document Intelligence REST API v4.0: 2024-11-30 REST API (GA) is now generally
available (GA)! The v4.0 REST API includes the following changes:
NEW
Batch API


<!---- Page 25 ---------------------------------------------------------------------------------------------------------------------------------->
o Batch API now supports all models, including all read, layout, prebuilt verticals,
and custom models.
o Batch API supports LIST function to allow users to list batch jobs within past
seven days.
o Batch API supports DELETE function to explicitly delete batch job for GDPR and
privacy compliance.
o GetAnalyzeBatchResult supports resultld in response to LIST all resultlds.
NEW
Searchable PDF. The prebuilt read model now supports images formats
(JPEG/JPG, PNG, BMP, TIFF, HEIF) and language expansion to include Chinese,
Japanese, and Korean for PDF output.
· Custom classification model
o Custom classification model supports incremental training. You can add new
samples to existing classes or add new classes by referencing an existing
classifier.
o With v4.0, custom classification model doesn't split documents by default
during analysis. You need to explicitly set 'splitMode' property to auto to
preserve the older behavior.
o Custom classification model now supports 25,000 pages as new training page
limit.
· Custom Neural Model
o Custom Neural model now supports signature detection.
o Custom neural models support paid training for longer duration when you need
to train model with a larger labeled dataset. The first 20 training runs in a
calendar month continue to be free. Any training operations over 20 is on the
paid tier. Learn more details on billing.
· US Bank statement model
o US Bank Statement Model now supports check table extraction.
· Check model
o Supports Payer's Signature extraction
· Mortgage documents model
· Mortgage model now supports signature detection for forms 1003, 1004, 1005
and closing disclosure.
· Receipt Model
o Receipt Model now supports more fields including ReceiptType, Tax rate,
CountryRegion, net amount and description.
NEW
US Tax model


<!---- Page 26 ---------------------------------------------------------------------------------------------------------------------------------->
o New prebuilt tax models added for 1095A, 1095C, 1099SSA, and W4.
· Delete analyze response
o Analyze response is stored for 24 hours from when the operation completes for
retrieval. For scenarios where you want to delete the response sooner, use the
delete analyze response API to delete the response.
· The v4.0 API includes cumulative updates from preview releases as listed:
o August 2024
o May 2024
o Feb 2024
August 2024
The Document Intelligence 2024-07-31-preview REST API is now available. This preview
API introduces new and updated capabilities:
· Public preview version 2024-07-31-preview is currently available only in the
following Azure regions. The new document field extraction model in Azure AI
Foundry portal is only available in North Central US region:
· East US
· West US2
· West Europe
· North Central US
NEW
Model compose with custom classifiers
o Document Intelligence now adds support for composing model with an explicit
custom classification model. Learn more about the benefits of using the new
compose capability.
· Custom classification model
o Custom classification model now supports updating the model in-place as well.
o Custom classification model adds support for model copy operation to enable
backup and disaster recovery.
o Custom classification model now supports explicitly specifying pages to be
classified from an input document.
NEW
Mortgage documents model
o Extract information from Appraisal (Form 1004).
o Extract information from Validation of Employment (Form 1005).


<!---- Page 27 ---------------------------------------------------------------------------------------------------------------------------------->
NEW
Check model
o Extract payee, amount, date, and other relevant information from checks.
NEW
Pay Stub model
o New prebuilt to process pay stubs to extract wages, hours, deductions, net pay
and more.
NEW
Bank statement model
o New prebuilt to extract account information including beginning and ending
balances, transaction details from bank statements.
NEW
US Tax model
o New unified US tax model that can extract from forms such as W-2, 1098, 1099,
and 1040.
NEW
Searchable PDF. The prebuilt read model now supports PDF output to
download PDFs with embedded text from extraction results, allowing for PDF to be
utilized in scenarios such as search copy of contents.
· Layout model now supports improved figure detection where figures from
documents can now be downloaded as an image file to be used for further figure
understanding. The layout model also features improvements to the OCR model
for scanned text targeting improvements for single characters, boxed text, and
dense text documents.
NEW
Batch API
o Document Intelligence now adds support for batch analysis operation to
support analyzing a set of documents to simplify developer experience and
improve efficiency.
· Add-on capabilities
o Query fields Al quality of extraction is improved with the latest model.
May 2024
The Document Intelligence Studio adds support for Microsoft Entra (formerly Azure
Active Directory) authentication. For more information, see Authentication in Document
Intelligence Studio.
February 2024


<!---- Page 28 ---------------------------------------------------------------------------------------------------------------------------------->
The Document Intelligence 2024-07-31-preview REST API is now available. This preview
API introduces new and updated capabilities:
· Public preview version 2024-07-31-preview is currently available only in the
following Azure regions:
o East US
o West US2
o West Europe
· Layout model now supports figure detection and hierarchical document structure
analysis (sections and subsections). The AI quality of reading order and logical
roles detection is also improved.
· Custom extraction models
o Custom extraction models now support cell, row, and table level confidence
scores. Learn more about table, row, and cell confidence.
o Custom extraction models have Al quality improvements for field extraction.
o Custom template extraction model now supports extracting overlapping fields.
Learn more about overlapping fields and how you use them.
· Custom classification model
o Custom classification model now supported incremental training for scenarios
where you need to update the classifier model with added samples or classes.
Learn more about incremental training.
o Custom classification model adds support for Office document types (.docx,
.pptx, and .xls). Learn more about expanded document type support.
· Invoice model
o Support for new locales:
[] Expand table
Locale
Code
Arabic
(ar)
Bulgarian
(bg)
Greek
(el)
Hebrew
(he)
Macedonian
(mk )
Russian (ru)
Serbian Cyrillic ( sr-cyrl )

| Locale | Code |
| --- | --- |
| Arabic | (ar) |
| Bulgarian | (bg) |
| Greek | (el) |
| Hebrew | (he) |
| Macedonian | (mk ) |
| Russian (ru) | Serbian Cyrillic ( sr-cyrl ) |


<!---- Page 29 ---------------------------------------------------------------------------------------------------------------------------------->
Locale
Code
Ukrainian
(uk)
Thai
(th)
Turkish
(tr)
Vietnamese
(vi)
· Support for new currency codes:
0
Expand table
Currency
Locale
Code
BAM
Bosnian Convertible Mark
(ba)
BGN
Bulgarian Lev
(bg)
ILS
Israeli New Shekel
(il)
MKD
Macedonian Denar
(mk )
RUB
Russian Ruble
(ru)
THB
Thai Baht
(th)
TRY
Turkish Lira
(tr)
UAH
Ukrainian Hryvnia
(ua)
VND
Vietnamese Dong
(vn)
o Tax items support expansion for Germany ( de ), Spain (es ), Portugal (pt),
English Canada en-CA.
· ID model
o Expanded field support for European Union IDs and driver license.
NEW
Mortgage documents
o Extract information from Uniform Residential Loan Application (Form 1003).
o Extract information from Uniform Underwriting and Transmittal Summary or
Form 1008.
o Extract information from mortgage closing disclosure.
NEW
Credit/Debit card model
o Extract information from bank cards.
Marriage certificate
NEW

| Locale | Code |
| --- | --- |
| Ukrainian | (uk) |
| Thai | (th) |
| Turkish | (tr) |
| Vietnamese | (vi) |

| Currency | Locale | Code |
| --- | --- | --- |
| BAM | Bosnian Convertible Mark | (ba) |
| BGN | Bulgarian Lev | (bg) |
| ILS | Israeli New Shekel | (il) |
| MKD | Macedonian Denar | (mk ) |
| RUB | Russian Ruble | (ru) |
| THB | Thai Baht | (th) |
| TRY | Turkish Lira | (tr) |
| UAH | Ukrainian Hryvnia | (ua) |
| VND | Vietnamese Dong | (vn) |


<!---- Page 30 ---------------------------------------------------------------------------------------------------------------------------------->
o New prebuilt to extract information from marriage certificates.
December 2023
The Document Intelligence client libraries targeting REST API 2023-10-31-preview are
now available for use!
November 2023
The Document Intelligence 2023-10-31-preview REST API is now available. This preview
API introduces new and updated capabilities:
· Public preview version 2023-10-31-preview is currently only available in the
following Azure regions:
o East US
o West US2
o West Europe
· Read model
o Language Expansion for Handwriting: Russian( ru), Arabic( ar ), Thai(th).
o Cyber Executive Order (EO) compliance.
· Layout model
o Support office and HTML files.
o Markdown output support.
o Table extraction, reading order, and section heading detection improvements.
o With the Document Intelligence 2023-10-31-preview, the general document
model (prebuilt-document) is deprecated. Going forward, to extract key-value
pairs from documents, use the prebuilt-layout model with the optional query
string parameter features=keyValuePairs enabled.
· Receipt model
o Now extracts currency for all price-related fields.
. Health Insurance Card model
o New field support for Medicare and Medicaid information.
. US Tax Document models
· New 1099 tax model. Supports base 1099 form and the following variations: A,
B, C, CAP, DIV, G, H, INT, K, LS, LTC, MISC, NEC, OID, PATR, Q, QA, R, S, SA, SB.
· Invoice model


<!---- Page 31 ---------------------------------------------------------------------------------------------------------------------------------->
o Support for KVK field.
o Support for BPAY field.
o Numerous field refinements.
. Custom Classification
o Support for multi-language documents.
· New page splitting options: autosplit, always split by page, no split.
· Add-on capabilities
o Query fields are available with the 2023-10-31-preview release.
o Add-on capabilities are available within all models excluding the Read model.
1 Note
With the 2022-08-31 API general availability (GA) release, the associated preview
APIs are being deprecated. If you're using the 2021-09-30-preview, 2022-01-30-
preview, or 2022-06-30-preview API versions, update your applications to target
the 2022-08-31 API version. There are a few minor changes involved, for more
information, see the migration guide.
Feedback
Was this page helpful?
Yes
P
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 32 ---------------------------------------------------------------------------------------------------------------------------------->
Which model should I choose?
Article · 12/11/2024
Azure AI Document Intelligence supports a wide variety of models that enable you to
add intelligent document processing to your applications and optimize your workflows.
Selecting the right model is essential to ensure the success of your enterprise. In this
article, we explore the available Document Intelligence models and provide guidance for
how to choose the best solution for your projects.
https://www.microsoft.com/en-us/videoplayer/embed/RE5fX1b?postJsllMsg=true
The following decision charts highlight the features of each supported model to help
you choose the model that best meets the needs and requirements of your application.
1 Important
Be sure to check the language support page for supported language text and field
extraction by feature.
Pretrained document-analysis models
0
Expand table
Document type
Example
Data to extract
Your best solution
A generic
document.
A contract or letter.
You want to primarily
extract written or
printed text lines,
words, locations, and
Read OCR model
detected languages.
A document that
includes
structural
information.
A report or study.
In addition to written
or printed text, you
need to extract
structural information
like tables, selection
marks, paragraphs,
titles, headings, and
subheadings.
Layout analysis model
A structured or
semi-structured
document that
includes content
A form or document
that is a
You want to extract
fields and values
including ones not
covered by the
** Layout analysis model
with the optional query
standardized format
commonly used in
string parameter

| Document type | Example | Data to extract | Your best solution |
| --- | --- | --- | --- |
| A generic document. | A contract or letter. | You want to primarily extract written or printed text lines, words, locations, and | Read OCR model |
|  |  | detected languages. |  |
| A document that includes structural information. | A report or study. | In addition to written or printed text, you need to extract structural information like tables, selection marks, paragraphs, titles, headings, and subheadings. | Layout analysis model |
| A structured or semi-structured document that includes content | A form or document that is a | You want to extract fields and values including ones not covered by the | ** Layout analysis model |
|  |  |  | with the optional query |
|  | standardized format commonly used in |  | string parameter |


<!---- Page 33 ---------------------------------------------------------------------------------------------------------------------------------->
Document type
Example
Data to extract
Your best solution
formatted as
your business or
scenario-specific
features=keyValuePairs
enabled **
fields (keys) and
industry like a credit
prebuilt models
values.
application or
without having to
survey.
train a custom model.
Pretrained scenario-specific models
0
Expand table
Document type
Data to extract
Your best
solution
US Unified Tax
You want to extract key information across all tax
forms of W2, 1040, 1090, 1098 from a single file
without running any custom classification of your
own.
US Unified tax
model
US Tax W-2 tax
You want to extract key information such as
salary, wages, and taxes withheld.
US tax W-2
model
US Tax W-4 tax
You want to extract key information such as claim
adjustments, personal information.
US tax W-4
model
US Tax 1095(A,C)
You want to extract premium tax credit, advance
credit payment details.
US tax 1095
model
US Tax 1098
You want to extract mortgage interest details
such as principal, points, and tax.
US tax 1098
model
US Tax 1098-E
You want to extract student loan interest details
such as lender and interest amount.
US tax 1098-E
model
US Tax 1098T
You want to extract qualified tuition details such
as scholarship adjustments, student status, and
lender information.
US tax 1098-T
model
US Tax 1099(Variations)
You want to extract information from 1099 forms
and its variations (A, B, C, CAP, DIV, G, H, INT, K,
LS, LTC, MISC, NEC, OID, PATR, Q, QA, R, S, SA,
SB).
US tax 1099
model
US Tax 1040(Variations)
You want to extract information from 1040 forms
and its variations (Schedule 1, Schedule 2,
Schedule 3, Schedule 8812, Schedule A, Schedule
B, Schedule C, Schedule D, Schedule E, Schedule
EIC, Schedule F, Schedule H, Schedule J,
Schedule R, Schedule SE, Schedule Senior).
US tax 1040
model

| Document type | Example | Data to extract | Your best solution |
| --- | --- | --- | --- |
| formatted as | your business or | scenario-specific | features=keyValuePairs enabled ** |
| fields (keys) and | industry like a credit | prebuilt models |  |
| values. | application or | without having to |  |
|  | survey. | train a custom model. |  |

| Document type | Data to extract | Your best solution |
| --- | --- | --- |
| US Unified Tax | You want to extract key information across all tax forms of W2, 1040, 1090, 1098 from a single file without running any custom classification of your own. | US Unified tax model |
| US Tax W-2 tax | You want to extract key information such as salary, wages, and taxes withheld. | US tax W-2 model |
| US Tax W-4 tax | You want to extract key information such as claim adjustments, personal information. | US tax W-4 model |
| US Tax 1095(A,C) | You want to extract premium tax credit, advance credit payment details. | US tax 1095 model |
| US Tax 1098 | You want to extract mortgage interest details such as principal, points, and tax. | US tax 1098 model |
| US Tax 1098-E | You want to extract student loan interest details such as lender and interest amount. | US tax 1098-E model |
| US Tax 1098T | You want to extract qualified tuition details such as scholarship adjustments, student status, and lender information. | US tax 1098-T model |
| US Tax 1099(Variations) | You want to extract information from 1099 forms and its variations (A, B, C, CAP, DIV, G, H, INT, K, LS, LTC, MISC, NEC, OID, PATR, Q, QA, R, S, SA, SB). | US tax 1099 model |
| US Tax 1040(Variations) | You want to extract information from 1040 forms and its variations (Schedule 1, Schedule 2, Schedule 3, Schedule 8812, Schedule A, Schedule B, Schedule C, Schedule D, Schedule E, Schedule EIC, Schedule F, Schedule H, Schedule J, Schedule R, Schedule SE, Schedule Senior). | US tax 1040 model |


<!---- Page 34 ---------------------------------------------------------------------------------------------------------------------------------->
Document type
Data to extract
Your best
solution
Bank Statement
You want to extract key information from US
bank statement
\Bank
Statement
Bank check
You want to extract key information from check
document.
Bank Check
Contract (legal agreement
between parties).
You want to extract contract agreement details
such as parties, dates, and intervals.
Contract model
Health insurance card or
health insurance ID.
You want to extract key information such as
insurer, member ID, prescription coverage, and
group number.
Health
insurance card
model
Credit/Debit card
You want to extract key information bank cards
such as card number and bank name.
Credit/Debit
card model
Marriage Certificate
You want to extract key information from
marriage certificates.
Marriage
certificate
model
Invoice or billing
statement
You want to extract key information such as
customer name, billing address, and amount due.
Invoice model
Receipt, voucher, or
single-page hotel receipt.
You want to extract key information such as
merchant name, transaction date, and transaction
total.
Receipt model
Identity document (ID)
like a U.S. driver's license
or international passport
You want to extract key information such as first
name, surname, date of birth, address, and
signature.
Identity
document (ID)
model
Pay stub
You want to extract key information from the pay
stub document.
Pay stub Model
US Mortgage 1003
You want to extract key information from the
Uniform Residential loan application.
1003 form
model
US Mortgage 1004
You want to extract key information from the
Uniform Residential Appraisal Report (URAR).
1004 form
model
US Mortgage 1005
You want to extract key information from the
Verification of employment form
1005 form
model
US Mortgage 1008
You want to extract key information from the
Uniform Underwriting and Transmittal summary.
1008 form
model
US Mortgage Closing
Disclosure
You want to extract key information from a
mortgage closing disclosure form.
Mortgage
closing

| Document type | Data to extract | Your best solution |
| --- | --- | --- |
| Bank Statement | You want to extract key information from US bank statement | \Bank Statement |
| Bank check | You want to extract key information from check document. | Bank Check |
| Contract (legal agreement between parties). | You want to extract contract agreement details such as parties, dates, and intervals. | Contract model |
| Health insurance card or health insurance ID. | You want to extract key information such as insurer, member ID, prescription coverage, and group number. | Health insurance card model |
| Credit/Debit card | You want to extract key information bank cards such as card number and bank name. | Credit/Debit card model |
| Marriage Certificate | You want to extract key information from marriage certificates. | Marriage certificate model |
| Invoice or billing statement | You want to extract key information such as customer name, billing address, and amount due. | Invoice model |
| Receipt, voucher, or single-page hotel receipt. | You want to extract key information such as merchant name, transaction date, and transaction total. | Receipt model |
| Identity document (ID) like a U.S. driver's license or international passport | You want to extract key information such as first name, surname, date of birth, address, and signature. | Identity document (ID) model |
| Pay stub | You want to extract key information from the pay stub document. | Pay stub Model |
| US Mortgage 1003 | You want to extract key information from the Uniform Residential loan application. | 1003 form model |
| US Mortgage 1004 | You want to extract key information from the Uniform Residential Appraisal Report (URAR). | 1004 form model |
| US Mortgage 1005 | You want to extract key information from the Verification of employment form | 1005 form model |
| US Mortgage 1008 | You want to extract key information from the Uniform Underwriting and Transmittal summary. | 1008 form model |
| US Mortgage Closing Disclosure | You want to extract key information from a mortgage closing disclosure form. | Mortgage closing |


<!---- Page 35 ---------------------------------------------------------------------------------------------------------------------------------->
Document type
Data to extract
Your best
solution
disclosure form
model
Mixed-type document(s)
with structured, semi-
structured, and/or
unstructured elements
You want to extract key-value pairs, selection
marks, tables, signature fields, and selected
regions not extracted by prebuilt or general
document models.
Custom model
? Tip
· If you're still unsure which pretrained model to use, try the layout model with
the optional query string parameter features=keyValuePairs enabled.
· The layout model is powered by the Read OCR engine to detect pages, tables,
styles, text, lines, words, locations, and languages.
Custom extraction models
[] Expand table
Training set
Example documents
Your best
solution
Structured, consistent, documents with
a static layout.
Structured forms such as
questionnaires or applications.
Custom template
model
Structured and semi-structured.
· Structured - surveys
· Semi-structured - invoices
Custom neural
model
A collection of several models each
trained on similar-type documents.
· Supply purchase orders
· Equipment purchase orders
· Furniture purchase orders
All composed into a single
model.
Composed
custom model
Custom classification model
Expand table

| Document type | Data to extract | Your best solution |
| --- | --- | --- |
|  |  | disclosure form model |
| Mixed-type document(s) with structured, semi- structured, and/or unstructured elements | You want to extract key-value pairs, selection marks, tables, signature fields, and selected regions not extracted by prebuilt or general document models. | Custom model |

| Training set | Example documents | Your best solution |
| --- | --- | --- |
| Structured, consistent, documents with a static layout. | Structured forms such as questionnaires or applications. | Custom template model |
| Structured and semi-structured. | · Structured - surveys · Semi-structured - invoices | Custom neural model |
| A collection of several models each trained on similar-type documents. | · Supply purchase orders · Equipment purchase orders · Furniture purchase orders All composed into a single model. | Composed custom model |


<!---- Page 36 ---------------------------------------------------------------------------------------------------------------------------------->
Training set
Example documents
Your best solution
At least two different types of
Forms, letters, or
documents
Custom classification
model
documents.
Next steps
. Learn how to process your own forms and documents with the Document
Intelligence Studio
Feedback
Was this page helpful?
Yes
No
Provide product feedback z | Get help at Microsoft Q&A

| Training set | Example documents | Your best solution |
| --- | --- | --- |
| At least two different types of | Forms, letters, or documents | Custom classification model |
| documents. |  |  |


<!---- Page 37 ---------------------------------------------------------------------------------------------------------------------------------->
Create a Document Intelligence
resource
Article · 11/19/2024
This content applies to:
v4.0 (GA)
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Azure AI Document Intelligence is a cloud-based Azure AI service that uses machine-
learning models to extract key-value pairs, text, and tables from your documents. In this
article, learn how to create a Document Intelligence resource in the Azure portal.
Visit the Azure portal
The Azure portal is a single platform you can use to create and manage Azure services.
Let's get started:
1. Sign in to the Azure portal .
2. Select Create a resource from the Azure home page.
3. Search for and choose Document Intelligence from the search bar.
4. Select the Create button.
Create a resource
1. Next, you're going to fill out the Create Document Intelligence fields with the
following values:
· Subscription. Select your current subscription.
· Resource group. The Azure resource group that contains your resource. You
can create a new group or add it to an existing group.
· Region. Select your local region.
. Name. Enter a name for your resource. We recommend using a descriptive
name, for example YourNameFormRecognizer.
· Pricing tier. The cost of your resource depends on the pricing tier you choose
and your usage. For more information, see pricing details . You can use the
free pricing tier (F0) to try the service, and upgrade later to a paid tier for
production.
2. Select Review + Create.


<!---- Page 38 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Create a resource > Form Recognizer >
Create Form Recognizer
Basics Network Identity Tags Review + create
Accelerate your business processes by automating information extraction. Form Recognizer applies advanced machine
learning to accurately extract text, key/value pairs, and tables from documents. With just a few samples, Form Recognizer
tailors its understanding to your documents, both on-premises and in the cloud. Turn forms into usable data at a
fraction of the time and cost, so you can focus more time acting on the information rather than compiling it. Learn more.
Project Details
Subscription *
0
your subscription
V
Resource group * @
your-resource-group
V
Create new
Instance Details
Region @
West US
V
Name * @
your-form-recognizer-resource
V
Pricing tier * @
Free FO (500 Pages per month, 20 Calls per minute for recognizer AP ...
V
View full pricing details
Review + create
< Previous
Next : Network >
3. Azure will run a quick validation check, after a few seconds you should see a green
banner that says Validation Passed.
4. Once the validation banner appears, select the Create button from the bottom-left
corner.
5. After you select create, you'll be redirected to a new page that says Deployment in
progress. After a few seconds, you'll see a message that says, Your deployment is
complete.
Get Endpoint URL and keys
1. Once you receive the deployment is complete message, select the Go to resource
button.
2. Copy the key and endpoint values from your Document Intelligence resource paste
them in a convenient location, such as Microsoft Notepad. You need the key and
endpoint values to connect your application to the Document Intelligence API.
3. If your overview page doesn't have the keys and endpoint visible, you can select
the Keys and Endpoint button, on the left navigation bar, and retrieve them there.


<!---- Page 39 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
P Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
& Encryption
KEY 2
Pricing tier
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
> Automation
Los
> Help
That's it! You're now ready to start automating data extraction using Azure AI Document
Intelligence.
Next steps
. Try the Document Intelligence Studio, an online tool for visually exploring,
understanding, and integrating features from the Document Intelligence service
into your applications.
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice:
o C#
o Python
o Java
o JavaScript
Feedback
Was this page helpful?
& Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 40 ---------------------------------------------------------------------------------------------------------------------------------->
Studio experience for Document
Intelligence
Article · 03/19/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
The studio is an online tool to visually explore, understand, train, and integrate features
from the Document Intelligence service into your applications. The studio provides a
platform for you to experiment with the different Document Intelligence models and
sample returned data in an interactive manner without the need to write code. You can
use the studio experience to:
· Learn more about the different capabilities in Document Intelligence.
. Use your Document Intelligence resource to test models on sample documents or
upload your own documents.
· Experiment with different add-on and preview features to adapt the output to your
needs.
· Train custom classification models to classify documents.
· Train custom extraction models to extract fields from documents.
. Get sample code for the language specific SDKs to integrate into your applications.
Currently, we're undergoing the migration of features from the Document Intelligence
Studio to the new Azure AI Foundry portal . There are some differences in the
offerings for the two studios, which determine the correct studio for your use case.
Choosing the correct studio experience
There are currently two studios, the Azure AI Foundry portal and the Document
Intelligence Studio [ for building and validating Document Intelligence models. As the
experiences migrate to the new Azure AI Foundry portal, some experiences are available
in both studios, while other experiences/models are only available in only one of the
studios. To follow are a few guidelines for choosing the Studio experience for your
needs. All of our prebuilt models and general extraction models are available on both
studios.
When to use Document Intelligence Studio
Document Intelligence Studio contains all features released on or before November
2024. For any of the v2.1, v3.0, v3.1 features, continue to use the Document Intelligence


<!---- Page 41 ---------------------------------------------------------------------------------------------------------------------------------->
Studio. Studios provide a visual experience for labeling, training, and validating custom
models. For custom document field extraction models, use the Document Intelligence
Studio for template and neural models. Custom classification models can only be trained
and used on Document Intelligence Studio. Use Document Intelligence Studio if you
want to try out GA versions of the models from version v3.0, v3.1, and v4.0.
When to use Azure AI Foundry portal
Start with the new Azure AI Foundry and try any of the prebuilt document models from
2024-11-30 version including general extraction models like Read or Layout.
Learn more about Document Intelligence
Studio
Select the studio experience from the following tabs to learn more about each studio
and how you can get started.
Document Intelligence Studio
1 Important
· Document Intelligence Studio has distinct URLs for sovereign cloud
regions.
· Azure for US Government: Document Intelligence Studio (Azure Fairfax),
· Microsoft Azure operated by 21Vianet: Document Intelligence Studio
(Azure China), zZ
The studio supports Document Intelligence v3.0 and later API versions for model
analysis and custom model training. Previously trained v2.1 models with labeled
data are supported, but not v2.1 model training. Refer to the REST API migration
guide for detailed information about migrating from v2.1 to v3.0.
Use the Document Intelligence Studio quickstart to get started analyzing
documents with document analysis or prebuilt models. Build custom models and
reference the models in your applications using one of the language specific SDKs.
Document Intelligence model support


<!---- Page 42 ---------------------------------------------------------------------------------------------------------------------------------->
Use the help wizard, labeling interface, training step, and interactive visualizations
to understand how each feature works.
. Read: Try out Document Intelligence's Studio Read feature [ with sample
documents or your own documents and extract text lines, words, detected
languages, and handwritten style if detected. To learn more, see Read
overview.
. Layout: Try out Document Intelligence's Studio Layout feature [ with sample
documents or your own documents and extract text, tables, selection marks,
and structure information. To learn more, see Layout overview.
· Prebuilt models: Document Intelligence's prebuilt models enable you to add
intelligent document processing to your apps and flows without having to
train and build your own models. As an example, start with the Studio Invoice
feature . To learn more, see Models overview.
. Custom extraction models: Document Intelligence's Studio Custom models
feature enables you to extract fields and values from models trained with
your data, tailored to your forms and documents. To extract data from
multiple form types, create standalone custom models or combine two, or
more, custom models and create a composed model. Test the custom model
with your sample documents and iterate to improve the model. To learn more,
see the Custom models overview.
· Custom classification models: Document classification is a new scenario
supported by Document Intelligence. The document classifier API supports
classification and splitting scenarios. Train a classification model to identify the
different types of documents your application supports. The input file for the
classification model can contain multiple documents and classifies each
document within an associated page range. To learn more, see custom
classification models.
. Add-on Capabilities: Document Intelligence supports more sophisticated
analysis capabilities. These optional capabilities can be enabled and disabled
in the studio using the Analyze Options button in each model page. There are
four add-on capabilities available: highResolution, formula, font, and
barcode extraction capabilities. To learn more, see Add-on capabilities.
Next steps


<!---- Page 43 ---------------------------------------------------------------------------------------------------------------------------------->
. Visit Document Intelligence Studio .
. Visit Azure Al Foundry portal .
· Get started with Document Intelligence Studio quickstart.
Feedback
Was this page helpful?
Yes
P No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 44 ---------------------------------------------------------------------------------------------------------------------------------->
Language support: document analysis
Article · 11/19/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v2.1 (GA)
v3.0 (GA)
Azure AI Document Intelligence models provide multilingual document processing
support. Our language support capabilities enable your users to communicate with your
applications in natural ways and empower global outreach. Document analysis models
enable text extraction from forms and documents and return structured business-ready
content ready for your organization's action, use, or progress. The following tables list
the available language and locale support by model and feature:
. Read: The read model enables extraction and analysis of printed and handwritten
text. This model is the underlying OCR engine for other Document Intelligence
prebuilt models like layout, general document, invoice, receipt, identity (ID)
document, health insurance card, tax documents and custom models. For more
information, see Read model overview
. Layout: The layout model enables extraction and analysis of text, tables, document
structure, and selection marks (like radio buttons and checkboxes) from forms and
documents.
1 Note
Language code optional
· Document Intelligence's deep learning based universal models extract all
multi-lingual text in your documents, including text lines with mixed
languages, and don't require specifying a language code.
· Don't provide the language code as the parameter unless you are sure of the
language and want to force the service to apply only the relevant model.
Otherwise, the service may return incomplete and incorrect text.
. Also, It's not necessary to specify a locale. This is an optional parameter. The
Document Intelligence deep-learning technology will auto-detect the text
language in your image.


<!---- Page 45 ---------------------------------------------------------------------------------------------------------------------------------->
Read model
Model ID: prebuilt-read
Read: printed text
The following table lists read model language support for extracting and analyzing
printed text.
Expand table
Language
Code (optional)
Abaza
abq
Abkhazian
ab
Achinese
ace
Acoli
ach
Adangme
ada
Adyghe
ady
Afar
aa
Afrikaans
af
Akan
ak
Albanian
sq
Algonquin
alq
Angika (Devanagari)
anp
Arabic
ar
Asturian
ast
Asu (Tanzania)
asa
Avaric
av
Awadhi-Hindi (Devanagari)
awa

| Language | Code (optional) |
| --- | --- |
| Abaza | abq |
| Abkhazian | ab |
| Achinese | ace |
| Acoli | ach |
| Adangme | ada |
| Adyghe | ady |
| Afar | aa |
| Afrikaans | af |
| Akan | ak |
| Albanian | sq |
| Algonquin | alq |
| Angika (Devanagari) | anp |
| Arabic | ar |
| Asturian | ast |
| Asu (Tanzania) | asa |
| Avaric | av |
| Awadhi-Hindi (Devanagari) | awa |


<!---- Page 46 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Aymara
ay
Azerbaijani (Latin)
az
Bafia
ksf
Bagheli
bfy
Bambara
bm
Bashkir
ba
Basque
eu
Belarusian (Cyrillic)
be
be-cyrl
Belarusian (Latin)
be
be-latn
Bemba (Zambia)
bem
Bena (Tanzania)
bez
Bhojpuri-Hindi (Devanagari)
bho
Bikol
bik
Bini
bin
Bislama
bi
Bodo (Devanagari)
brx
Bosnian (Latin)
bs
Brajbha
bra
Breton
br
Bulgarian
bg
Bundeli
bns
Buryat (Cyrillic)
bua
Catalan
ca
Cebuano
ceb
Chamling
rab
Chamorro
ch

| Language | Code (optional) |
| --- | --- |
| Aymara | ay |
| Azerbaijani (Latin) | az |
| Bafia | ksf |
| Bagheli | bfy |
| Bambara | bm |
| Bashkir | ba |
| Basque | eu |
| Belarusian (Cyrillic) | be be-cyrl |
| Belarusian (Latin) | be be-latn |
| Bemba (Zambia) | bem |
| Bena (Tanzania) | bez |
| Bhojpuri-Hindi (Devanagari) | bho |
| Bikol | bik |
| Bini | bin |
| Bislama | bi |
| Bodo (Devanagari) | brx |
| Bosnian (Latin) | bs |
| Brajbha | bra |
| Breton | br |
| Bulgarian | bg |
| Bundeli | bns |
| Buryat (Cyrillic) | bua |
| Catalan | ca |
| Cebuano | ceb |
| Chamling | rab |
| Chamorro | ch |


<!---- Page 47 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Chechen
ce
Chhattisgarhi (Devanagari)
hne
Chiga
cgg
Chinese Simplified
zh-Hans
Chinese Traditional
zh-Hant
Choctaw
cho
Chukot
ckt
Chuvash
CV
Cornish
kw
Corsican
co
Cree
cr
Creek
mus
Crimean Tatar (Latin)
crh
Croatian
hr
Crow
cro
Czech
CS
Danish
da
Dargwa
dar
Dari
prs
Dhimal (Devanagari)
dhi
Dogri (Devanagari)
doi
Duala
dua
Dungan
dng
Dutch
nl
Efik
efi
English
en

| Language | Code (optional) |
| --- | --- |
| Chechen | ce |
| Chhattisgarhi (Devanagari) | hne |
| Chiga | cgg |
| Chinese Simplified | zh-Hans |
| Chinese Traditional | zh-Hant |
| Choctaw | cho |
| Chukot | ckt |
| Chuvash | CV |
| Cornish | kw |
| Corsican | co |
| Cree | cr |
| Creek | mus |
| Crimean Tatar (Latin) | crh |
| Croatian | hr |
| Crow | cro |
| Czech | CS |
| Danish | da |
| Dargwa | dar |
| Dari | prs |
| Dhimal (Devanagari) | dhi |
| Dogri (Devanagari) | doi |
| Duala | dua |
| Dungan | dng |
| Dutch | nl |
| Efik | efi |
| English | en |


<!---- Page 48 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Erzya (Cyrillic)
myv
Estonian
et
Faroese
fo
Fijian
fj
Filipino
fil
Finnish
fi
0
'Expand table
Language
Code (optional)
Fon
fon
French
fr
Friulian
fur
Ga
gaa
Gagauz (Latin)
gag
Galician
gl
Ganda
lg
Gayo
gay
German
de
Gilbertese
gil
Gondi (Devanagari)
gon
Greek
el
Greenlandic
kl
Guarani
gn
Gurung (Devanagari)
gvr
Gusii
guz
Haitian Creole
ht

| Language | Code (optional) |
| --- | --- |
| Erzya (Cyrillic) | myv |
| Estonian | et |
| Faroese | fo |
| Fijian | fj |
| Filipino | fil |
| Finnish | fi |

| Language | Code (optional) |
| --- | --- |
| Fon | fon |
| French | fr |
| Friulian | fur |
| Ga | gaa |
| Gagauz (Latin) | gag |
| Galician | gl |
| Ganda | lg |
| Gayo | gay |
| German | de |
| Gilbertese | gil |
| Gondi (Devanagari) | gon |
| Greek | el |
| Greenlandic | kl |
| Guarani | gn |
| Gurung (Devanagari) | gvr |
| Gusii | guz |
| Haitian Creole | ht |


<!---- Page 49 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Halbi (Devanagari)
hlb
Hani
hni
Haryanvi
bgc
Hawaiian
haw
Hebrew
he
Herero
hz
Hiligaynon
hil
Hindi
hi
Hmong Daw (Latin)
mww
Ho(Devanagiri)
hoc
Hungarian
hu
Iban
iba
Icelandic
is
Igbo
ig
Iloko
ilo
Inari Sami
smn
Indonesian
id
Ingush
inh
Interlingua
ia
Inuktitut (Latin)
iu
Irish
ga
Italian
it
Japanese
ja
Jaunsari (Devanagari)
Jns
Javanese
jv
Jola-Fonyi
dyo

| Language | Code (optional) |
| --- | --- |
| Halbi (Devanagari) | hlb |
| Hani | hni |
| Haryanvi | bgc |
| Hawaiian | haw |
| Hebrew | he |
| Herero | hz |
| Hiligaynon | hil |
| Hindi | hi |
| Hmong Daw (Latin) | mww |
| Ho(Devanagiri) | hoc |
| Hungarian | hu |
| Iban | iba |
| Icelandic | is |
| Igbo | ig |
| Iloko | ilo |
| Inari Sami | smn |
| Indonesian | id |
| Ingush | inh |
| Interlingua | ia |
| Inuktitut (Latin) | iu |
| Irish | ga |
| Italian | it |
| Japanese | ja |
| Jaunsari (Devanagari) | Jns |
| Javanese | jv |
| Jola-Fonyi | dyo |


<!---- Page 50 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Kabardian
kbd
Kabuverdianu
kea
Kachin (Latin)
kac
Kalenjin
kln
Kalmyk
xal
Kangri (Devanagari)
xnr
Kanuri
kr
Karachay-Balkar
krc
Kara-Kalpak (Cyrillic)
kaa-cyrl
Kara-Kalpak (Latin)
kaa
Kashubian
csb
Kazakh (Cyrillic)
kk-cyrl
Kazakh (Latin)
kk-latn
Khakas
kjh
Khaling
klr
Khasi
kha
K'iche'
quc
Kikuyu
ki
Kildin Sami
sjd
Kinyarwanda
rw
Komi
kv
Kongo
kg
Korean
ko
Korku
kfq
Koryak
kpy
Kosraean
kos

| Language | Code (optional) |
| --- | --- |
| Kabardian | kbd |
| Kabuverdianu | kea |
| Kachin (Latin) | kac |
| Kalenjin | kln |
| Kalmyk | xal |
| Kangri (Devanagari) | xnr |
| Kanuri | kr |
| Karachay-Balkar | krc |
| Kara-Kalpak (Cyrillic) | kaa-cyrl |
| Kara-Kalpak (Latin) | kaa |
| Kashubian | csb |
| Kazakh (Cyrillic) | kk-cyrl |
| Kazakh (Latin) | kk-latn |
| Khakas | kjh |
| Khaling | klr |
| Khasi | kha |
| K'iche' | quc |
| Kikuyu | ki |
| Kildin Sami | sjd |
| Kinyarwanda | rw |
| Komi | kv |
| Kongo | kg |
| Korean | ko |
| Korku | kfq |
| Koryak | kpy |
| Kosraean | kos |


<!---- Page 51 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Kpelle
kpe
Kuanyama
kj
Kumyk (Cyrillic)
kum
Kurdish (Arabic)
ku-arab
Kurdish (Latin)
ku-latn
Kurukh (Devanagari)
kru
Kyrgyz (Cyrillic)
ky
Lak
lbe
Lakota
lkt
0
Expand table
Language
Code (optional)
Latin
la
Latvian
lv
Lezghian
lex
Lingala
ln
Lithuanian
lt
Lower Sorbian
dsb
Lozi
loz
Lule Sami
smj
Luo (Kenya and Tanzania)
luo
Luxembourgish
lb
Luyia
luy
Macedonian
mk
Machame
jmc
Madurese
mad

| Language | Code (optional) |
| --- | --- |
| Kpelle | kpe |
| Kuanyama | :selected: kj |
| Kumyk (Cyrillic) | kum |
| Kurdish (Arabic) | ku-arab |
| Kurdish (Latin) | ku-latn |
| Kurukh (Devanagari) | kru |
| Kyrgyz (Cyrillic) | ky |
| Lak | lbe |
| Lakota | lkt |

| Language | Code (optional) |
| --- | --- |
| Latin | la |
| Latvian | lv |
| Lezghian | lex |
| Lingala | ln |
| Lithuanian | lt |
| Lower Sorbian | dsb |
| Lozi | loz |
| Lule Sami | smj |
| Luo (Kenya and Tanzania) | luo |
| Luxembourgish | lb |
| Luyia | luy |
| Macedonian | mk |
| Machame | jmc |
| Madurese | mad |


<!---- Page 52 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Mahasu Pahari (Devanagari)
bfz
Makhuwa-Meetto
mgh
Makonde
kde
Malagasy
mg
Malay (Latin)
ms
Maltese
mt
Malto (Devanagari)
kmj
Mandinka
mnk
Manx
gv
Maori
mi
Mapudungun
arn
Marathi
mr
Mari (Russia)
chm
Masai
mas
Mende (Sierra Leone)
men
Meru
mer
Meta'
mgo
Minangkabau
min
Mohawk
moh
Mongolian (Cyrillic)
mn
Mongondow
mog
Montenegrin (Cyrillic)
cnr-cyrl
Montenegrin (Latin)
cnr-latn
Morisyen
mfe
Mundang
mua
Nahuatl
nah

| Language | Code (optional) |
| --- | --- |
| Mahasu Pahari (Devanagari) | bfz |
| Makhuwa-Meetto | mgh |
| Makonde | kde |
| Malagasy | mg |
| Malay (Latin) | ms |
| Maltese | mt |
| Malto (Devanagari) | kmj |
| Mandinka | mnk |
| Manx | gv |
| Maori | mi |
| Mapudungun | arn |
| Marathi | mr |
| Mari (Russia) | chm |
| Masai | mas |
| Mende (Sierra Leone) | men |
| Meru | mer |
| Meta' | mgo |
| Minangkabau | min |
| Mohawk | moh |
| Mongolian (Cyrillic) | mn |
| Mongondow | mog |
| Montenegrin (Cyrillic) | cnr-cyrl |
| Montenegrin (Latin) | cnr-latn |
| Morisyen | mfe |
| Mundang | mua |
| Nahuatl | nah |


<!---- Page 53 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Navajo
nv
Ndonga
ng
Neapolitan
nap
Nepali
ne
Ngomba
jgo
Niuean
niu
Nogay
nog
North Ndebele
nd
Northern Sami (Latin)
sme
Norwegian
no
Nyanja
ny
Nyankole
nyn
Nzima
nzi
Occitan
oc
Ojibwa
oj
Oromo
om
Ossetic
os
Pampanga
pam
Pangasinan
pag
Papiamento
pap
Pashto
ps
Pedi
nso
Persian
fa
Polish
pl
Portuguese
pt
Punjabi (Arabic)
pa

| Language | Code (optional) |
| --- | --- |
| Navajo | nv |
| Ndonga | ng |
| Neapolitan | nap |
| Nepali | ne |
| Ngomba | jgo |
| Niuean | niu |
| Nogay | nog |
| North Ndebele | nd |
| Northern Sami (Latin) | sme |
| Norwegian | no |
| Nyanja | ny |
| Nyankole | nyn |
| Nzima | nzi |
| Occitan | oc |
| Ojibwa | oj |
| Oromo | om |
| Ossetic | os |
| Pampanga | pam |
| Pangasinan | pag |
| Papiamento | pap |
| Pashto | ps |
| Pedi | nso |
| Persian | fa |
| Polish | pl |
| Portuguese | pt |
| Punjabi (Arabic) | pa |


<!---- Page 54 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Quechua
qu
Ripuarian
ksh
Romanian
ro
Romansh
rm
Rundi
rn
Russian
ru
Rwa
rwk
Sadri (Devanagari)
sck
Sakha
sah
Samburu
saq
Samoan (Latin)
sm
Sango
sg
0
Expand table
Language
Code (optional)
Sangu (Gabon)
snq
Sanskrit (Devanagari)
sa
Santali(Devanagiri)
sat
Scots
Sco
Scottish Gaelic
gd
Sena
seh
Serbian (Cyrillic)
sr-cyrl
Serbian (Latin)
sr, Sr-latn
Shambala
ksb
Shona
sn
Siksika
bla

| Language | Code (optional) |
| --- | --- |
| Quechua | qu |
| Ripuarian | ksh |
| Romanian | ro |
| Romansh | rm |
| Rundi | rn |
| Russian | ru |
| Rwa | rwk |
| Sadri (Devanagari) | sck |
| Sakha | sah |
| Samburu | saq |
| Samoan (Latin) | sm |
| Sango | sg 0 Expand table |

| Language | Code (optional) |
| --- | --- |
| Sangu (Gabon) | snq |
| Sanskrit (Devanagari) | sa |
| Santali(Devanagiri) | sat |
| Scots | Sco |
| Scottish Gaelic | gd |
| Sena | seh |
| Serbian (Cyrillic) | sr-cyrl |
| Serbian (Latin) | sr, Sr-latn |
| Shambala | ksb |
| Shona | sn |
| Siksika | bla |


<!---- Page 55 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Sirmauri (Devanagari)
srx
Skolt Sami
sms
Slovak
sk
Slovenian
sl
Soga
xog
Somali (Arabic)
So
Somali (Latin)
so-latn
Songhai
son
South Ndebele
nr
Southern Altai
alt
Southern Sami
sma
Southern Sotho
st
Spanish
es
Sundanese
su
Swahili (Latin)
SW
Swati
SS
Swedish
SV
Tabassaran
tab
Tachelhit
shi
Tahitian
ty
Taita
dav
Tajik (Cyrillic)
tg
Tamil
ta
Tatar (Cyrillic)
tt-cyrl
Tatar (Latin)
tt
Teso
teo

| Language | Code (optional) |
| --- | --- |
| Sirmauri (Devanagari) | srx |
| Skolt Sami | sms |
| Slovak | sk |
| Slovenian | sl |
| Soga | xog |
| Somali (Arabic) | So |
| Somali (Latin) | so-latn |
| Songhai | son |
| South Ndebele | nr |
| Southern Altai | alt |
| Southern Sami | sma |
| Southern Sotho | st |
| Spanish | es |
| Sundanese | su |
| Swahili (Latin) | SW |
| Swati | SS |
| Swedish | SV |
| Tabassaran | tab |
| Tachelhit | shi |
| Tahitian | ty |
| Taita | dav |
| Tajik (Cyrillic) | tg |
| Tamil | ta |
| Tatar (Cyrillic) | tt-cyrl |
| Tatar (Latin) | tt |
| Teso | teo |


<!---- Page 56 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Tetum
tet
Thai
th
Thangmi
thf
Tok Pisin
tpi
Tongan
to
Tsonga
ts
Tswana
tn
Turkish
tr
Turkmen (Latin)
tk
Tuvan
tyv
Udmurt
udm
Uighur (Cyrillic)
ug-cyrl
Ukrainian
uk
Upper Sorbian
hsb
Urdu
ur
Uyghur (Arabic)
ug
Uzbek (Arabic)
uz-arab
Uzbek (Cyrillic)
uz-cyrl
Uzbek (Latin)
uz
Vietnamese
vi
Volapük
vo
Vunjo
vun
Walser
wae
Welsh
cy
Western Frisian
fy
Wolof
WO

| Language | Code (optional) |
| --- | --- |
| Tetum | tet |
| Thai | th |
| Thangmi | thf |
| Tok Pisin | tpi |
| Tongan | to |
| Tsonga | ts |
| Tswana | tn |
| Turkish | tr |
| Turkmen (Latin) | tk |
| Tuvan | tyv |
| Udmurt | udm |
| Uighur (Cyrillic) | ug-cyrl |
| Ukrainian | uk |
| Upper Sorbian | hsb |
| Urdu | ur |
| Uyghur (Arabic) | ug |
| Uzbek (Arabic) | uz-arab |
| Uzbek (Cyrillic) | uz-cyrl |
| Uzbek (Latin) | uz |
| Vietnamese | vi |
| Volapük | vo |
| Vunjo | vun |
| Walser | wae |
| Welsh | cy |
| Western Frisian | fy |
| Wolof | WO |


<!---- Page 57 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Xhosa
xh
Yucatec Maya
yua
Zapotec
zap
Zarma
dje
Zhuang
za
Zulu
zu
Layout
Model ID: prebuilt-layout
Layout: printed text
The following table lists the supported languages for printed text:
@ Expand table
Language
Code (optional)
Abaza
abq
Abkhazian
ab
Achinese
ace
Acoli
ach
Adangme
ada
Adyghe
ady
Afar
aa
Afrikaans
af
Akan
ak
Albanian
sq

| Language | Code (optional) |
| --- | --- |
| Xhosa | :selected: xh |
| Yucatec Maya | yua |
| Zapotec | zap |
| Zarma | dje |
| Zhuang | za |
| Zulu | zu |

| Language | Code (optional) |
| --- | --- |
| Abaza | abq |
| Abkhazian | ab |
| Achinese | ace |
| Acoli | ach |
| Adangme | ada |
| Adyghe | ady |
| Afar | aa |
| Afrikaans | af |
| Akan | ak |
| Albanian | sq |


<!---- Page 58 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Algonquin
alq
Angika (Devanagari)
anp
Arabic
ar
Asturian
ast
Asu (Tanzania)
asa
Avaric
av
Awadhi-Hindi (Devanagari)
awa
Aymara
ay
Azerbaijani (Latin)
az
Bafia
ksf
Bagheli
bfy
Bambara
bm
Bashkir
ba
Basque
eu
Belarusian (Cyrillic)
be
be-cyrl
Belarusian (Latin)
be
be-latn
Bemba (Zambia)
bem
Bena (Tanzania)
bez
Bhojpuri-Hindi (Devanagari)
bho
Bikol
bik
Bini
bin
Bislama
bi
Bodo (Devanagari)
brx
Bosnian (Latin)
bs
Brajbha
bra
Breton
br

| Language | Code (optional) |
| --- | --- |
| Algonquin | alq |
| Angika (Devanagari) | anp |
| Arabic | ar |
| Asturian | ast |
| Asu (Tanzania) | asa |
| Avaric | av |
| Awadhi-Hindi (Devanagari) | awa |
| Aymara | ay |
| Azerbaijani (Latin) | az |
| Bafia | ksf |
| Bagheli | bfy |
| Bambara | bm |
| Bashkir | ba |
| Basque | eu |
| Belarusian (Cyrillic) | be be-cyrl |
| Belarusian (Latin) | be be-latn |
| Bemba (Zambia) | bem |
| Bena (Tanzania) | bez |
| Bhojpuri-Hindi (Devanagari) | bho |
| Bikol | bik |
| Bini | bin |
| Bislama | bi |
| Bodo (Devanagari) | brx |
| Bosnian (Latin) | bs |
| Brajbha | bra |
| Breton | br |


<!---- Page 59 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Bulgarian
bg
Bundeli
bns
Buryat (Cyrillic)
bua
Catalan
ca
Cebuano
ceb
Chamling
rab
Chamorro
ch
Chechen
ce
Chhattisgarhi (Devanagari)
hne
Chiga
cgg
Chinese Simplified
zh-Hans
Chinese Traditional
zh-Hant
Choctaw
cho
Chukot
ckt
Chuvash
CV
Cornish
kw
Corsican
co
Cree
cr
Creek
mus
Crimean Tatar (Latin)
crh
Croatian
hr
Crow
cro
Czech
CS
Danish
da
Dargwa
dar
Dari
prs

| Language | Code (optional) |
| --- | --- |
| Bulgarian | bg |
| Bundeli | bns |
| Buryat (Cyrillic) | bua |
| Catalan | ca |
| Cebuano | ceb |
| Chamling | rab |
| Chamorro | ch |
| Chechen | ce |
| Chhattisgarhi (Devanagari) | hne |
| Chiga | cgg |
| Chinese Simplified | zh-Hans |
| Chinese Traditional | zh-Hant |
| Choctaw | cho |
| Chukot | ckt |
| Chuvash | CV |
| Cornish | kw |
| Corsican | co |
| Cree | cr |
| Creek | mus |
| Crimean Tatar (Latin) | crh |
| Croatian | hr |
| Crow | cro |
| Czech | CS |
| Danish | da |
| Dargwa | dar |
| Dari | prs |


<!---- Page 60 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Dhimal (Devanagari)
dhi
Dogri (Devanagari)
doi
Duala
dua
Dungan
dng
Dutch
nl
Efik
efi
English
en
Erzya (Cyrillic)
myv
Estonian
et
Faroese
fo
Fijian
fj
Filipino
fil
Finnish
fi
0
Expand table
Language
Code (optional)
Fon
fon
French
fr
Friulian
fur
Ga
gaa
Gagauz (Latin)
gag
Galician
gl
Ganda
lg
Gayo
gay
German
de
Gilbertese
gil

| Language | Code (optional) |
| --- | --- |
| Dhimal (Devanagari) | dhi |
| Dogri (Devanagari) | doi |
| Duala | dua |
| Dungan | dng |
| Dutch | nl |
| Efik | efi |
| English | en |
| Erzya (Cyrillic) | :selected: myv |
| Estonian | et |
| Faroese | fo |
| Fijian | fj |
| Filipino | fil |
| Finnish | fi |

| Language | Code (optional) |
| --- | --- |
| Fon | fon |
| French | fr |
| Friulian | fur |
| Ga | gaa |
| Gagauz (Latin) | gag |
| Galician | gl |
| Ganda | lg |
| Gayo | gay |
| German | de |
| Gilbertese | gil |


<!---- Page 61 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Gondi (Devanagari)
gon
Greek
el
Greenlandic
kl
Guarani
gn
Gurung (Devanagari)
gvr
Gusii
guz
Haitian Creole
ht
Halbi (Devanagari)
hlb
Hani
hni
Haryanvi
bgc
Hawaiian
haw
Hebrew
he
Herero
hz
Hiligaynon
hil
Hindi
hi
Hmong Daw (Latin)
mww
Ho(Devanagiri)
hoc
Hungarian
hu
Iban
iba
Icelandic
is
Igbo
ig
Iloko
ilo
Inari Sami
smn
Indonesian
id
Ingush
inh
Interlingua
ia

| Language | Code (optional) |
| --- | --- |
| Gondi (Devanagari) | gon |
| Greek | el |
| Greenlandic | kl |
| Guarani | gn |
| Gurung (Devanagari) | gvr |
| Gusii | guz |
| Haitian Creole | ht |
| Halbi (Devanagari) | hlb |
| Hani | hni |
| Haryanvi | bgc |
| Hawaiian | haw |
| Hebrew | he |
| Herero | hz |
| Hiligaynon | hil |
| Hindi | hi |
| Hmong Daw (Latin) | mww |
| Ho(Devanagiri) | hoc |
| Hungarian | hu |
| Iban | iba |
| Icelandic | is |
| Igbo | ig |
| Iloko | ilo |
| Inari Sami | smn |
| Indonesian | id |
| Ingush | inh |
| Interlingua | ia |


<!---- Page 62 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Inuktitut (Latin)
iu
Irish
ga
Italian
it
Japanese
ja
Jaunsari (Devanagari)
Jns
Javanese
jv
Jola-Fonyi
dyo
Kabardian
kbd
Kabuverdianu
kea
Kachin (Latin)
kac
Kalenjin
kln
Kalmyk
xal
Kangri (Devanagari)
xnr
Kanuri
kr
Karachay-Balkar
krc
Kara-Kalpak (Cyrillic)
kaa-cyrl
Kara-Kalpak (Latin)
kaa
Kashubian
csb
Kazakh (Cyrillic)
kk-cyrl
Kazakh (Latin)
kk-latn
Khakas
kjh
Khaling
klr
Khasi
kha
K'iche'
quc
Kikuyu
ki
Kildin Sami
sjd

| Language | Code (optional) |
| --- | --- |
| Inuktitut (Latin) | iu |
| Irish | ga |
| Italian | it |
| Japanese | ja |
| Jaunsari (Devanagari) | Jns |
| Javanese | jv |
| Jola-Fonyi | dyo |
| Kabardian | kbd |
| Kabuverdianu | kea |
| Kachin (Latin) | kac |
| Kalenjin | kln |
| Kalmyk | xal |
| Kangri (Devanagari) | xnr |
| Kanuri | kr |
| Karachay-Balkar | krc |
| Kara-Kalpak (Cyrillic) | kaa-cyrl |
| Kara-Kalpak (Latin) | kaa |
| Kashubian | csb |
| Kazakh (Cyrillic) | kk-cyrl |
| Kazakh (Latin) | kk-latn |
| Khakas | kjh |
| Khaling | klr |
| Khasi | kha |
| K'iche' | quc |
| Kikuyu | ki |
| Kildin Sami | sjd |


<!---- Page 63 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Kinyarwanda
rw
Komi
kv
Kongo
kg
Korean
ko
Korku
kfq
Koryak
kpy
Kosraean
kos
Kpelle
kpe
Kuanyama
kj
Kumyk (Cyrillic)
kum
Kurdish (Arabic)
ku-arab
Kurdish (Latin)
ku-latn
0
Expand table
Language
Code (optional)
Kurukh (Devanagari)
kru
Kyrgyz (Cyrillic)
ky
Lak
lbe
Lakota
lkt
Latin
la
Latvian
lv
Lezghian
lex
Lingala
ln
Lithuanian
lt
Lower Sorbian
dsb
Lozi
loz

| Language | Code (optional) |
| --- | --- |
| Kinyarwanda | rw |
| Komi | :selected: kv |
| Kongo | :selected: kg |
| Korean | :selected: ko |
| Korku | :selected: kfq |
| Koryak | :selected: kpy |
| Kosraean | kos |
| Kpelle | :selected: kpe |
| Kuanyama | :selected: kj |
| Kumyk (Cyrillic) | kum |
| Kurdish (Arabic) | ku-arab |
| Kurdish (Latin) | ku-latn |

| Language | Code (optional) |
| --- | --- |
| Kurukh (Devanagari) | kru |
| Kyrgyz (Cyrillic) | :selected: ky |
| Lak | lbe |
| Lakota | lkt |
| Latin | la |
| Latvian | lv |
| Lezghian | lex |
| Lingala | ln |
| Lithuanian | lt |
| Lower Sorbian | dsb |
| Lozi | loz |


<!---- Page 64 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Lule Sami
smj
Luo (Kenya and Tanzania)
luo
Luxembourgish
lb
Luyia
luy
Macedonian
mk
Machame
jmc
Madurese
mad
Mahasu Pahari (Devanagari)
bfz
Makhuwa-Meetto
mgh
Makonde
kde
Malagasy
mg
Malay (Latin)
ms
Maltese
mt
Malto (Devanagari)
kmj
Mandinka
mnk
Manx
gv
Maori
mi
Mapudungun
arn
Marathi
mr
Mari (Russia)
chm
Masai
mas
Mende (Sierra Leone)
men
Meru
mer
Meta'
mgo
Minangkabau
min
Mohawk
moh

| Language | Code (optional) |
| --- | --- |
| Lule Sami | smj |
| Luo (Kenya and Tanzania) | luo |
| Luxembourgish | lb |
| Luyia | luy |
| Macedonian | mk |
| Machame | jmc |
| Madurese | mad |
| Mahasu Pahari (Devanagari) | bfz |
| Makhuwa-Meetto | mgh |
| Makonde | kde |
| Malagasy | mg |
| Malay (Latin) | ms |
| Maltese | mt |
| Malto (Devanagari) | kmj |
| Mandinka | mnk |
| Manx | gv |
| Maori | mi |
| Mapudungun | arn |
| Marathi | mr |
| Mari (Russia) | chm |
| Masai | mas |
| Mende (Sierra Leone) | men |
| Meru | mer |
| Meta' | mgo |
| Minangkabau | min |
| Mohawk | moh |


<!---- Page 65 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Mongolian (Cyrillic)
mn
Mongondow
mog
Montenegrin (Cyrillic)
cnr-cyrl
Montenegrin (Latin)
cnr-latn
Morisyen
mfe
Mundang
mua
Nahuatl
nah
Navajo
nv
Ndonga
ng
Neapolitan
nap
Nepali
ne
Ngomba
jgo
Niuean
niu
Nogay
nog
North Ndebele
nd
Northern Sami (Latin)
sme
Norwegian
no
Nyanja
ny
Nyankole
nyn
Nzima
nzi
Occitan
oc
Ojibwa
oj
Oromo
om
Ossetic
OS
Pampanga
pam
Pangasinan
pag

| Language | Code (optional) |
| --- | --- |
| Mongolian (Cyrillic) | mn |
| Mongondow | mog |
| Montenegrin (Cyrillic) | cnr-cyrl |
| Montenegrin (Latin) | cnr-latn |
| Morisyen | mfe |
| Mundang | mua |
| Nahuatl | nah |
| Navajo | nv |
| Ndonga | ng |
| Neapolitan | nap |
| Nepali | ne |
| Ngomba | jgo |
| Niuean | niu |
| Nogay | nog |
| North Ndebele | nd |
| Northern Sami (Latin) | sme |
| Norwegian | no |
| Nyanja | ny |
| Nyankole | nyn |
| Nzima | nzi |
| Occitan | oc |
| Ojibwa | oj |
| Oromo | om |
| Ossetic | OS |
| Pampanga | pam |
| Pangasinan | pag |


<!---- Page 66 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Papiamento
pap
Pashto
ps
Pedi
nso
Persian
fa
Polish
pl
Portuguese
pt
Punjabi (Arabic)
pa
Quechua
qu
Ripuarian
ksh
Romanian
ro
Romansh
rm
Rundi
rn
Russian
ru
[] Expand table
Language
Code (optional)
Rwa
rwk
Sadri (Devanagari)
sck
Sakha
sah
Samburu
saq
Samoan (Latin)
sm
Sango
sg
Sangu (Gabon)
snq
Sanskrit (Devanagari)
sa
Santali(Devanagiri)
sat
Scots
Sco

| Language | Code (optional) |
| --- | --- |
| Papiamento | pap |
| Pashto | ps |
| Pedi | nso |
| Persian | fa |
| Polish | pl |
| Portuguese | pt |
| Punjabi (Arabic) | pa |
| Quechua | qu |
| Ripuarian | ksh |
| Romanian | ro |
| Romansh | rm |
| Rundi | rn |
| Russian | ru |

| Language | Code (optional) |
| --- | --- |
| Rwa | rwk |
| Sadri (Devanagari) | sck |
| Sakha | sah |
| Samburu | saq |
| Samoan (Latin) | sm |
| Sango | sg |
| Sangu (Gabon) | snq |
| Sanskrit (Devanagari) | sa |
| Santali(Devanagiri) | sat |
| Scots | Sco |


<!---- Page 67 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Scottish Gaelic
gd
Sena
seh
Serbian (Cyrillic)
sr-cyrl
Serbian (Latin)
sr
sr-latn
Shambala
ksb
Shona
sn
Siksika
bla
Sirmauri (Devanagari)
srx
Skolt Sami
sms
Slovak
sk
Slovenian
sl
Soga
xog
Somali (Arabic)
So
Somali (Latin)
so-latn
Songhai
son
South Ndebele
nr
Southern Altai
alt
Southern Sami
sma
Southern Sotho
st
Spanish
es
Sundanese
su
Swahili (Latin)
SW
Swati
SS
Swedish
SV
Tabassaran
tab
Tachelhit
shi

| Language | Code (optional) |
| --- | --- |
| Scottish Gaelic | gd |
| Sena | seh |
| Serbian (Cyrillic) | sr-cyrl |
| Serbian (Latin) | sr sr-latn |
| Shambala | ksb |
| Shona | sn |
| Siksika | bla |
| Sirmauri (Devanagari) | srx |
| Skolt Sami | sms |
| Slovak | sk |
| Slovenian | sl |
| Soga | xog |
| Somali (Arabic) | So |
| Somali (Latin) | so-latn |
| Songhai | son |
| South Ndebele | nr |
| Southern Altai | alt |
| Southern Sami | sma |
| Southern Sotho | st |
| Spanish | es |
| Sundanese | su |
| Swahili (Latin) | SW |
| Swati | SS |
| Swedish | SV |
| Tabassaran | tab |
| Tachelhit | shi |


<!---- Page 68 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Tahitian
ty
Taita
dav
Tajik (Cyrillic)
tg
Tamil
ta
Tatar (Cyrillic)
tt-cyrl
Tatar (Latin)
tt
Teso
teo
Tetum
tet
Thai
th
Thangmi
thf
Tok Pisin
tpi
Tongan
to
Tsonga
ts
Tswana
tn
Turkish
tr
Turkmen (Latin)
tk
Tuvan
tyv
Udmurt
udm
Uighur (Cyrillic)
ug-cyrl
Ukrainian
uk
Upper Sorbian
hsb
Urdu
ur
Uyghur (Arabic)
ug
Uzbek (Arabic)
uz-arab
Uzbek (Cyrillic)
uz-cyrl
Uzbek (Latin)
uz

| Language | Code (optional) |
| --- | --- |
| Tahitian | ty |
| Taita | dav |
| Tajik (Cyrillic) | tg |
| Tamil | ta |
| Tatar (Cyrillic) | tt-cyrl |
| Tatar (Latin) | tt |
| Teso | teo |
| Tetum | tet |
| Thai | th |
| Thangmi | thf |
| Tok Pisin | tpi |
| Tongan | to |
| Tsonga | ts |
| Tswana | tn |
| Turkish | tr |
| Turkmen (Latin) | tk |
| Tuvan | tyv |
| Udmurt | udm |
| Uighur (Cyrillic) | ug-cyrl |
| Ukrainian | uk |
| Upper Sorbian | hsb |
| Urdu | ur |
| Uyghur (Arabic) | ug |
| Uzbek (Arabic) | uz-arab |
| Uzbek (Cyrillic) | uz-cyrl |
| Uzbek (Latin) | uz |


<!---- Page 69 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Vietnamese
vi
Volapük
Vo
Vunjo
vun
Walser
wae
Welsh
cy
Western Frisian
fy
Wolof
WO
Xhosa
xh
Yucatec Maya
yua
Zapotec
zap
Zarma
dje
Zhuang
za
Zulu
zu
General document
1 Important
With Document Intelligence v4.0:2024-11-30 (GA), the general document model
(prebuilt-document) is being added to layout (prebuilt-layout). To extract key-value
pairs, selection marks, text, tables, and structure from documents, use the following
models:
[] Expand table
Key value pairs
version
Model ID
Layout model with query string
· v4:2024-11-30
prebuilt-
features=keyValuePairs specified.
(GA)
layout
· v3.1:2023-07-31
(GA)

| Language | Code (optional) |
| --- | --- |
| Vietnamese | vi |
| Volapük | Vo |
| Vunjo | vun |
| Walser | wae |
| Welsh | cy |
| Western Frisian | fy :selected: |
| Wolof | WO |
| Xhosa | :selected: xh |
| Yucatec Maya | yua |
| Zapotec | zap |
| Zarma | dje |
| Zhuang | za |
| Zulu | zu |

| Key value pairs | version | Model ID |
| --- | --- | --- |
| Layout model with query string | · v4:2024-11-30 | prebuilt- |
| features=keyValuePairs specified. | (GA) | layout |
|  | · v3.1:2023-07-31 (GA) |  |
|  |  |  |


<!---- Page 70 ---------------------------------------------------------------------------------------------------------------------------------->
Key value pairs
version
Model ID
General document model
· v3.1:2023-07-31
prebuilt-
(GA)
· v3.0:2022-08-31
(GA)
document
Feedback
Was this page helpful?
3 Yes
No
Provide product feedback [ | Get help at Microsoft Q&A

| Key value pairs | version | Model ID |
| --- | --- | --- |
| General document model | · v3.1:2023-07-31 | prebuilt- |
|  | (GA) · v3.0:2022-08-31 (GA) | document |
|  |  |  |


<!---- Page 71 ---------------------------------------------------------------------------------------------------------------------------------->
Language support: prebuilt models
Article · 12/11/2024
This content applies to:
v3.0 (GA)
v4.0 (GA) | Previous versions:
v3.1 (GA)
v2.1 (GA)
Azure AI Document Intelligence models provide multilingual document processing
support. Our language support capabilities enable your users to communicate with your
applications in natural ways and empower global outreach. Prebuilt models enable you
to add intelligent domain-specific document processing to your apps and flows without
having to train and build your own models. The following tables list the available
language and locale support by model and feature:
Business card
1 Important
Starting with Document Intelligence v4.0 preview versions, and going forward, the
business card model (prebuilt-businessCard) is deprecated. To extract data from
business cards, use earlier models.
0
Expand table
Feature
version
Model ID
Business card model
· v3.1:2023-07-31 (GA)
prebuilt-businessCard
· v3.0:2022-08-31 (GA)
· v2.1 (GA)
Bank Statement
Model ID: prebuilt-bankStatement
[ Expand table
Language Locale code
Default
English (United States) en-US
English (United States) en-US

| Feature | version | Model ID |
| --- | --- | --- |
| Business card model | · v3.1:2023-07-31 (GA) | prebuilt-businessCard |
|  | · v3.0:2022-08-31 (GA) |  |
|  | · v2.1 (GA) |  |

| Language Locale code | Default |
| --- | --- |
| English (United States) en-US | English (United States) en-US |


<!---- Page 72 ---------------------------------------------------------------------------------------------------------------------------------->
Contract
Model ID: prebuilt-contract
Expand table
Language Locale code
Default
English (United States) en-US
English (United States) en-US
Check
Model ID: prebuilt-check
" Expand table
Language Locale code
Default
English (United States) en-US
English (United States) en-US
Health insurance card
Model ID: prebuilt-healthInsuranceCard.us
[] Expand table
Language Locale code
Default
English (United States)
English (United States) en-US
ID document
Model ID: prebuilt-idDocument
Supported document types
[] Expand table
Region
Document Types
Worldwide
Passport Book, Passport Card

| Language Locale code | Default |
| --- | --- |
| English (United States) en-US | English (United States) en-US |

| Region | Document Types |
| --- | --- |
| Worldwide | Passport Book, Passport Card |


<!---- Page 73 ---------------------------------------------------------------------------------------------------------------------------------->
Region
Document Types
United States
Driver License, Identification Card, Residency Permit (Green card), Social Security
Card, Military ID
North
America
Driver License, Identification Card, Residency Permit
South
America
Driver License, Identification Card, Residency Permit
Europe
Driver License, Identification Card, Residency Permit
Southeast
Asia
Driver License, Identification Card, Residency Permit
India
Driver License, PAN Card, Aadhaar Card
Australia
Driver License, Photo Card, Key-pass ID (including digital version)
New Zealand
Driver License, Identification Card, Residency Permit
Model ID: prebuilt-idDocument
Supported document types
0
Expand table
Region
Document types
Worldwide
Passport Book, Passport Card
United
Driver License, Identification Card, Residency Permit (Green card), Social Security
States
Card, Military ID
Europe
Driver License, Identification Card, Residency Permit
India
Driver License, PAN Card, Aadhaar Card
Canada
Driver License, Identification Card, Residency Permit (Maple Card)
Australia
Driver License, Photo Card, Key-pass ID (including digital version)
Invoice
Model ID: prebuilt-invoice

| Region | Document Types |
| --- | --- |
| United States | Driver License, Identification Card, Residency Permit (Green card), Social Security Card, Military ID |
| North America | Driver License, Identification Card, Residency Permit |
| South America | Driver License, Identification Card, Residency Permit |
| Europe | Driver License, Identification Card, Residency Permit |
| Southeast Asia | Driver License, Identification Card, Residency Permit |
| India | Driver License, PAN Card, Aadhaar Card |
| Australia | Driver License, Photo Card, Key-pass ID (including digital version) |
| New Zealand | Driver License, Identification Card, Residency Permit |

| Region | Document types |
| --- | --- |
| Worldwide | Passport Book, Passport Card |
| United | Driver License, Identification Card, Residency Permit (Green card), Social Security |
| States | Card, Military ID |
| Europe | Driver License, Identification Card, Residency Permit |
| India | Driver License, PAN Card, Aadhaar Card |
| Canada | Driver License, Identification Card, Residency Permit (Maple Card) |
| Australia | Driver License, Photo Card, Key-pass ID (including digital version) |


<!---- Page 74 ---------------------------------------------------------------------------------------------------------------------------------->
Supported languages
0
Expand table
Languages
Details
· Albanian ( sq)
Albania ( al)
· Arabic (ar )
Arabic (ar)
· Bulgarian (bg)
Bulgaria (bg)
· Chinese (simplified ( zh-
hans ))
China ( zh-hans-cn)
· Chinese (traditional ( zh-
hant ))
Hong Kong SAR ( zh-hant-hk ), Taiwan ( zh-hant-tw)
· Croatian (hr)
Bosnia and Herzegovina (ba), Croatia (hr ), Serbia (rs)
· Czech ( cs)
Czech Republic ( cz)
· Danish ( da)
Denmark ( dk)
· Dutch ( nl)
Netherlands ( nl )
· English ( en )
United States ( us ), Australia ( au), Canada (ca), United
Kingdom (-uk), India (-in)
· Estonian (et )
Estonia ( ee)
· Finnish ( fi)
Finland ( fl )
· French ( fr)
France ( fr)
· German ( de )
Germany (de)
· Greek ( el)
Greece ( el)
· Hebrew ( he )
Hebrew (he)
· Hungarian ( hu)
Hungary ( hu)
· Icelandic ( is )
Iceland ( is)
· Italian (it)
Italy (it)
· Japanese ( ja)
Japan ( ja)
· Korean ( ko )
Korea ( kr)

| Languages | Details |
| --- | --- |
| · Albanian ( sq) | Albania ( al) |
| · Arabic (ar ) | Arabic (ar) |
| · Bulgarian (bg) | Bulgaria (bg) |
| · Chinese (simplified ( zh- hans )) | China ( zh-hans-cn) |
| · Chinese (traditional ( zh- hant )) | Hong Kong SAR ( zh-hant-hk ), Taiwan ( zh-hant-tw) |
| · Croatian (hr) | Bosnia and Herzegovina (ba), Croatia (hr ), Serbia (rs) |
| · Czech ( cs) | Czech Republic ( cz) |
| · Danish ( da) | Denmark ( dk) |
| · Dutch ( nl) | Netherlands ( nl ) |
| · English ( en ) | United States ( us ), Australia ( au), Canada (ca), United Kingdom (-uk), India (-in) |
| · Estonian (et ) | Estonia ( ee) |
| · Finnish ( fi) | Finland ( fl ) |
| · French ( fr) | France ( fr) |
| · German ( de ) | Germany (de) |
| · Greek ( el) | Greece ( el) |
| · Hebrew ( he ) | Hebrew (he) |
| · Hungarian ( hu) | Hungary ( hu) |
| · Icelandic ( is ) | Iceland ( is) |
| · Italian (it) | Italy (it) |
| · Japanese ( ja) | Japan ( ja) |
| · Korean ( ko ) | Korea ( kr) |


<!---- Page 75 ---------------------------------------------------------------------------------------------------------------------------------->
Languages
Details
· Latvian ( 1v)
Latvia ( lv )
· Lithuanian ( It)
Lithuania ( lt )
· Macedonian ( mk )
North Macedonia ( mk )
· Malay (ms )
Malaysia (ms )
· Norwegian ( nb )
Norway (no)
· Polish (pl)
Poland (pl)
· Portuguese (pt )
Portugal (pt ), Brazil (br)
· Romanian ( ro)
Romania (ro)
· Russian (ru)
Russia (ru)
· Serbian (Cyrillic) (sr-
cyrl )
Serbia ( sr)
· Serbian (sr-Latn)
Serbia (latn-rs)
· Slovak ( sk)
Slovakia ( sv)
· Slovenian ( s1)
Slovenia ( sl )
· Spanish ( es )
Spain (es)
· Swedish ( sv )
Sweden ( se)
· Thai (th)
Thailand ( th)
· Turkish ( tr)
Turkey (tr)
· Ukrainian ( uk )
Ukraine ( uk )
· Vietnamese (vi )
Vietnam (vi)
Mortgage
Model ID: prebuilt-mortgage
Expand table

| Languages | Details |
| --- | --- |
| · Latvian ( 1v) | Latvia ( lv ) |
| · Lithuanian ( It) | Lithuania ( lt ) |
| · Macedonian ( mk ) | North Macedonia ( mk ) |
| · Malay (ms ) | Malaysia (ms ) |
| · Norwegian ( nb ) | Norway (no) |
| · Polish (pl) | Poland (pl) |
| · Portuguese (pt ) | Portugal (pt ), Brazil (br) |
| · Romanian ( ro) | Romania (ro) |
| · Russian (ru) | Russia (ru) |
| · Serbian (Cyrillic) (sr- cyrl ) | Serbia ( sr) |
| · Serbian (sr-Latn) | Serbia (latn-rs) |
| · Slovak ( sk) | Slovakia ( sv) |
| · Slovenian ( s1) | Slovenia ( sl ) |
| · Spanish ( es ) | Spain (es) |
| · Swedish ( sv ) | Sweden ( se) |
| · Thai (th) | Thailand ( th) |
| · Turkish ( tr) | Turkey (tr) |
| · Ukrainian ( uk ) | Ukraine ( uk ) |
| · Vietnamese (vi ) | Vietnam (vi) |


<!---- Page 76 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Language Locale code
Default
prebuilt-mortgage-1003
English (United States)
English (United States) en-US
prebuilt-mortgage-1004
English (United States)
English (United States) en-US
prebuilt-mortgage-1005
English (United States)
English (United States) en-us
prebuilt-mortgage-1008
English (United States)
English (United States) en-US
prebuilt-mortgage -. closingDisclosure
English (United States)
English (United States) en-US
Pay stub
Model ID: prebuilt-paystub
[] Expand table
Language Locale code
Default
English (United States) en-US
English (United States) en-US
Receipt
Model ID: prebuilt-receipt
Thermal receipts
0
Expand table
Language name
Language code
Language name
Language code
English
en
Lithuanian
lt
Afrikaans
af
Luxembourgish
lb
Akan
ak
Macedonian
mk
Albanian
sq
Malagasy
mg
Arabic
ar
Malay
ms
Azerbaijani
az
Maltese
mt
Bamanankan
bm
Maori
mi

| Model ID | Language Locale code | Default |
| --- | --- | --- |
| prebuilt-mortgage-1003 | English (United States) | English (United States) en-US |
| prebuilt-mortgage-1004 | English (United States) | English (United States) en-US |
| prebuilt-mortgage-1005 | English (United States) | English (United States) en-us |
| prebuilt-mortgage-1008 | English (United States) | English (United States) en-US |
| prebuilt-mortgage -. closingDisclosure | English (United States) | English (United States) en-US |

| Language Locale code | Default |
| --- | --- |
| English (United States) en-US | English (United States) en-US |

| Language name | Language code | Language name | Language code |
| --- | --- | --- | --- |
| English | en | Lithuanian | lt |
| Afrikaans | af | Luxembourgish | lb |
| Akan | ak | Macedonian | mk |
| Albanian | sq | Malagasy | mg |
| Arabic | ar | Malay | ms |
| Azerbaijani | az | Maltese | mt |
| Bamanankan | bm | Maori | mi |


<!---- Page 77 ---------------------------------------------------------------------------------------------------------------------------------->
Language name
Language code
Language name
Language code
Basque
eu
Marathi
mr
Belarusian
be
Maya, Yucatán
yua
Bhojpuri
bho
Mongolian
mn
Bosnian
bs
Nepali
ne
Bulgarian
bg
Norwegian
no
Catalan
ca
Nyanja
ny
Cebuano
ceb
Oromo
om
Corsican
co
Pashto
ps
Croatian
hr
Persian
fa
Czech
CS
Persian (Dari)
prs
Danish
da
Polish
pl
Dutch
nl
Portuguese
pt
Estonian
et
Punjabi
pa
Faroese
fo
Quechua
qu
Fijian
fj
Romanian
ro
Filipino
fil
Russian
ru
Finnish
fi
Samoan
sm
French
fr
Sanskrit
sa
Galician
gl
Scottish Gaelic
gd
Ganda
lg
Serbian (Cyrillic)
sr-cyrl
German
de
Serbian (Latin)
sr-latn
Greek
el
Sesotho
st
Guarani
gn
Sesotho sa Leboa
nso
Haitian Creole
ht
Shona
sn
Hawaiian
haw
Slovak
sk
Hebrew
he
Slovenian
sl

| Language name | Language code | Language name | Language code |
| --- | --- | --- | --- |
| Basque | eu | Marathi | mr |
| Belarusian | be | Maya, Yucatán | yua |
| Bhojpuri | bho | Mongolian | mn |
| Bosnian | bs | Nepali | ne |
| Bulgarian | bg | Norwegian | no |
| Catalan | ca | Nyanja | ny |
| Cebuano | ceb | Oromo | om |
| Corsican | co | Pashto | ps |
| Croatian | hr | Persian | fa |
| Czech | CS | Persian (Dari) | prs |
| Danish | da | Polish | pl |
| Dutch | nl | Portuguese | pt |
| Estonian | et | Punjabi | pa |
| Faroese | fo | Quechua | qu |
| Fijian | fj | Romanian | ro |
| Filipino | fil | Russian | ru |
| Finnish | fi | Samoan | sm |
| French | fr | Sanskrit | sa |
| Galician | gl | Scottish Gaelic | gd |
| Ganda | lg | Serbian (Cyrillic) | sr-cyrl |
| German | de | Serbian (Latin) | sr-latn |
| Greek | el | Sesotho | st |
| Guarani | gn | Sesotho sa Leboa | nso |
| Haitian Creole | ht | Shona | sn |
| Hawaiian | haw | Slovak | sk |
| Hebrew | he | Slovenian | sl |


<!---- Page 78 ---------------------------------------------------------------------------------------------------------------------------------->
Language name
Language code
Language name
Language code
Hindi
hi
Somali (Latin)
so-latn
Hmong Daw
mww
Spanish
es
Hungarian
hu
Sundanese
su
Icelandic
is
Swedish
SV
Igbo
ig
Tahitian
ty
Iloko
ilo
Tajik
tg
Indonesian
id
Tamil
ta
Irish
ga
Tatar
tt
isiXhosa
xh
Tatar (Latin)
tt-latn
isiZulu
zu
Thai
th
Italian
it
Tongan
to
Japanese
ja
Turkish
tr
Javanese
jv
Turkmen
tk
Kazakh
kk
Ukrainian
uk
Kazakh (Latin)
kk-latn
Upper Sorbian
hsb
Kinyarwanda
rw
Uyghur
ug
Kiswahili
Sw
Uyghur (Arabic)
ug-arab
Korean
ko
Uzbek
uz
Kurdish
ku
Uzbek (Latin)
uz-latn
Kurdish (Latin)
ku-latn
Vietnamese
vi
Kyrgyz
ky
Welsh
cy
Latin
la
Western Frisian
fy
Latvian
lv
Xitsonga
ts
Lingala
ln

| Language name | Language code | Language name | Language code |
| --- | --- | --- | --- |
| Hindi | hi | Somali (Latin) | so-latn |
| Hmong Daw | mww | Spanish | es |
| Hungarian | hu | Sundanese | su |
| Icelandic | is | Swedish | SV |
| Igbo | ig | Tahitian | ty |
| Iloko | ilo | Tajik | tg |
| Indonesian | id | Tamil | ta |
| Irish | ga | Tatar | tt |
| isiXhosa | xh | Tatar (Latin) | tt-latn |
| isiZulu | zu | Thai | th |
| Italian | it | Tongan | to |
| Japanese | ja | Turkish | tr |
| Javanese | jv | Turkmen | tk |
| Kazakh | kk | Ukrainian | uk |
| Kazakh (Latin) | kk-latn | Upper Sorbian | hsb |
| Kinyarwanda | rw | Uyghur | ug |
| Kiswahili | Sw | Uyghur (Arabic) | ug-arab |
| Korean | ko | Uzbek | uz |
| Kurdish | ku | Uzbek (Latin) | uz-latn |
| Kurdish (Latin) | ku-latn | Vietnamese | vi :selected: |
| Kyrgyz | ky :selected: | Welsh | cy :selected: |
| Latin | la | Western Frisian | fy :selected: |
| Latvian | lv | Xitsonga | ts |
| Lingala | ln |  |  |


<!---- Page 79 ---------------------------------------------------------------------------------------------------------------------------------->
Tax documents
" Expand table
Model ID
Language Locale code
Default
prebuilt-tax.us.w2
English (United States)
English (United States)
en-US
prebuilt-tax.us.w4
English (United States)
English (United States)
en-US
prebuilt-tax.us
English (United States)
English (United States)
en-US
prebuilt-tax.us.1099Combo
English (United States)
English (United States)
en-US
prebuilt-tax.us.1098
English (United States)
English (United States)
en-US
prebuilt-tax.us.1095
English (United States)
English (United States)
en-US
prebuilt-tax.us.1098E
English (United States)
English (United States)
en-US
prebuilt-tax.us.1098T
English (United States)
English (United States) en-US
prebuilt-tax.us.1099
English (United States)
English (United States)
en-US
Feedback
Was this page helpful?
Yes
No
Provide product feedback z | Get help at Microsoft Q&A

| Model ID | Language Locale code | Default |
| --- | --- | --- |
| prebuilt-tax.us.w2 | English (United States) | English (United States) en-US |
| prebuilt-tax.us.w4 | English (United States) | English (United States) en-US |
| prebuilt-tax.us | English (United States) | English (United States) en-US |
| prebuilt-tax.us.1099Combo | English (United States) | English (United States) en-US |
| prebuilt-tax.us.1098 | English (United States) | English (United States) en-US |
| prebuilt-tax.us.1095 | English (United States) | English (United States) en-US |
| prebuilt-tax.us.1098E | English (United States) | English (United States) en-US |
| prebuilt-tax.us.1098T | English (United States) | English (United States) en-US |
| prebuilt-tax.us.1099 | English (United States) | English (United States) en-US |


<!---- Page 80 ---------------------------------------------------------------------------------------------------------------------------------->
Language support: custom models
Article · 11/19/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Azure AI Document Intelligence models provide multilingual document processing
support. Our language support capabilities enable your users to communicate with your
applications in natural ways and empower global outreach. Custom models are trained
using your labeled datasets to extract distinct data from structured, semi-structured, and
unstructured documents specific to your use cases. Standalone custom models can be
combined to create composed models. The following tables list the available language
and locale support by model and feature:
Custom classifier
( Expand table
Language
Code (optional)
Afrikaans
af
Albanian
sq
Arabic
ar
Bulgarian
bg
Chinese (Han (Simplified variant))
zh-Hans
Chinese (Han (Traditional variant))
zh-Hant
Croatian
hr
Czech
CS
Danish
da
Dutch
nl
Estonian
et
Finnish
fi
French
fr

| Language | Code (optional) |
| --- | --- |
| Afrikaans | af |
| Albanian | sq |
| Arabic | ar |
| Bulgarian | bg |
| Chinese (Han (Simplified variant)) | zh-Hans |
| Chinese (Han (Traditional variant)) | zh-Hant |
| Croatian | hr |
| Czech | CS |
| Danish | da |
| Dutch | nl |
| Estonian | et |
| Finnish | fi |
| French | fr |


<!---- Page 81 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
German
de
Hebrew
he
Hindi
hi
Hungarian
hu
Indonesian
id
Italian
it
Japanese
ja
Korean
ko
Latvian
lv
Lithuanian
lt
Macedonian
mk
Marathi
mr
Modern Greek (1453-)
el
Nepali (macrolanguage)
ne
Norwegian
no
Panjabi
pa
Persian
fa
Polish
pl
Portuguese
pt
Romanian
rm
Russian
ru
Slovak
sk
Slovenian
sl
Somali (Arabic)
So
Somali (Latin)
so-latn
Spanish
es

| Language | Code (optional) |
| --- | --- |
| German | de |
| Hebrew | he |
| Hindi | hi |
| Hungarian | hu |
| Indonesian | id |
| Italian | it |
| Japanese | ja |
| Korean | ko |
| Latvian | lv |
| Lithuanian | lt |
| Macedonian | mk |
| Marathi | mr |
| Modern Greek (1453-) | el |
| Nepali (macrolanguage) | ne |
| Norwegian | no |
| Panjabi | pa |
| Persian | fa |
| Polish | pl |
| Portuguese | pt |
| Romanian | rm |
| Russian | ru |
| Slovak | sk |
| Slovenian | sl |
| Somali (Arabic) | So |
| Somali (Latin) | so-latn |
| Spanish | es |


<!---- Page 82 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Swahili (macrolanguage)
Sw
Swedish
SV
Tamil
ta
Thai
th
Turkish
tr
Ukrainian
uk
Urdu
ur
Vietnamese
vi
Custom neural
Printed text
The following table lists the supported languages for printed text.
[] Expand table
Language
Code (optional)
Afrikaans
af
Albanian
sq
Arabic
ar
Bulgarian
bg
Chinese Simplified
zh-Hans
Chinese Traditional
zh-Hant
Croatian
hr
Czech
CS
Danish
da
Dutch
nl
English
en

| Language | Code (optional) |
| --- | --- |
| Swahili (macrolanguage) | Sw |
| Swedish | SV |
| Tamil | ta |
| Thai | th |
| Turkish | tr |
| Ukrainian | uk |
| Urdu | ur |
| Vietnamese | vi |

| Language | Code (optional) |
| --- | --- |
| Afrikaans | af |
| Albanian | sq |
| Arabic | ar |
| Bulgarian | bg |
| Chinese Simplified | zh-Hans |
| Chinese Traditional | zh-Hant |
| Croatian | hr |
| Czech | CS |
| Danish | da |
| Dutch | nl |
| English | en |


<!---- Page 83 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Estonian
et
Finnish
fi
French
fr
German
de
Hebrew
he
Hindi
hi
Hungarian
hu
Indonesian
id
Italian
it
Japanese
ja
Korean
ko
Latvian
lv
Lithuanian
lt
Macedonian
mk
Marathi
mr
Modern Greek (1453-)
el
Nepali (macrolanguage)
ne
Norwegian
no
Panjabi
pa
Persian
fa
Polish
pl
Portuguese
pt
Romanian
rm
Russian
ru
Slovak
sk
Slovenian
sl

| Language | Code (optional) |
| --- | --- |
| Estonian | et |
| Finnish | fi |
| French | fr |
| German | de |
| Hebrew | he |
| Hindi | hi |
| Hungarian | hu |
| Indonesian | id |
| Italian | it |
| Japanese | ja |
| Korean | ko |
| Latvian | lv |
| Lithuanian | lt |
| Macedonian | mk |
| Marathi | mr |
| Modern Greek (1453-) | el |
| Nepali (macrolanguage) | ne |
| Norwegian | no |
| Panjabi | pa |
| Persian | fa |
| Polish | pl |
| Portuguese | pt |
| Romanian | rm |
| Russian | ru |
| Slovak | sk |
| Slovenian | sl |


<!---- Page 84 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Somali (Arabic)
So
Somali (Latin)
so-latn
Spanish
es
Swahili (macrolanguage)
SW
Swedish
SV
Tamil
ta
Thai
th
Turkish
tr
Ukrainian
uk
Urdu
ur
Vietnamese
vi
Custom template
Printed
The following table lists the supported languages for printed text.
[] Expand table
Language
Code (optional)
Abaza
abq
Abkhazian
ab
Achinese
ace
Acoli
ach
Adangme
ada
Adyghe
ady
Afar
aa

| Language | Code (optional) |
| --- | --- |
| Somali (Arabic) | So |
| Somali (Latin) | so-latn |
| Spanish | es |
| Swahili (macrolanguage) | SW |
| Swedish | SV |
| Tamil | ta |
| Thai | th |
| Turkish | tr |
| Ukrainian | uk |
| Urdu | ur |
| Vietnamese | vi |

| Language | Code (optional) |
| --- | --- |
| Abaza | abq |
| Abkhazian | ab |
| Achinese | ace |
| Acoli | ach |
| Adangme | ada |
| Adyghe | ady |
| Afar | aa |


<!---- Page 85 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Afrikaans
af
Akan
ak
Albanian
sq
Algonquin
alq
Angika (Devanagari)
anp
Arabic
ar
Asturian
ast
Asu (Tanzania)
asa
Avaric
av
Awadhi-Hindi (Devanagari)
awa
Aymara
ay
Azerbaijani (Latin)
az
Bafia
ksf
Bagheli
bfy
Bambara
bm
Bashkir
ba
Basque
eu
Belarusian (Cyrillic)
be, be-cyrl
Belarusian (Latin)
be, be-latn
Bemba (Zambia)
bem
Bena (Tanzania)
bez
Bhojpuri-Hindi (Devanagari)
bho
Bikol
bik
Bini
bin
Bislama
bi
Bodo (Devanagari)
brx

| Language | Code (optional) |
| --- | --- |
| Afrikaans | af |
| Akan | ak |
| Albanian | sq |
| Algonquin | alq |
| Angika (Devanagari) | anp |
| Arabic | ar |
| Asturian | ast |
| Asu (Tanzania) | asa |
| Avaric | av |
| Awadhi-Hindi (Devanagari) | awa |
| Aymara | ay |
| Azerbaijani (Latin) | az |
| Bafia | ksf |
| Bagheli | bfy |
| Bambara | bm |
| Bashkir | ba |
| Basque | eu |
| Belarusian (Cyrillic) | be, be-cyrl |
| Belarusian (Latin) | be, be-latn |
| Bemba (Zambia) | bem |
| Bena (Tanzania) | bez |
| Bhojpuri-Hindi (Devanagari) | bho |
| Bikol | bik |
| Bini | bin |
| Bislama | bi |
| Bodo (Devanagari) | brx |


<!---- Page 86 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Bosnian (Latin)
bs
Brajbha
bra
Breton
br
Bulgarian
bg
Bundeli
bns
Buryat (Cyrillic)
bua
Catalan
ca
Cebuano
ceb
Chamling
rab
Chamorro
ch
Chechen
ce
Chhattisgarhi (Devanagari)
hne
Chiga
cgg
Chinese Simplified
zh-Hans
Chinese Traditional
zh-Hant
Choctaw
cho
Chukot
ckt
Chuvash
CV
Cornish
kw
Corsican
CO
Cree
cr
Creek
mus
Crimean Tatar (Latin)
crh
Croatian
hr
Crow
cro
Czech
CS

| Language | Code (optional) |
| --- | --- |
| Bosnian (Latin) | bs |
| Brajbha | bra |
| Breton | br |
| Bulgarian | bg |
| Bundeli | bns |
| Buryat (Cyrillic) | bua |
| Catalan | ca |
| Cebuano | ceb |
| Chamling | rab |
| Chamorro | ch |
| Chechen | ce |
| Chhattisgarhi (Devanagari) | hne |
| Chiga | cgg |
| Chinese Simplified | zh-Hans |
| Chinese Traditional | zh-Hant |
| Choctaw | cho |
| Chukot | ckt |
| Chuvash | CV |
| Cornish | kw |
| Corsican | CO |
| Cree | cr |
| Creek | mus |
| Crimean Tatar (Latin) | crh |
| Croatian | hr |
| Crow | cro |
| Czech | CS |


<!---- Page 87 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Danish
da
Dargwa
dar
Dari
prs
Dhimal (Devanagari)
dhi
Dogri (Devanagari)
doi
Duala
dua
Dungan
dng
Dutch
nl
Efik
efi
English
en
Erzya (Cyrillic)
myv
Estonian
et
Faroese
fo
Fijian
fj
Filipino
fil
Finnish
fi
[] Expand table
Language
Code (optional)
Fon
fon
French
fr
Friulian
fur
Ga
gaa
Gagauz (Latin)
gag
Galician
gl
Ganda
lg

| Language | Code (optional) |
| --- | --- |
| Danish | da |
| Dargwa | dar |
| Dari | prs |
| Dhimal (Devanagari) | dhi |
| Dogri (Devanagari) | doi |
| Duala | dua |
| Dungan | dng |
| Dutch | nl |
| Efik | efi |
| English | en |
| Erzya (Cyrillic) | myv |
| Estonian | et |
| Faroese | fo |
| Fijian | fj |
| Filipino | fil |
| Finnish | fi |

| Language | Code (optional) |
| --- | --- |
| Fon | fon |
| French | fr |
| Friulian | fur |
| Ga | gaa |
| Gagauz (Latin) | gag |
| Galician | gl |
| Ganda | lg |


<!---- Page 88 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Gayo
gay
German
de
Gilbertese
gil
Gondi (Devanagari)
gon
Greek
el
Greenlandic
kl
Guarani
gn
Gurung (Devanagari)
gvr
Gusii
guz
Haitian Creole
ht
Halbi (Devanagari)
hlb
Hani
hni
Haryanvi
bgc
Hawaiian
haw
Hebrew
he
Herero
hz
Hiligaynon
hil
Hindi
hi
Hmong Daw (Latin)
mww
Ho(Devanagiri)
hoc
Hungarian
hu
Iban
iba
Icelandic
is
Igbo
ig
Iloko
ilo
Inari Sami
smn

| Language | Code (optional) |
| --- | --- |
| Gayo | gay |
| German | de |
| Gilbertese | gil |
| Gondi (Devanagari) | gon |
| Greek | el |
| Greenlandic | kl |
| Guarani | gn |
| Gurung (Devanagari) | gvr |
| Gusii | guz |
| Haitian Creole | ht |
| Halbi (Devanagari) | hlb |
| Hani | hni |
| Haryanvi | bgc |
| Hawaiian | haw |
| Hebrew | he |
| Herero | hz |
| Hiligaynon | hil |
| Hindi | hi |
| Hmong Daw (Latin) | mww |
| Ho(Devanagiri) | hoc |
| Hungarian | hu |
| Iban | iba |
| Icelandic | is |
| Igbo | ig |
| Iloko | ilo |
| Inari Sami | smn |


<!---- Page 89 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Indonesian
id
Ingush
inh
Interlingua
ia
Inuktitut (Latin)
iu
Irish
ga
Italian
it
Japanese
ja
Jaunsari (Devanagari)
Jns
Javanese
jv
Jola-Fonyi
dyo
Kabardian
kbd
Kabuverdianu
kea
Kachin (Latin)
kac
Kalenjin
kln
Kalmyk
xal
Kangri (Devanagari)
xnr
Kanuri
kr
Karachay-Balkar
krc
Kara-Kalpak (Cyrillic)
kaa-cyrl
Kara-Kalpak (Latin)
kaa
Kashubian
csb
Kazakh (Cyrillic)
kk-cyrl
Kazakh (Latin)
kk-latn
Khakas
kjh
Khaling
klr
Khasi
kha

| Language | Code (optional) |
| --- | --- |
| Indonesian | id |
| Ingush | inh |
| Interlingua | ia |
| Inuktitut (Latin) | iu |
| Irish | ga |
| Italian | it |
| Japanese | ja |
| Jaunsari (Devanagari) | Jns |
| Javanese | jv |
| Jola-Fonyi | dyo |
| Kabardian | kbd |
| Kabuverdianu | kea |
| Kachin (Latin) | kac |
| Kalenjin | kln |
| Kalmyk | xal |
| Kangri (Devanagari) | xnr |
| Kanuri | kr |
| Karachay-Balkar | krc |
| Kara-Kalpak (Cyrillic) | kaa-cyrl |
| Kara-Kalpak (Latin) | kaa |
| Kashubian | csb |
| Kazakh (Cyrillic) | kk-cyrl |
| Kazakh (Latin) | kk-latn |
| Khakas | kjh |
| Khaling | klr |
| Khasi | kha |


<!---- Page 90 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
K'iche'
quc
Kikuyu
ki
Kildin Sami
sjd
Kinyarwanda
rw
Komi
kv
Kongo
kg
Korean
ko
Korku
kfq
Koryak
kpy
Kosraean
kos
Kpelle
kpe
Kuanyama
kj
Kumyk (Cyrillic)
kum
Kurdish (Arabic)
ku-arab
Kurdish (Latin)
ku-latn
Kurukh (Devanagari)
kru
Kyrgyz (Cyrillic)
ky
Lak
lbe
Lakota
lkt
[ Expand table
Language
Code (optional)
Latin
la
Latvian
lv
Lezghian
lex
Lingala
ln

| Language | Code (optional) |
| --- | --- |
| K'iche' | quc |
| Kikuyu | ki |
| Kildin Sami | sjd |
| Kinyarwanda | rw |
| Komi | kv |
| Kongo | kg |
| Korean | ko |
| Korku | kfq |
| Koryak | kpy |
| Kosraean | kos |
| Kpelle | kpe |
| Kuanyama | kj |
| Kumyk (Cyrillic) | kum |
| Kurdish (Arabic) | ku-arab |
| Kurdish (Latin) | ku-latn |
| Kurukh (Devanagari) | kru |
| Kyrgyz (Cyrillic) | ky |
| Lak | lbe |
| Lakota | lkt |

| Language | Code (optional) |
| --- | --- |
| Latin | la |
| Latvian | lv |
| Lezghian | lex |
| Lingala | ln |


<!---- Page 91 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Lithuanian
lt
Lower Sorbian
dsb
Lozi
loz
Lule Sami
smj
Luo (Kenya and Tanzania)
luo
Luxembourgish
lb
Luyia
luy
Macedonian
mk
Machame
jmc
Madurese
mad
Mahasu Pahari (Devanagari)
bfz
Makhuwa-Meetto
mgh
Makonde
kde
Malagasy
mg
Malay (Latin)
ms
Maltese
mt
Malto (Devanagari)
kmj
Mandinka
mnk
Manx
gv
Maori
mi
Mapudungun
arn
Marathi
mr
Mari (Russia)
chm
Masai
mas
Mende (Sierra Leone)
men
Meru
mer

| Language | Code (optional) |
| --- | --- |
| Lithuanian | lt |
| Lower Sorbian | dsb |
| Lozi | loz |
| Lule Sami | smj |
| Luo (Kenya and Tanzania) | luo |
| Luxembourgish | lb |
| Luyia | luy |
| Macedonian | mk |
| Machame | jmc |
| Madurese | mad |
| Mahasu Pahari (Devanagari) | bfz |
| Makhuwa-Meetto | mgh |
| Makonde | kde |
| Malagasy | mg |
| Malay (Latin) | ms |
| Maltese | mt |
| Malto (Devanagari) | kmj |
| Mandinka | mnk |
| Manx | gv |
| Maori | mi |
| Mapudungun | arn |
| Marathi | mr |
| Mari (Russia) | chm |
| Masai | mas |
| Mende (Sierra Leone) | men |
| Meru | mer |


<!---- Page 92 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Meta'
mgo
Minangkabau
min
Mohawk
moh
Mongolian (Cyrillic)
mn
Mongondow
mog
Montenegrin (Cyrillic)
cnr-cyrl
Montenegrin (Latin)
cnr-latn
Morisyen
mfe
Mundang
mua
Nahuatl
nah
Navajo
nv
Ndonga
ng
Neapolitan
nap
Nepali
ne
Ngomba
jgo
Niuean
niu
Nogay
nog
North Ndebele
nd
Northern Sami (Latin)
sme
Norwegian
no
Nyanja
ny
Nyankole
nyn
Nzima
nzi
Occitan
oc
Ojibwa
oj
Oromo
om

| Language | Code (optional) |
| --- | --- |
| Meta' | mgo |
| Minangkabau | min |
| Mohawk | moh |
| Mongolian (Cyrillic) | mn |
| Mongondow | mog |
| Montenegrin (Cyrillic) | cnr-cyrl |
| Montenegrin (Latin) | cnr-latn |
| Morisyen | mfe |
| Mundang | mua |
| Nahuatl | nah |
| Navajo | nv |
| Ndonga | ng |
| Neapolitan | nap |
| Nepali | ne |
| Ngomba | jgo |
| Niuean | niu |
| Nogay | nog |
| North Ndebele | nd |
| Northern Sami (Latin) | sme |
| Norwegian | no |
| Nyanja | ny |
| Nyankole | nyn |
| Nzima | nzi |
| Occitan | oc |
| Ojibwa | oj |
| Oromo | om |


<!---- Page 93 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Ossetic
OS
Pampanga
pam
Pangasinan
pag
Papiamento
pap
Pashto
ps
Pedi
nso
Persian
fa
Polish
pl
Portuguese
pt
Punjabi (Arabic)
pa
Quechua
qu
Ripuarian
ksh
Romanian
ro
Romansh
rm
Rundi
rn
Russian
ru
Rwa
rwk
Sadri (Devanagari)
sck
Sakha
sah
Samburu
saq
Samoan (Latin)
sm
Sango
sg
[] Expand table
Language
Code (optional)
Sangu (Gabon)
snq

| Language | Code (optional) |
| --- | --- |
| Ossetic | OS |
| Pampanga | pam |
| Pangasinan | pag |
| Papiamento | pap |
| Pashto | ps |
| Pedi | nso |
| Persian | fa |
| Polish | pl |
| Portuguese | pt |
| Punjabi (Arabic) | pa |
| Quechua | qu |
| Ripuarian | ksh |
| Romanian | ro |
| Romansh | rm |
| Rundi | rn |
| Russian | ru |
| Rwa | rwk |
| Sadri (Devanagari) | sck |
| Sakha | sah |
| Samburu | saq |
| Samoan (Latin) | sm |
| Sango | sg |


<!---- Page 94 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Sanskrit (Devanagari)
sa
Santali(Devanagiri)
sat
Scots
SCO
Scottish Gaelic
gd
Sena
seh
Serbian (Cyrillic)
sr-cyrl
Serbian (Latin)
sr, sr-latn
Shambala
ksb
Shona
sn
Siksika
bla
Sirmauri (Devanagari)
srx
Skolt Sami
sms
Slovak
sk
Slovenian
sl
Soga
xog
Somali (Arabic)
SO
Somali (Latin)
so-latn
Songhai
son
South Ndebele
nr
Southern Altai
alt
Southern Sami
sma
Southern Sotho
st
Spanish
es
Sundanese
su
Swahili (Latin)
SW
Swati
SS

| Language | Code (optional) |
| --- | --- |
| Sanskrit (Devanagari) | sa |
| Santali(Devanagiri) | sat |
| Scots | SCO |
| Scottish Gaelic | gd |
| Sena | seh |
| Serbian (Cyrillic) | sr-cyrl |
| Serbian (Latin) | sr, sr-latn |
| Shambala | ksb |
| Shona | sn |
| Siksika | bla |
| Sirmauri (Devanagari) | srx |
| Skolt Sami | sms |
| Slovak | sk |
| Slovenian | sl |
| Soga | xog |
| Somali (Arabic) | SO |
| Somali (Latin) | so-latn |
| Songhai | son |
| South Ndebele | nr |
| Southern Altai | alt |
| Southern Sami | sma |
| Southern Sotho | st |
| Spanish | es |
| Sundanese | su |
| Swahili (Latin) | SW |
| Swati | SS |


<!---- Page 95 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Swedish
SV
Tabassaran
tab
Tachelhit
shi
Tahitian
ty
Taita
dav
Tajik (Cyrillic)
tg
Tamil
ta
Tatar (Cyrillic)
tt-cyrl
Tatar (Latin)
tt
Teso
teo
Tetum
tet
Thai
th
Thangmi
thf
Tok Pisin
tpi
Tongan
to
Tsonga
ts
Tswana
tn
Turkish
tr
Turkmen (Latin)
tk
Tuvan
tyv
Udmurt
udm
Uighur (Cyrillic)
ug-cyrl
Ukrainian
uk
Upper Sorbian
hsb
Urdu
ur
Uyghur (Arabic)
ug

| Language | Code (optional) |
| --- | --- |
| Swedish | SV |
| Tabassaran | tab |
| Tachelhit | shi |
| Tahitian | ty |
| Taita | dav |
| Tajik (Cyrillic) | tg |
| Tamil | ta |
| Tatar (Cyrillic) | tt-cyrl |
| Tatar (Latin) | tt |
| Teso | teo |
| Tetum | tet |
| Thai | th |
| Thangmi | thf |
| Tok Pisin | tpi |
| Tongan | to |
| Tsonga | ts |
| Tswana | tn |
| Turkish | tr |
| Turkmen (Latin) | tk |
| Tuvan | tyv |
| Udmurt | udm |
| Uighur (Cyrillic) | ug-cyrl |
| Ukrainian | uk |
| Upper Sorbian | hsb |
| Urdu | ur |
| Uyghur (Arabic) | ug |


<!---- Page 96 ---------------------------------------------------------------------------------------------------------------------------------->
Language
Code (optional)
Uzbek (Arabic)
uz-arab
Uzbek (Cyrillic)
uz-cyrl
Uzbek (Latin)
uz
Vietnamese
vi
Volapük
VO
Vunjo
vun
Walser
wae
Welsh
cy
Western Frisian
fy
Wolof
WO
Xhosa
xh
Yucatec Maya
yua
Zapotec
zap
Zarma
dje
Zhuang
za
Zulu
zu
Feedback
Was this page helpful?
Yes
P
No
Provide product feedback | Get help at Microsoft Q&A

| Language | Code (optional) |
| --- | --- |
| Uzbek (Arabic) | uz-arab |
| Uzbek (Cyrillic) | uz-cyrl |
| Uzbek (Latin) | uz |
| Vietnamese | vi |
| Volapük | VO |
| Vunjo | vun |
| Walser | wae |
| Welsh | cy |
| Western Frisian | fy |
| Wolof | WO |
| Xhosa | xh |
| Yucatec Maya | yua |
| Zapotec | zap |
| Zarma | dje |
| Zhuang | za |
| Zulu | zu |


<!---- Page 97 ---------------------------------------------------------------------------------------------------------------------------------->
Service quotas and limits
Article · 04/04/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
This article contains both a quick reference and detailed description of Azure AI Document
Intelligence service Quotas and Limits for all pricing tiers [7. It also contains some best practices
to avoid request throttling.
Model usage
[ Expand table
Document types supported
Read
Layout
Prebuilt
models
Custom
models
Add-on
capabilities
PDF
Images: JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office: DOCX, PPTX,
XLS
x
x
×
V = supported X = Not supported
For Document Intelligence v4.0 2024-11-30 (GA) supports page and line features with the
following restrictions:
· Angle, width/height, and unit aren't supported.
· For each object detected, bounding polygon or bounding regions aren't supported.
· The lines object isn't supported.
V Document Intelligence SDKs
V Document Intelligence REST API
V Document Intelligence Studio v3.0
[] Expand table
Quota
Free (F0) 1
Standard (S0)
Analyze transactions Per Second limit
1
15 (default value)
Adjustable
No
Yes 2

| Document types supported | Read | Layout | Prebuilt models | Custom models | Add-on capabilities |
| --- | --- | --- | --- | --- | --- |
| PDF | :selected: | :selected: | :selected: | :selected: | :selected: |
| Images: JPEG/JPG, PNG, BMP, TIFF, HEIF | :selected: | :selected: | :selected: | :selected: | :selected: |
| Microsoft Office: DOCX, PPTX, XLS | :selected: | :selected: | x :selected: | x :selected: | × :selected: |

| Quota | Free (F0) 1 | Standard (S0) |
| --- | --- | --- |
| Analyze transactions Per Second limit | 1 | 15 (default value) |
| Adjustable | No | Yes 2 |


<!---- Page 98 ---------------------------------------------------------------------------------------------------------------------------------->
Quota
Free (F0) 1
Standard (S0)
Get operations Per Second limit
1
50 (default value)
Adjustable
No
Yes 2
Model management operations Per Second limit
1
5 (default value)
Adjustable
No
Yes 2
List operations Per Second limit
1
10 (default value)
Adjustable
No
Yes 2
Max document size
4 MB
500 MB
Adjustable
No
No
Max number of pages (Analysis)
2
2000
Adjustable
No
No
Max size of labels file
10 MB
10 MB
Adjustable
No
No
Max size of OCR json response
500 MB
500 MB
Adjustable
No
No
Max number of Template models
500
5000
Adjustable
No
No
Max number of Neural models
100
500
Adjustable
No
No
Custom model usage
V Custom template model
V Custom neural model
V Composed classification models
V Composed custom models
[ Expand table

| Quota | Free (F0) 1 | Standard (S0) |
| --- | --- | --- |
| Get operations Per Second limit | 1 | 50 (default value) |
| Adjustable | No | Yes 2 |
| Model management operations Per Second limit | 1 | 5 (default value) |
| Adjustable | No | Yes 2 |
| List operations Per Second limit | 1 | 10 (default value) |
| Adjustable | No | Yes 2 |
| Max document size | 4 MB | 500 MB |
| Adjustable | No | No |
| Max number of pages (Analysis) | 2 | 2000 |
| Adjustable | No | No |
| Max size of labels file | 10 MB | 10 MB |
| Adjustable | No | No |
| Max size of OCR json response | 500 MB | 500 MB |
| Adjustable | No | No |
| Max number of Template models | 500 | 5000 |
| Adjustable | No | No |
| Max number of Neural models | 100 | 500 |
| Adjustable | No | No |


<!---- Page 99 ---------------------------------------------------------------------------------------------------------------------------------->
Quota
Free (F0) 1
Standard (S0)
Compose Model limit
5
500 (default value)
Adjustable
No
No
Training dataset size * Neural and
Generative
1 GB 3
1 GB (default value)
Adjustable
No
No
Training dataset size * Template
50 MB 4
50 MB (default value)
Adjustable
No
No
Max number of pages (Training) *
Template
500
500 (default value)
Adjustable
No
No
Max number of pages (Training) *
Neural and Generative
50,000
50,000 (default value)
Adjustable
No
No
Custom neural model train
10 hours per
month 5
no limit (pay by the hour), start with 10
free hours each month
Adjustable
No
Yes 3
Max number of pages (Training) *
Classifier
10,000
10,000 (default value)
Adjustable
No
No
Max number of document types
(classes) * Classifier
500
500 (default value)
Adjustable
No
No
Training dataset size * Classifier
1GB
2GB (default value)
Adjustable
No
No
Min number of samples per class *
Classifier
5
5 (default value)
Adjustable
No
No
1 For Free (F0) pricing tier see also monthly allowances at the pricing page .
2 See best practices, and adjustment instructions.
3 Neural models training count is reset every calendar month. Open a support request to

| Quota | Free (F0) 1 | Standard (S0) |
| --- | --- | --- |
| Compose Model limit | 5 | :unselected: 500 (default value) |
| Adjustable | :unselected: No | :unselected: No |
| Training dataset size * Neural and Generative | 1 GB 3 | 1 GB (default value) |
| Adjustable | :unselected: No | :unselected: No |
| Training dataset size * Template | :unselected: 50 MB 4 | :unselected: 50 MB (default value) |
| Adjustable | :unselected: No | :unselected: No |
| Max number of pages (Training) * Template | :unselected: 500 | :unselected: 500 (default value) |
| Adjustable | :unselected: No | :unselected: No |
| Max number of pages (Training) * Neural and Generative | :unselected: 50,000 | :unselected: 50,000 (default value) |
| Adjustable | :unselected: No | :unselected: No |
| Custom neural model train | 10 hours per :unselected: month 5 | no limit (pay by the hour), start with 10 free hours each month |
| Adjustable | :unselected: No | Yes 3 |
| Max number of pages (Training) * Classifier | :unselected: 10,000 | :unselected: 10,000 (default value) |
| Adjustable | :unselected: No | :unselected: No |
| Max number of document types (classes) * Classifier | :unselected: 500 | :unselected: 500 (default value) |
| Adjustable | :unselected: No | :unselected: No |
| Training dataset size * Classifier | :unselected: 1GB | :unselected: 2GB (default value) |
| Adjustable | :unselected: No | :unselected: No |
| Min number of samples per class * Classifier | :unselected: 5 | :unselected: 5 (default value) |
| Adjustable | :unselected: No | :unselected: No |


<!---- Page 100 ---------------------------------------------------------------------------------------------------------------------------------->
increase the monthly training limit. Starting with the v4.0 API, training requests over 20
requests in a calendar month are billed on the training tier. See pricing for details.
4 This limit applies to all documents found in your training dataset folder prior to any
labeling-related updates.
5 This limit applies for v 4.0 (2024-11-30 GA) custom neural models only. Starting from
V
4.0, we support training larger documents for longer durations (up to 10 hours for free,
and incurring charges after). For more information, please refer to custom neural model
page.
Detailed description, Quota adjustment, and best
practices
The default limits can be extended by requesting an increase via a support ticket. Before
requesting a quota increase (where applicable), ensure that it's necessary. Document
Intelligence service uses autoscaling to bring the required computational resources on-demand,
keep the customer costs low, and deprovision unused resources by not maintaining an
excessive amount of hardware capacity.
If your application returns Response Code 429 (Too many requests) you are over the threshold
for one or more of the transactions per second limits (TPS):
· Analyze transactions Per Second limit The TPS for submitting analyze requests (POST)
. Get operations Per Second limit The TPS for polling for results on analyze operations
(GET)
. Model management operations Per Second limit Operations related to model
management like build/train and copy.
. List operations Per Second limit Operations related to listing models, operations.
General best practices to mitigate throttling during
autoscaling
To minimize issues related to throttling (Response Code 429), we recommend using the
following techniques:
· Implement retry logic in your application
. Avoid sharp changes in the workload. Increase the workload gradually
Example. Your application is using Document Intelligence and your current workload is 10
TPS (transactions per second). The next second you increase the load to 40 TPS. The result


<!---- Page 101 ---------------------------------------------------------------------------------------------------------------------------------->
is a 429 response code for some requests as you are over the 15 TPS limit for submitting
analyze operations. You could either back off the processing to stay under the 15 TPS or
request an increase on the TPS to support your higher volumes.
The next sections describe specific cases of adjusting quotas. Jump to Document Intelligence:
increasing concurrent request limit
Increasing transactions per second request limit
By default the number of transactions per second is limited to 15 transactions per second for a
Document Intelligence resource. For the Standard pricing tier, this amount can be increased.
Before submitting the request, ensure you're familiar with the material in this section and
aware of these best practices.
The fist step would be to enable auto scaling. Follow this document to enable auto scaling on
your resource * enable auto scaling. With auto scaling enabled your resource can continue to
accept requests over the TPS limits configured if there's capacity on the service. It can still
result in request throttled.
Increasing the Concurrent Request limit does not directly affect your costs. Document
Intelligence service uses "Pay only for what you use" model. The limit defines how high the
Service can scale before it starts throttle your requests.
The existing value of different request limit categories is available via Azure portal, under the
monitoring tab on the resource overview blade.
Create and submit support request for TPS increase
Initiate the increase of transactions per second(TPS) limit for your resource by submitting the
Support Request:
· Sign in to the Azure portal
. Select the Document Intelligence Resource for which you would like to increase the TPS
limit
· Select -New support request- (-Support + troubleshooting- group). A new window
appears with autopopulated information about your Azure Subscription and Azure
Resource
· Enter -Summary- (like "Increase Document Intelligence TPS limit")
· Select "Quota or usage validation" for problem type field.
· Select -Next: Solutions-
. Proceed further with the request creation
. Enter the following information in the -Description- field, under the Details tab:


<!---- Page 102 ---------------------------------------------------------------------------------------------------------------------------------->
o a note, that the request is about Document Intelligence quota.
o Provide a TPS expectation you would like to scale to meet. While TPS increases are
free, you should only request a TPS that is reasonable for your workload.
o Azure resource information
o Complete entering the required information and select -Create- button in -Review +
create- tab
o Note the support request number in Azure portal notifications. Look for Support to
contact you shortly for further processing.
Example of a workload pattern best practice
This example presents the approach we recommend following to mitigate possible request
throttling due to Autoscaling being in progress. It isn't an exact recipe, but merely a template
we invite to follow and adjust as necessary.
Let us suppose that a Document Intelligence resource has the default limit set. Start the
workload to submit your analyze requests. If you find that you're seeing frequent throttling
with response code 429 when checking for completion, start by implementing an exponential
backoff on the GET analyze response request. By using a progressively longer wait time
between retries for consecutive error responses, for example a 2-5-13-34 pattern of delays
between requests. In general, we recommended not calling the get analyze response more
than once every 2 seconds for a corresponding POST request. The analyze response also
contains a retry-after header that indicates how long you should wait in seconds before
checking for completion of that request.
If you find that you're being throttled on the number of POST requests for documents being
submitted, consider adding a delay between the requests. If your workload requires a higher
degree of concurrent processing, you then need to create a support request to increase your
service limits on transactions per second.
Generally, we recommended testing the workload and the workload patterns before going to
production.
Next steps
Learn about error codes and troubleshooting


<!---- Page 103 ---------------------------------------------------------------------------------------------------------------------------------->
SDK target: REST API v4.0 (GA)
Article · 02/11/2025
REST API version 2024-11-30 GA
Azure AI Document Intelligence is a cloud service that uses machine learning to analyze text
and structured data from documents. The Document Intelligence software development kit
(SDK) is a set of libraries and tools that enable you to easily integrate Document Intelligence
models and capabilities into your applications. Document Intelligence SDK is available across
platforms in C#/.NET, Java, JavaScript, and Python programming languages.
Supported programming languages
Document Intelligence SDK supports the following languages and platforms:
Expand table
Language - Document Intelligence
SDK version
Package
Supported API version
Platform
support
.NET/C# -> 1.0.0 (GA)
NuGetlzz
2024-11-30 (GA)
Windows,
macOS, Linux,
Docker
Java - 1.0.0 (GA
Maven
repository
2024-11-30 (GA)
Windows,
macOS, Linux
JavaScript - 1.0.0 (GA)
npm z
2024-11-30 (GA)
Browser,
Windows,
macOS, Linux EZ
Python - 1.0.0 (GA)
PyPl z
2024-11-30 (GA)
Windows,
macOS, Linux
For more information on other SDK versions, see:
· 2023-07-31 v3.1 (GA)
· 2022-08-31 v3.0 (GA)
· v2.1 (GA)
Supported Clients
The following tables present the correlation between each SDK version the supported API
versions of the Document Intelligence service.

| Language - Document Intelligence SDK version | Package | Supported API version | Platform support |
| --- | --- | --- | --- |
| .NET/C# -> 1.0.0 (GA) | NuGetlzz | 2024-11-30 (GA) | Windows, macOS, Linux, Docker |
| Java - 1.0.0 (GA | Maven repository | 2024-11-30 (GA) | Windows, macOS, Linux |
| JavaScript - 1.0.0 (GA) | npm z | 2024-11-30 (GA) | Browser, Windows, macOS, Linux EZ |
| Python - 1.0.0 (GA) | PyPl z | 2024-11-30 (GA) | Windows, macOS, Linux |


<!---- Page 104 ---------------------------------------------------------------------------------------------------------------------------------->
C#/.NET
[] Expand table
Language
SDK
alias
API version (default)
Supported clients
.NET/C#
v4.0
2024-11-30 GA
DocumentIntelligenceClient
1.0.0 (GA)
(GA)
DocumentIntelligenceAdministrationClient
.NET/C#
v3.1
2023-07-31
DocumentAnalysisClient
4.1.0
latest
(GA)
DocumentModelAdministrationClient
.NET/C#
v3.0
2022-08-31
DocumentAnalysisClient
4.0.0
(GA)
DocumentModelAdministrationClient
.NET/C#
v2.1
v2.1
FormRecognizerClient
3.1.x
Form Training Client
.NET/C#
v2.0
v2.0
FormRecognizerClient
3.0.x
Form TrainingClient
Use Document Intelligence SDK in your
applications
The Document Intelligence SDK enables the use and management of the Document
Intelligence service in your application. The SDK builds on the underlying Document
Intelligence REST API allowing you to easily use those APIs within your programming language
paradigm. Here's how you use the Document Intelligence SDK for your preferred language:
1. Install the SDK client library
C#/.NET
.NET CLI
dotnet add package Azure.AI.DocumentIntelligence -Version 1.0.0
PowerShell

| Language | SDK alias | API version (default) | Supported clients |
| --- | --- | --- | --- |
| .NET/C# | v4.0 | 2024-11-30 GA | DocumentIntelligenceClient |
| 1.0.0 (GA) | (GA) |  | DocumentIntelligenceAdministrationClient |
| .NET/C# | v3.1 | 2023-07-31 | DocumentAnalysisClient |
| 4.1.0 | latest (GA) |  | DocumentModelAdministrationClient |
| .NET/C# | v3.0 | 2022-08-31 | DocumentAnalysisClient |
| 4.0.0 | (GA) |  | DocumentModelAdministrationClient |
| .NET/C# | v2.1 | v2.1 | FormRecognizerClient |
| 3.1.x |  |  | Form Training Client |
| .NET/C# | v2.0 | v2.0 | FormRecognizerClient |
| 3.0.x |  |  | Form TrainingClient |


<!---- Page 105 ---------------------------------------------------------------------------------------------------------------------------------->
Install-Package Azure.AI.DocumentIntelligence -Version 1.0.0
2. Import the SDK client library into your application
C#/.NET
C#
using Azure;
using Azure.AI. DocumentIntelligence;
3. Set up authentication
There are two supported methods for authentication:
· Use a Document Intelligence API key with AzureKeyCredential from azure.core.credentials.
. Use a token credential from azure-identity to authenticate with Microsoft Entra ID.
Use your API key
Here's where to find your Document Intelligence API key in the Azure portal:


<!---- Page 106 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
& Access control (IAM)
Tags
Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
C
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
> Automation
Ls
> Help
1 Important
We recommend Microsoft Entra ID authentication with managed identities for Azure
resources to avoid storing credentials with your applications that run in the cloud.
If you use an API key, store it securely somewhere else, such as in Azure Key Vault. Don't
include the API key directly in your code, and never post it publicly.
For more information about AI services security, see Authenticate requests to Azure AI
services.
C#/.NET
C#
//set ><your-endpoint>` and `<your-key>> variables with the values from the
Azure portal to create your AzureKeyCredential` and
`DocumentIntelligenceClient' instance
string key = "<your-key>";
string endpoint = "<your-endpoint>";
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient(new
Uri(endpoint), new AzureKeyCredential(key));


<!---- Page 107 ---------------------------------------------------------------------------------------------------------------------------------->
Use a Microsoft Entra token credential
1 Note
Regional endpoints don't support Microsoft Entra authentication. Create a custom
subdomain for your resource in order to use this type of authentication.
Authorization is easiest using the DefaultAzureCredential. It provides a default token
credential, based upon the running environment, capable of handling most Azure
authentication scenarios.
C#/.NET
Here's how to acquire and use the DefaultAzureCredential for .NET applications:
1. Install the Azure Identity library for .NET:
Console
dotnet add package Azure. Identity
PowerShell
Install-Package Azure. Identity
2. Register a Microsoft Entra application and create a new service principal.
3. Grant access to Document Intelligence by assigning the Cognitive Services User
role to your service principal.
4. Set the values of the client ID, tenant ID, and client secret in the Microsoft Entra
application as environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, and
AZURE_CLIENT_SECRET, respectively.
5. Create your DocumentIntelligenceClient instance including the
DefaultAzureCredential :
C#
string endpoint = "<your-endpoint>";
var client = new DocumentIntelligenceClient(new Uri(endpoint), new
DefaultAzureCredential());


<!---- Page 108 ---------------------------------------------------------------------------------------------------------------------------------->
For more information, see Authenticate the client .
4. Build your application
Create a client object to interact with the Document Intelligence SDK, and then call methods
on that client object to interact with the service. The SDKs provide both synchronous and
asynchronous methods. For more insight, try a quickstart in a language of your choice.
Help options
The Microsoft Q&A and Stack Overflow forums are available for the developer community to
ask and answer questions about Azure AI Document Intelligence and other services. Microsoft
monitors the forums and replies to questions that the community has yet to answer. To make
sure, use the following tags so that we see your question.
· Microsoft Q&A: Azure AI Document Intelligence.
· Stack Overflow: azure-ai-document-intelligence.
Next steps
Explore
Document Intelligence REST API 2023-10-31-rest
operations.


<!---- Page 109 ---------------------------------------------------------------------------------------------------------------------------------->
SDK target: REST API v3.1 (GA)
Article · 02/10/2025
REST API version 2023-07-31 (GA)
Azure AI Document Intelligence is a cloud service that uses machine learning to analyze text
and structured data from documents. The Document Intelligence software development kit
(SDK) is a set of libraries and tools that enable you to easily integrate Document Intelligence
models and capabilities into your applications. Document Intelligence SDK is available across
platforms in C#/.NET, Java, JavaScript, and Python programming languages.
Supported programming languages
Document Intelligence SDK supports the following languages and platforms:
"
Expand table
Language - Document Intelligence
SDK version
Package
Supported API version
Platform
support
.NET/C# - latest (GA)
NuGetlzz
2023-07-31 (GA)
Java + latest (GA)
Maven
repository
2023-07-31 (GA)
Windows,
macOS, Linux
JavaScript > latest (GA)
npm
2023-07-31 (GA)
Browser,
Windows,
macOS, Linux EZ
Python + latest (GA)
PyPI EZ
2023-07-31 (GA)
Windows,
macOS, Linux
For more information on other SDK versions, see:
· 2024-02-29 v4.0 (preview)
· 2022-08-31 v3.0 (GA)
· v2.1 (GA)
Supported Clients
The following tables present the correlation between each SDK version the supported API
versions of the Document Intelligence service.

| Language - Document Intelligence SDK version | Package | Supported API version | Platform support |
| --- | --- | --- | --- |
| .NET/C# - latest (GA) | NuGetlzz | 2023-07-31 (GA) |  |
| Java + latest (GA) | Maven repository | 2023-07-31 (GA) | Windows, macOS, Linux |
| JavaScript > latest (GA) | npm | 2023-07-31 (GA) | Browser, Windows, macOS, Linux EZ |
| Python + latest (GA) | PyPI EZ | 2023-07-31 (GA) | Windows, macOS, Linux |


<!---- Page 110 ---------------------------------------------------------------------------------------------------------------------------------->
C#/.NET
[] Expand table
Language
SDK
version
API version (default)
Supported clients
.NET/C#
v3.1 latest
2023-07-31
DocumentAnalysisClient
4.1.0
(GA)
DocumentModelAdministrationClient
.NET/C#
v3.0 (GA)
2022-08-31
DocumentAnalysisClient
4.0.0
DocumentModelAdministrationClient
.NET/C#
v2.1
v2.1
FormRecognizerClient
3.1.x
FormTrainingClient
.NET/C#
v2.0
v2.0
FormRecognizerClient
3.0.x
Form Training Client
Use Document Intelligence SDK in your
applications
The Document Intelligence SDK enables the use and management of the Document
Intelligence service in your application. The SDK builds on the underlying Document
Intelligence REST API allowing you to easily use those APIs within your programming language
paradigm. Here's how you use the Document Intelligence SDK for your preferred language:
1. Install the SDK client library
C#/.NET
.NET CLI
dotnet add package Azure.AI.FormRecognizer -- version 4.1.0
PowerShell
Install-Package Azure.AI. FormRecognizer -Version 4.1.0

| Language | SDK version | API version (default) | Supported clients |
| --- | --- | --- | --- |
| .NET/C# | v3.1 latest | 2023-07-31 | DocumentAnalysisClient |
| 4.1.0 | (GA) |  | DocumentModelAdministrationClient |
| .NET/C# | v3.0 (GA) | 2022-08-31 | DocumentAnalysisClient |
| 4.0.0 |  |  | DocumentModelAdministrationClient |
| .NET/C# | v2.1 | v2.1 | FormRecognizerClient |
| 3.1.x |  |  | FormTrainingClient |
| .NET/C# | v2.0 | v2.0 | FormRecognizerClient |
| 3.0.x |  |  | Form Training Client |


<!---- Page 111 ---------------------------------------------------------------------------------------------------------------------------------->
2. Import the SDK client library into your application
C#/.NET
C#
using Azure;
using Azure.AI. FormRecognizer. DocumentAnalysis;
3. Set up authentication
There are two supported methods for authentication:
· Use a Document Intelligence API key with AzureKeyCredential from azure.core.credentials.
. Use a token credential from azure-identity to authenticate with Microsoft Entra ID.
Use your API key
Here's where to find your Document Intelligence API key in the Azure portal:
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
& Access control (IAM)
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
D
Networking
Location/Region @
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
18
> Automation
> Help
1 Important


<!---- Page 112 ---------------------------------------------------------------------------------------------------------------------------------->
We recommend Microsoft Entra ID authentication with managed identities for Azure
resources to avoid storing credentials with your applications that run in the cloud.
If you use an API key, store it securely somewhere else, such as in Azure Key Vault. Don't
include the API key directly in your code, and never post it publicly.
For more information about AI services security, see Authenticate requests to Azure AI
services.
C#/.NET
C#
/ set `<your-endpoint> and `<your-key> variables with the values from the
Azure portal to create your AzureKeyCredential` and `DocumentAnalysisClient'
instance
string key = "<your-key>";
string endpoint = "<your-endpoint>";
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentAnalysisClient client = new DocumentAnalysisClient(new Uri(endpoint),
credential);
Use a Microsoft Entra token credential
1 Note
Regional endpoints don't support Microsoft Entra authentication. Create a custom
subdomain for your resource in order to use this type of authentication.
Authorization is easiest using the DefaultAzureCredential . It provides a default token
credential, based upon the running environment, capable of handling most Azure
authentication scenarios.
C#/.NET
Here's how to acquire and use the DefaultAzureCredential for .NET applications:
1. Install the Azure Identity library for .NET:
Console


<!---- Page 113 ---------------------------------------------------------------------------------------------------------------------------------->
dotnet add package Azure. Identity
PowerShell
Install-Package Azure. Identity
2. Register a Microsoft Entra application and create a new service principal.
3. Grant access to Document Intelligence by assigning the Cognitive Services User
role to your service principal.
4. Set the values of the client ID, tenant ID, and client secret in the Microsoft Entra
application as environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, and
AZURE_CLIENT_SECRET, respectively.
5. Create your DocumentAnalysisClient instance including the DefaultAzureCredential :
C#
string endpoint = "<your-endpoint>";
var client = new DocumentAnalysisClient(new Uri(endpoint), new
DefaultAzureCredential());
For more information, see Authenticate the client .
4. Build your application
Create a client object to interact with the Document Intelligence SDK, and then call methods
on that client object to interact with the service. The SDKs provide both synchronous and
asynchronous methods. For more insight, try a quickstart in a language of your choice.
Help options
The Microsoft Q & A and Stack Overflow forums are available for the developer community
to ask and answer questions about Azure AI Document Intelligence and other services.
Microsoft monitors the forums and replies to questions that the community has yet to answer.
To make sure that we see your question, tag it with azure-form-recognizer.
Next steps


<!---- Page 114 ---------------------------------------------------------------------------------------------------------------------------------->
Explore
Document Intelligence REST API 2023-07-31
operations.


<!---- Page 115 ---------------------------------------------------------------------------------------------------------------------------------->
SDK target: REST API 2022-08-31 (GA)
Article · 11/19/2024
![Document Intelligence checkmark] .. /media/yes-icon.png) REST API version 2022-08-31 (GA)
Azure AI Document Intelligence is a cloud service that uses machine learning to analyze text
and structured data from documents. The Document Intelligence software development kit
(SDK) is a set of libraries and tools that enable you to easily integrate Document Intelligence
models and capabilities into your applications. Document Intelligence SDK is available across
platforms in C#/.NET, Java, JavaScript, and Python programming languages.
Supported programming languages
Document Intelligence SDK supports the following languages and platforms:
0
Expand table
Language - Document
Intelligence SDK version
Package
Supported API
version
Platform support
.NET/C# + 4.0.0 (GA)
NuGet
v3.0
Windows, macOS, Linux,
Docker zz
Java + 4.0.6 (GA)
Maven
repository
v3.0
Windows, macOS, Linux
JavaScript + 4.0.0 (GA)
npm 2
v3.0
Browser, Windows,
macOS, Linux EZ
Python + 3.2.0 (GA)
PyPI EZ
v3.0
Windows, macOS, Linux
For more information on other SDK versions, see:
· 2023-07-31 v3.1 (GA)
· v2.1 (GA)
Supported Clients
[] Expand table

| Language - Document Intelligence SDK version | Package | Supported API version | Platform support |
| --- | --- | --- | --- |
| .NET/C# + 4.0.0 (GA) | NuGet | v3.0 | Windows, macOS, Linux, Docker zz |
| Java + 4.0.6 (GA) | Maven repository | v3.0 | Windows, macOS, Linux |
| JavaScript + 4.0.0 (GA) | npm 2 | v3.0 | Browser, Windows, macOS, Linux EZ |
| Python + 3.2.0 (GA) | PyPI EZ | v3.0 | Windows, macOS, Linux |


<!---- Page 116 ---------------------------------------------------------------------------------------------------------------------------------->
Language
SDK version
API version
Supported clients
.NET/C#
Java
JavaScript
4.0.0 (GA)
v3.0:2022-08-31 (default)
DocumentAnalysisClient
DocumentModelAdministrationClient
.NET/C#
Java
JavaScript
3.1.x
v2.1 (default)
v2.0
FormRecognizerClient
FormTrainingClient
.NET/C#
Java
JavaScript
3.0.x
v2.0
FormRecognizerClient
FormTrainingClient
Python
3.2.x (GA)
v3.0:2022-08-31 (default)
DocumentAnalysisClient
DocumentModelAdministrationClient
Python
3.1.x
v2.1 (default)
v2.0
FormRecognizerClient
FormTrainingClient
Python
3.0.0
v2.0
FormRecognizerClient
FormTrainingClient
Use Document Intelligence SDK in your
applications
The Document Intelligence SDK enables the use and management of the Document
Intelligence service in your application. The SDK builds on the underlying Document
Intelligence REST API allowing you to easily use those APIs within your programming language
paradigm. Here's how you use the Document Intelligence SDK for your preferred language:
1. Install the SDK client library
C#/.NET
.NET CLI
dotnet add package Azure.AI.FormRecognizer -- version 4.0.0
PowerShell
Install-Package Azure.AI.FormRecognizer -Version 4.0.0

| Language | SDK version | API version | Supported clients |
| --- | --- | --- | --- |
| .NET/C# Java JavaScript | 4.0.0 (GA) | v3.0:2022-08-31 (default) | DocumentAnalysisClient DocumentModelAdministrationClient |
| .NET/C# Java JavaScript | 3.1.x | v2.1 (default) v2.0 | FormRecognizerClient FormTrainingClient |
| .NET/C# Java JavaScript | 3.0.x | v2.0 | FormRecognizerClient FormTrainingClient |
| Python | 3.2.x (GA) | v3.0:2022-08-31 (default) | DocumentAnalysisClient DocumentModelAdministrationClient |
| Python | 3.1.x | v2.1 (default) v2.0 | FormRecognizerClient FormTrainingClient |
| Python | 3.0.0 | v2.0 | FormRecognizerClient FormTrainingClient |


<!---- Page 117 ---------------------------------------------------------------------------------------------------------------------------------->
2. Import the SDK client library into your application
C#/.NET
C#
using Azure;
using Azure.AI. FormRecognizer. DocumentAnalysis;
3. Set up authentication
There are two supported methods for authentication:
· Use a Document Intelligence API key with AzureKeyCredential from azure.core.credentials.
. Use a token credential from azure-identity to authenticate with Microsoft Entra ID.
Use your API key
Here's where to find your Document Intelligence API key in the Azure portal:
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
& Access control (IAM)
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
D
Networking
Location/Region @
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
18
> Automation
> Help
1 Important


<!---- Page 118 ---------------------------------------------------------------------------------------------------------------------------------->
We recommend Microsoft Entra ID authentication with managed identities for Azure
resources to avoid storing credentials with your applications that run in the cloud.
If you use an API key, store it securely somewhere else, such as in Azure Key Vault. Don't
include the API key directly in your code, and never post it publicly.
For more information about AI services security, see Authenticate requests to Azure AI
services.
C#/.NET
C#
/set `<your-endpoint> and `<your-key> variables with the values from the
Azure portal to create your AzureKeyCredential` and `DocumentAnalysisClient'
instance
string key = "<your-key>";
string endpoint = "<your-endpoint>";
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentAnalysisClient client = new DocumentAnalysisClient(new Uri(endpoint),
credential);
Use a Microsoft Entra token credential
1 Note
Regional endpoints do not support Microsoft Entra authentication. Create a custom
subdomain for your resource in order to use this type of authentication.
Authorization is easiest using the DefaultAzureCredential . It provides a default token
credential, based upon the running environment, capable of handling most Azure
authentication scenarios.
C#/.NET
Here's how to acquire and use the DefaultAzureCredential for .NET applications:
1. Install the Azure Identity library for .NET:
Console


<!---- Page 119 ---------------------------------------------------------------------------------------------------------------------------------->
dotnet add package Azure. Identity
PowerShell
Install-Package Azure. Identity
2. Register a Microsoft Entra application and create a new service principal.
3. Grant access to Document Intelligence by assigning the Cognitive Services User
role to your service principal.
4. Set the values of the client ID, tenant ID, and client secret in the Microsoft Entra
application as environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, and
AZURE_CLIENT_SECRET, respectively.
5. Create your DocumentAnalysisClient instance including the DefaultAzureCredential :
C#
string endpoint = "<your-endpoint>";
var client = new DocumentAnalysisClient(new Uri(endpoint), new
DefaultAzureCredential());
For more information, see Authenticate the client .
4. Build your application
Create a client object to interact with the Document Intelligence SDK, and then call methods
on that client object to interact with the service. The SDKs provide both synchronous and
asynchronous methods. For more insight, try a quickstart in a language of your choice.
Help options
The Microsoft Q & A and Stack Overflow forums are available for the developer community
to ask and answer questions about Azure AI Document Intelligence and other services.
Microsoft monitors the forums and replies to questions that the community has yet to answer.
To make sure that we see your question, tag it with azure-form-recognizer.
Next steps


<!---- Page 120 ---------------------------------------------------------------------------------------------------------------------------------->
Explore Document Intelligence REST API v3.0
Try a Document Intelligence quickstart


<!---- Page 121 ---------------------------------------------------------------------------------------------------------------------------------->
SDK changelog, release history, and
migration guide
Article · 12/18/2024
This reference article provides a version-based description of Document Intelligence feature
and capability releases, changes, updates, and enhancements.
December 2024 (GA) release
.NET (C#)
· Document Intelligence 1.0.0
· Targets REST API 2024-11-30 (GA) by default
Changelog/Release History
Package (NuGet) z
Azure SDK for .NET
ReadMe z
Samples
Migration guide
August 2024 (preview) release
.NET (C#)
· Document Intelligence 1.0.0-beta.3
· Targets REST API 2024-07-31-preview by default
Changelog/Release History
Package (NuGet) z
ReadMe
Samples Z
Migration guide


<!---- Page 122 ---------------------------------------------------------------------------------------------------------------------------------->
March 2024 (preview) release
.NET (C#)
· Document Intelligence 1.0.0-beta.2
· Targets REST API 2024-02-29-preview by default
Changelog/Release History
Package (NuGet) [
ReadMe z
Samples
Migration guide ~
November 2023 (preview) release
.NET (C#)
· Document Intelligence 1.0.0-beta.1
· Targets REST API 2023-10-31-preview by default
Package (NuGet) [
ReadMe z
Samples
Migration guide
August 2023 (GA) release
C#
· Form Recognizer 4.1.0 (2023-08-10)
· Targets REST API 2023-07-31 by default
· REST API target 2023-02-28-preview is no longer supported
· Breaking changes
Changelog/Release History


<!---- Page 123 ---------------------------------------------------------------------------------------------------------------------------------->
Package (NuGet) [
ReadMe
Samples
April 2023 (preview) release
This release includes the following updates:
C#
· Form Recognizer 4.1.0-beta.1 (2023-04-13)
· Targets 2023-02-28-preview by default
· No breaking changes
Changelog/Release History [
Package (NuGet) z
ReadMe
Samples
September 2022 (GA) release
This release includes the following updates:
1 Important
The DocumentAnalysisClient and DocumentModelAdministrationClient now target API v3.0
GA, released 2022-08-31. Document Intelligence no longer supports clients from 2020-06-
30-preview APIs or earlier.
C#
· Form Recognizer 4.0.0 GA (2022-09-08)
· Supports REST API v3.0 and v2.0 clients
Changelog/Release History
Package (NuGet) [


<!---- Page 124 ---------------------------------------------------------------------------------------------------------------------------------->
Migration guide
ReadMe
Samples
August 2022 (preview) release
This release includes the following updates:
C#
· Form Recognizer 4.0.0-beta.5 (2022-08-09)
· Supports REST API 2022-06-30-preview clients
Changelog/Release History
Package (NuGet) [
SDK reference documentation
June 2022 (preview) release
This release includes the following updates:
C#
· Form Recognizer 4.0.0-beta.4 (2022-06-08)
Changelog/Release History
Package (NuGet) z
SDK reference documentation


<!---- Page 125 ---------------------------------------------------------------------------------------------------------------------------------->
Frequently asked questions
FAQ
This content applies to:
v4.0 (GA)
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Azure AI Document Intelligence is a cloud-based service that uses machine-learning models to
extract key/value pairs, text, and tables from your documents. The returned result is a
structured JSON output. Document Intelligence use cases include automated data processing,
enhanced data-driven strategies, and enriched document search capabilities.
Overview
Are Azure AI Document Intelligence and Azure AI
Form Recognizer the same service?
Yes.
Azure AI Document Intelligence and Azure AI Form Recognizer are the same service. The
service was renamed from Azure AI Form Recognizer to Azure AI Document Intelligence in July
2023. The service provides the same capabilities and features as before the renaming.
· Pricing changes: There are no changes to pricing. The names Cognitive Services and
Applied AI Services continue to be used in Azure billing, cost analysis, price lists, and price
APIs.
· Breaking changes: There are no breaking changes to APIs or client libraries.
Does Document Intelligence integrate with other
Microsoft services?
Yes.
Document Intelligence integrates with the following services:
· Al Builder workflows
· Azure Al Search
· Azure Functions
· Azure Logic Apps


<!---- Page 126 ---------------------------------------------------------------------------------------------------------------------------------->
AI capabilities
Can I use Document Intelligence with generative AI
for document processing?
Yes.
You can also use a document generative AI solution to chat with your documents (RAG),
generate captivating content from those documents, and access Azure OpenAI Service models
on your data.
· With Azure Al Document Intelligence and Azure OpenAI combined, you can build an
enterprise application to seamlessly interact with your documents using natural language.
You can easily find answers, gain valuable insights, and generate new and engaging
content from existing documents.
. You can find more details on the retrieval-augmented generation pattern here.
Can Document Intelligence help with semantic
chunking within documents for retrieval-augmented
generation?
Yes.
Document Intelligence can provide the building blocks to enable semantic chunking. Semantic
chunking is a key step in retrieval-augmented generation (RAG) to ensure context dense
chunks and relevance improvement.
· Document Intelligence provides a layout model that provides a visual decomposition of
the document into lines, paragraphs, sections, headers, and footers.
. You can then choose to retrieve the results in markdown format, to further chunk the
document on section or paragraph boundaries.
For more information, see overview of RAG in Document Intelligence
Document Intelligence Studio
Do I need specific permissions to access Document
Intelligence Studio?


<!---- Page 127 ---------------------------------------------------------------------------------------------------------------------------------->
Yes.
You need an active Azure account and subscription with at least a Reader role to access
Document Intelligence Studio.
For document analysis and prebuilt models, here are the role requirements for user scenarios:
· Basic
o Cognitive Services User: You need this role for a Document Intelligence or Azure
Cognitive Services multiple-service [ resource to use Document Intelligence Studio.
· Advanced
o Contributor: You need this role to create a resource group or a Document Intelligence
resource.
For custom model projects, here are the role requirements for user scenarios:
· Basic
o Cognitive Services User: You need this role for a Document Intelligence or Cognitive
Services multiple-service resource to train a custom model or analyze with trained
models.
o Storage Blob Data Contributor: You need this role for a storage account to create
project and label data.
· Advanced
o Storage Account Contributor: You need this role for the storage account to set up
cross-origin resource sharing (CORS) settings. It's a one-time effort if you reuse the
same storage account.
o Contributor: You need this role to create a resource group and resources. Contributor
or Storage Account Contributor role doesn't give you access to use your Document
Intelligence resource or storage account if local (key-based) authentication is disabled.
You still need the basic roles (Cognitive Services User and Storage Data Blob
Contributor) to use the functions on Document Intelligence Studio.
For more information, see Microsoft Entra built-in roles and the sections about Azure role
assignments in the Document Intelligence Studio quickstart.
Can I process documents with more than two pages
in Document Intelligence Studio?
Yes, for paid-tier resources.


<!---- Page 128 ---------------------------------------------------------------------------------------------------------------------------------->
No, for free-tier resources.
· For free-tier (FO) resources, only the first two pages are analyzed whether you're using
Document Intelligence Studio, the REST API, or client libraries.
· If you want to analyze all pages in a document, change to a paid (SO) resource. In
Document Intelligence Studio, select the Settings (gear) button, select the Resources tab,
and check the price tier to use for analyzing your documents.
Can I change directories or subscriptions in
Document Intelligence Studio?
Yes.
· To change a directory in Document Intelligence Studio, select the Settings (gear) button.
Under Directory, select the directory from the list, and then select Switch Directory. Sign
in again after you switch the directory.
· To change a subscription or resource, go to the Resource tab under Settings.
Can I use Document Intelligence Studio with a
resource that is configured with a firewall or virtual
network?
Yes.
For v4.0 11-30-2024 (GA), auto labeling is hosted natively with the rest of the service, so
there's no need for IP allowlisting. For any previous version, if your Document Intelligence
resource is configured with a firewall or virtual network, you need to add the dedicated IP
address 20.3.165.95 to the firewall allowlist for your Document Intelligence resource. Some
functions in custom projects (for example, autolabel, project management and human in the
loop) don't work if the public network access is disabled.
When I upload a file in Document Intelligence Studio
by "Fetch from URL" function, can I use a URL from
my blob storage?
Yes.
If your Azure blob storage URL includes a SAS token, and is accessible from public networks.
You can't use the Fetch function for storage accounts where the key access is disabled or


<!---- Page 129 ---------------------------------------------------------------------------------------------------------------------------------->
behind a firewall/VNet.
Can I reuse or customize the labeling experience
from Document Intelligence Studio and build it into
my own application?
Yes.
The labeling experience from Document Intelligence Studio is open sourced in the Toolkit
repo z .
Are there separate URL endpoints for Document
Intelligence sovereign cloud regions?
Yes.
Document Intelligence Studio has separate URL endpoints for sovereign cloud regions:
. URL for the Azure US Government cloud (Azure Fairfax): Document Intelligence Studio US
Government Z.
· URL Microsoft Azure operated by 21Vianet (Azure China): Document Intelligence Studio
China Z .
App development
Can I develop applications using Azure AI Document
Intelligence using the latest development options?
Yes.
Document Intelligence offers the latest development options within the following platforms:
· REST API
. Document Intelligence Studio
· C#/.NET
· Java z
. JavaScript/TypeScript


<!---- Page 130 ---------------------------------------------------------------------------------------------------------------------------------->
· Python
Can I migrate my application to the latest version of
Document Intelligence?
Yes.
The following table provides links to detailed instructions for migrating to the newest version
of Document Intelligence:
Expand table
Language/API
Migration guide
REST API
v3
C#/.NET
4.0.0
Java
4.0.0
JavaScript
4.0.0
Python
3.2.0
Can I specify a range of pages to be analyzed in a
document?
Yes.
Use the pages parameter (supported in v2.1, v3.0, and later versions of the REST API) and
specify pages for multiple-page PDF and TIFF documents. Accepted input includes the
following ranges:
· Single pages. For example, if you specify 1, 2, pages 1 and 2 are processed.
· Finite ranges. For example, if you specify 2-5, pages 2 to 5 are processed.
· Open-ended ranges. For example, if you specify 5-, all the pages from page 5 are
processed. If you specify -10, pages 1 to 10 are processed.

| Language/API | Migration guide |
| --- | --- |
| REST API | v3 |
| C#/.NET | 4.0.0 |
| Java | 4.0.0 |
| JavaScript | 4.0.0 |
| Python | 3.2.0 |


<!---- Page 131 ---------------------------------------------------------------------------------------------------------------------------------->
-5,
You can mix these parameters together, and ranges can overlap. For example, if you specify
1, 3, 5-10, pages 1 to 10 are processed.
The service accepts the request if it can process at least one page of the document. For
example, using 5-100 on a five-page document is a valid input that means page 5 is processed.
If you don't provide a page range, the entire document is processed.
Do you recommend using Document Intelligence
Studio rather than the FOTT Sample Labeling tool
for my project?
Yes.
We recommend Document Intelligence Studio [ most of the time because it can reduce your
time for configuring Document Intelligence resources and storage services.
Only consider using the Form Testing Tool (FOTT) for the following scenarios:
· Your data must remain within a single machine. Use the FOTT Sample Labeling tool and a
Document Intelligence container.
· Your project is highly dependent on Document Intelligence V2.1 and you want to keep
using the v2.1 APIs.
Are there best practices to mitigate throttling?
Yes.
Document Intelligence uses autoscaling to provide the required computational resources on
demand, while keeping customer costs low. To mitigate throttling during autoscaling, we
recommend the following approach:
· Implement retry logic in your application.
. If you find that you're being throttled on the number of POST requests, consider adding a
delay between the requests.
. Increase the workload gradually. Avoid sharp changes.
· Create a support request to increase transactions per second (TPS) limit.
Learn more about Document Intelligence service quotas and limits.


<!---- Page 132 ---------------------------------------------------------------------------------------------------------------------------------->
Custom models
Can I improve an estimated accuracy score for a
custom model?
Yes.
Variances in the visual structure of your documents can influence the accuracy of a model. Here
are some tips:
. Include all variations of a document in the training dataset. Variations include different
formats; for example, digital versus scanned PDFs.
· Separate visually distinct document types and train different models.
. Make sure that you don't have extraneous labels.
· For signature and region labeling, don't include the surrounding text.
For more information, see Accuracy and confidence scores.
Can I retrain a custom model?
No.
· Document Intelligence doesn't have an explicit retrain operation. Each train operation
generates a new model.
. If you find that your model needs to retrain, you can add more samples to your training
dataset and train a new model.
. You can also create a new model to compose with your original model as follows:
1. Create a dataset for your new template.
2. Label and train a new model.
3. Validate that the new model performs well for your specific document types.
4. Compose your new model with the existing model into a single endpoint. Document
Intelligence can then determine the best model for each document to be analyzed.
For more information, see composed models.


<!---- Page 133 ---------------------------------------------------------------------------------------------------------------------------------->
Can I move my trained models from one
environment (like beta) to another (like production)?
Yes.
You can use the Copy API to copy custom models from one Document Intelligence account
into others that exist in any supported geographical region. For detailed instructions, see
Disaster recovery.
The copy operation is limited to copying models within the specific cloud environment where
you trained the model. For instance, copying models from the public cloud to the Azure
Government cloud isn't supported.
Am I charged when using auto labeling?
Yes. Auto label incurs a cost which is equivalent to an analyze request for the corresponding
model for a document.
Am I charged when training a custom models?
Yes.
For v4.0 11-30-2024 (GA) custom neural models can be trained for free for a maximum of 10
hours. Whether you're training a single model for the 10 hours, or training multiple models for
the total of 10 hours, you aren't charged for the first 10 hours. After using up the free 10 hours,
you're automatically charged by the extra training hour. For details on prices, refer to the
pricing page . This new paid training feature enables training models for an extended
duration to process larger documents. For more information on this paid training feature,
check custom neural model billing section.
For v3.0 2022-08-31 or v3.1 2023-07-31, custom neural models can be trained for free for a
maximum of 20 training sessions, with each session capped at 30 minutes of training duration.
Once you use up all of the 20 training sessions, you can submit Azure support ticket to increase
the training session limit. To increase the limit, two training sessions are considered as one
training hour, and you're charged per two sessions / one training hour. For details on the
prices, refer to the pricing page . For more information on ways to increase the limit, check
custom neural model billing section. For v3.0 and v3.1, paid training feature is unavailable.
Paid training feature for custom neural model is only available on v4.0.
Storage account


<!---- Page 134 ---------------------------------------------------------------------------------------------------------------------------------->
Is there an expiry time for the shared access
signature (SAS) token that I for my storage account
authentication?
Yes.
When you create a shared access signature (SAS), the default duration is 48 hours. After 48
hours, you need to create a new token.
Consider setting a longer duration period for the time that you're using your storage account
with Document Intelligence.
Can Document Intelligence access data in my
storage account if it is behind a virtual network or
firewall?
No, not directly.
Document Intelligence can't access your storage account if it's protected by a virtual network
or firewall.
However, private Azure storage account access and authentication support managed identities
for Azure resources. When you use a managed identity, the Document Intelligence service can
access your storage account by using an assigned credential.
If you intend to analyze your private storage account data by using FOTT, you must deploy the
tool behind the virtual network or firewall.
Learn how to create and use a managed identity for your Document Intelligence resource.
Containers
Is there a difference between disconnected and
connected containers?
Yes.
Although the model capabilities are the same for connected and disconnected containers, the
billing and connectivity methods differ:


<!---- Page 135 ---------------------------------------------------------------------------------------------------------------------------------->
. Connected containers send billing information to Azure by using a Document Intelligence
resource on your Azure account. With connected containers, internet connectivity is
required to send billing information to Azure. Document Intelligence connected
containers send billing information to Azure by using a Document Intelligence resource
on your Azure account. Connected containers don't send customer data, such as the
image or text that's being analyzed, to Microsoft. For an example of the information that
connected containers send to Microsoft for billing, see the Azure AI container FAQ.
. Disconnected containers enable you to use APIs that are disconnected from the internet.
Billing information isn't sent via the internet. Instead, Charges are based on a purchased
commitment tier. Currently, disconnected container usage is available for Document
Intelligence custom and invoice models.
Can I use local storage for the Document
Intelligence Sample Labeling Tool (FOTT) container?
Yes.
FOTT has a version that uses local storage. The version needs to be installed on a Windows
machine. You can install it from this location.
On the project page, specify the label folder URI as /shared or /shared/sub-dir if your labeling
files are in a subdirectory. All other Document Intelligence Sample Labeling Tool behavior is the
same as the hosted service.
Is there a best practice for scaling up?
Yes.
For asynchronous calls, you can run multiple containers with shared storage. The container
that's processing the POST analyze call stores the output in the storage. Then, any other
container can fetch the results from the storage and serve the GET calls. The request ID isn't
tied to a container.
For synchronous calls, you can run multiple containers, but only one container serves a request.
Because it's a blocking call, any container from the pool can serve the request and send the
response. Here, only one container is tied to a request at a time, and no polling is required.
Can I set up containers with shared storage?
Yes.


<!---- Page 136 ---------------------------------------------------------------------------------------------------------------------------------->
The containers use the Mounts: Shared property while starting up for specifying the shared
storage to store the processing files. To see the use of this property, refer to the containers
documentation.
Security and privacy
Does Document Intelligence store my data?
Yes, briefly.
For all features, Document Intelligence temporarily stores data and results in Azure Storage in
the same region as the request. Your data is then deleted 24 hours from the time that you
submit an analyze request. If you would like the data deleted sooner, you can call the delete
analyze response. This API marks the results for deletion and is available in the v4.0 API.
Learn more about data, privacy, and security for Document Intelligence.
For trained custom models, the interim outputs after analysis and labeling are stored in the
same Azure Storage location where you store your training data. The trained custom models
are stored in Azure Storage in the same region, and are logically isolated with your Azure
subscription and API credentials.
More help and support
Are there other resources available to provide
solutions to Azure AI Document Intelligence
questions?
Yes.
Microsoft Q & A is the home for technical questions and answers at Microsoft. You can filter
queries that are specific to Document Intelligence.
Can I provide direct feedback if the service doesn't
recognize specific text, or recognizes it incorrectly,
when I'm labeling documents?
Yes.


<!---- Page 137 ---------------------------------------------------------------------------------------------------------------------------------->
We continually update and improve the Document Intelligence models. You can email the
Document Intelligence team. If possible, share a sample document with the issue highlighted.


<!---- Page 138 ---------------------------------------------------------------------------------------------------------------------------------->
Document processing models
Article · 03/14/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Azure AI Document Intelligence supports a wide variety of models that enable you to add intelligent document processing to your apps
and flows. You can use a prebuilt domain-specific model or train a custom model tailored to your specific business needs and use cases.
Document Intelligence can be used with the REST API or Python, C#, Java, and JavaScript client libraries.
Note
· Document processing projects that involve financial data, protected health data, personal data, or highly sensitive data require
careful attention.
. Be sure to comply with all national/regional and industry-specific requirements 2 .
Model overview
The following table shows the available models for each stable API:
0
Expand table
Model Type
Model
2024-11-30 (GA)
2023-07-31 (GA)
2022-08-31 (GA)
v2.1 (GA)
Document analysis models
Read
n/a
Document analysis models
Layout
Document analysis models
** General document
supported in
layout model
n/a
Prebuilt models
Bank Check
n/a
n/a
n/a
Prebuilt models
Bank Statement
n/a
n/a
n/a
Prebuilt models
Paystub
n/a
n/a
n/a
Prebuilt models
Contract
n/a
n/a
Prebuilt models
Health insurance card
n/a
Prebuilt models
ID document
Prebuilt models
Invoice
Prebuilt models
Receipt
Prebuilt models
US Unified Tax*
n/a
n/a
n/a
Prebuilt models
US 1040 Tax*
n/a
n/a
Prebuilt models
US 1095 Tax*
n/a
n/a
n/a
Prebuilt models
US 1098 Tax*
n/a
n/a
n/a
Prebuilt models
US 1099 Tax*
n/a
n/a
n/a
Prebuilt models
US W2 Tax
n/a
Prebuilt models
US W4 Tax
n/a
n/a
n/a
Prebuilt models
US Mortgage 1003 URLA
n/a
n/a
n/a
Prebuilt models
US Mortgage 1004 URAR
n/a
n/a
n/a
Prebuilt models
US Mortgage 1005
n/a
n/a
n/a
Prebuilt models
US Mortgage 1008 Summary
n/a
n/a
n/a
Prebuilt models
US Mortgage closing disclosure
n/a
n/a
n/a
Prebuilt models
Marriage certificate
n/a
n/a
n/a

| Model Type | Model | 2024-11-30 (GA) | 2023-07-31 (GA) | 2022-08-31 (GA) | v2.1 (GA) |
| --- | --- | --- | --- | --- | --- |
| Document analysis models | :unselected: Read | :selected: | :selected: | :selected: :unselected: | n/a |
| Document analysis models | :unselected: Layout | :selected: | :selected: | :selected: | :selected: |
| Document analysis models | :unselected: ** General document | supported in layout model | :selected: | :selected: :unselected: | n/a |
| Prebuilt models | Bank Check | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | :unselected: Bank Statement | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | Paystub | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | :unselected: Contract | :selected: | :selected: | :unselected: n/a :unselected: | n/a |
| Prebuilt models | :unselected: Health insurance card | :selected: | :selected: | :selected: :unselected: | n/a |
| Prebuilt models | :unselected: ID document | :selected: | :selected: | :selected: | :selected: |
| Prebuilt models | :unselected: Invoice | :selected: | :selected: | :selected: | :selected: |
| Prebuilt models | :unselected: Receipt | :selected: | :selected: | :selected: | :selected: |
| Prebuilt models | :unselected: US Unified Tax* | :selected: | n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US 1040 Tax* | :selected: | :selected: | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US 1095 Tax* | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US 1098 Tax* | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US 1099 Tax* | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US W2 Tax | :selected: | :selected: | :selected: :unselected: | n/a |
| Prebuilt models | US W4 Tax | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US Mortgage 1003 URLA | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US Mortgage 1004 URAR | :selected: | :unselected: n/a | :unselected: n/a | n/a |
| Prebuilt models | US Mortgage 1005 | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US Mortgage 1008 Summary | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | US Mortgage closing disclosure | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |
| Prebuilt models | :unselected: Marriage certificate | :selected: | :unselected: n/a | :unselected: n/a :unselected: | n/a |


<!---- Page 139 ---------------------------------------------------------------------------------------------------------------------------------->
Model Type
Model
2024-11-30 (GA)
2023-07-31 (GA)
2022-08-31 (GA)
v2.1 (GA)
Prebuilt models
Credit card
n/a
n/a
n/a
Prebuilt models
Business card
deprecated
Custom classification model
Custom classifier
n/a
n/a
Custom extraction model
Custom neural
n/a
Custom extraction model
Custom template
Custom extraction model
Custom composed
All models
Add-on capabilities
n/a
n/a
* Contains submodels. See the model specific information for supported variations and subtypes.
** All the General Document model capabilities are available in layout model. General model is no longer supported.
Latency
Latency is the amount of time it takes for an API server to handle and process an incoming request and deliver the outgoing response to
the client. The time to analyze a document depends on the size (for example, number of pages) and associated content on each page.
Document Intelligence is a multitenant service where latency for similar documents is comparable but not always identical. Occasional
variability in latency and performance is inherent in any microservice-based, stateless, asynchronous service that processes images and
large documents at scale. Although we're continuously scaling up the hardware and capacity and scaling capabilities, you might still have
latency issues at runtime.
Add-on Capability
Following are the add-on capability available in document intelligence. For all models, except Business card model, Document Intelligence
now supports add-on capabilities to allow for more sophisticated analysis. These optional capabilities can be enabled and disabled
depending on the scenario of the document extraction. There are seven add-on capabilities available for the 2023-07-31 (GA) and later API
version:
· ocrHighResolution
· formulas
· styleFont
· barcodes
· languages
· keyValuePairs
· queryFields Not available with the US. Tax models
. searchablePDF Only available for Read Model
0
Expand table
Add-on Capability
Add-On/Free
· 2024-11-30 (GA)
2023-07-31 (GA)
2022-08-31 (GA)
v2.1 (GA)
Font property extraction
Add-On
n/a
n/a
Formula extraction
Add-On
n/a
n/a
High resolution extraction
Add-On
n/a
n/a
Barcode extraction
Free
n/a
n/a
Language detection
Free
n/a
n/a
Key value pairs
Free
n/a
n/a
n/a
Query fields
Add-On*
n/a
n/a
n/a
Searchable pdf
Add-On*
n/a
n/a
n/a
Model analysis features
Expand table

| Model Type | Model | 2024-11-30 (GA) | 2023-07-31 (GA) | 2022-08-31 (GA) | v2.1 (GA) |
| --- | --- | --- | --- | --- | --- |
| Prebuilt models | Credit card | :selected: | n/a | n/a | n/a |
| Prebuilt models | Business card | deprecated | :selected: | :selected: | :selected: |
| Custom classification model | Custom classifier | :selected: | :selected: | n/a | n/a |
| Custom extraction model | Custom neural | :selected: | :selected: | :selected: | n/a |
| Custom extraction model | Custom template | :selected: | :selected: | :selected: | :selected: |
| Custom extraction model | Custom composed | :selected: | :selected: | :selected: | :selected: |
| All models | Add-on capabilities | :selected: | :selected: | n/a | n/a |

| Add-on Capability | Add-On/Free | · 2024-11-30 (GA) | 2023-07-31 (GA) | 2022-08-31 (GA) | v2.1 (GA) |
| --- | --- | --- | --- | --- | --- |
| Font property extraction | Add-On | :selected: | :selected: | :unselected: n/a | n/a |
| Formula extraction | Add-On | :selected: :unselected: | :selected: | :unselected: n/a | n/a |
| High resolution extraction | Add-On | :selected: | :selected: | :unselected: n/a | n/a |
| Barcode extraction | :unselected: Free | :selected: | :selected: | :unselected: n/a | n/a |
| Language detection | :unselected: Free | :selected: | :selected: | :unselected: n/a | n/a |
| Key value pairs | :unselected: Free | :selected: | n/a | :unselected: n/a | :unselected: n/a |
| Query fields | Add-On* | :selected: | :unselected: n/a | :unselected: n/a | n/a |
| Searchable pdf | Add-On* | :selected: | :unselected: n/a | :unselected: n/a | n/a |


<!---- Page 140 ---------------------------------------------------------------------------------------------------------------------------------->
Model ID
Content
Extraction
Query
fields
Paragraphs
Paragraph
Roles
Selection
Marks
Tables
Key-
Value
Pairs
Languages
Barcodes
Document
Analysis
Formulas*
prebuilt-read
✓
✓
O
O
O
prebuilt-layout
✓
✓
✓
✓
✓
✓
O
O
O
O
prebuilt-contract
✓
✓
✓
✓
✓
O
O
✓
O
prebuilt-
healthInsuranceCard.us
✓
✓
O
O
✓
O
prebuilt-idDocument
✓
✓
O
O
✓
O
prebuilt-invoice
✓
✓
✓
✓
O
O
O
✓
O
prebuilt-receipt
✓
✓
O
O
✓
O
prebuilt-
marriageCertificate.us
✓
✓
✓
O
O
✓
O
prebuilt-creditCard
✓
✓
O
O
✓
O
prebuilt-check.us
✓
✓
O
O
✓
O
prebuilt-payStub.us
✓
✓
O
O
✓
O
prebuilt-bankStatement
✓
✓
O
O
✓
O
prebuilt-mortgage.us.1003
✓
✓
✓
O
O
✓
O
prebuilt-mortgage.us.1004
✓
✓
✓
O
O
✓
O
prebuilt-mortgage.us.1005
✓
✓
✓
O
O
✓
O
prebuilt-mortgage.us.1008
✓
✓
✓
O
O
✓
O
prebuilt-
mortgage.us.closingDisclosure
✓
✓
✓
O
O
✓
O
prebuilt-tax.us
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.w2
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.w4
✓
✓
O
O
✓
O
prebuilt-tax.us.1040 (various)
✓
✓
✓
O
O
✓
O
prebuilt-tax.us. 1095A
✓
✓
O
O
✓
O
prebuilt-tax.us.1095C
✓
✓
O
O
✓
O
prebuilt-tax.us. 1098
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.1098E
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.1098T
✓
✓
✓
O
O
✓
O
prebuilt-tax.us.1099 (various)
✓
✓
✓
O
O
✓
O
prebuilt-tax.us. 1099SSA
✓
✓
O
O
✓
O
{ customModelName }
✓
✓
✓
✓
✓
✓
O
O
✓
O
V - Enabled
O - Optional
* - Premium features incur extra costs
Add-On* - Query fields are priced differently than the other add-on features. See pricing ? for details.
Bounding box and polygon coordinates
A bounding box ( polygon in v3.0 and later versions) is an abstract rectangle that surrounds text elements in a document used as a
reference point for object detection.

| Model ID | Content Extraction | Query fields | Paragraphs | Paragraph Roles | Selection Marks | Tables | Key- Value Pairs | Languages | Barcodes | Document Analysis | Formulas* |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| prebuilt-read | :selected: ✓ | :unselected: | ✓ :selected: | :unselected: |  |  |  | O :unselected: | O :unselected: |  | O :unselected: |
| prebuilt-layout | :selected: ✓ | ✓ :selected: | ✓ :selected: | ✓ :selected: | ✓ :selected: | ✓ :selected: | O :unselected: | O :unselected: | O :unselected: |  | O :unselected: |
| prebuilt-contract | :selected: ✓ | ✓ :selected: | ✓ :selected: | ✓ :selected: | ✓ :selected: |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt- healthInsuranceCard.us | ✓ :selected: | ✓ :selected: |  |  |  |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
|  |  |  |  |  |  |  |  |  |  |  |  |
| prebuilt-idDocument | :selected: ✓ | :selected: ✓ | :unselected: | :unselected: |  |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-invoice | :selected: ✓ | :selected: ✓ |  |  | ✓ :selected: | :selected: ✓ | O :unselected: | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-receipt | :selected: ✓ | :selected: ✓ |  |  |  |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt- marriageCertificate.us | :selected: ✓ | :selected: ✓ |  |  | ✓ :selected: |  |  | O :unselected: | :unselected: O | ✓ :selected: | O :unselected: |
| prebuilt-creditCard | :selected: ✓ | :selected: ✓ |  |  |  |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-check.us | :selected: ✓ | :selected: ✓ |  |  |  |  |  | O :unselected: | O :unselected: | :selected: ✓ | O :unselected: |
| prebuilt-payStub.us | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-bankStatement | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-mortgage.us.1003 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | O :unselected: | :unselected: O | ✓ :selected: | O :unselected: |
| prebuilt-mortgage.us.1004 | :selected: ✓ | ✓ :selected: |  |  | :selected: ✓ |  |  | O :unselected: | O :unselected: | ✓ :selected: | O :unselected: |
| prebuilt-mortgage.us.1005 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | O :unselected: |
| prebuilt-mortgage.us.1008 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt- mortgage.us.closingDisclosure | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.w2 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.w4 | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1040 (various) | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us. 1095A | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | O :unselected: | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1095C | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us. 1098 | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1098E | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1098T | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us.1099 (various) | :selected: ✓ | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| prebuilt-tax.us. 1099SSA | :selected: ✓ | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |
| { customModelName } | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O |


<!---- Page 141 ---------------------------------------------------------------------------------------------------------------------------------->
· The bounding box specifies position by using an x and y coordinate plane presented in an array of four numerical pairs. Each pair
represents a corner of the box in the following order: upper left, upper right, lower right, lower left.
· Image coordinates are presented in pixels. For a PDF, coordinates are presented in inches.
Language support
The deep-learning-based universal models in Document Intelligence support many languages that can extract multilingual text from your
images and documents, including text lines with mixed languages. Language support varies by Document Intelligence service functionality.
For a complete list, see the following articles:
· Language support: document analysis models
· Language support: prebuilt models
· Language support: custom models
Regional availability
Document Intelligence is generally available in many of the 60+ Azure global infrastructure regions .
For more information, see our Azure geographies [ page to help choose the region that's best for you and your customers.
Model details
This section describes the output you can expect from each model. You can extend the output of most models with add-on features.
Read OCR
The Read API analyzes and extracts lines, words, their locations, detected languages, and handwritten style if detected.
Sample document processed using the Document Intelligence Studio " :


<!---- Page 142 ---------------------------------------------------------------------------------------------------------------------------------->
Tu Analyze
API version: 2022-01-30-preview
Content
Result
Code
₼
IRS
IRS-Unterstützung in
Katastrophenfällen
Von der US-Bundesregierung erklärtes Katastrophengebiet
Sie können den vollständigen oder teilweisen Verlust Ihres Wohneigentums, Ihres Hausrats und Ihrer
Kraftfahrzeuge infolge einer Beschädigung im Katastrophenfall auf Ihrer persönlichen
Bundeseinkommenssteuererklärung absetzen. Wenn Sie im Steuerjahr unmittelbar vor dem Steuerjahr, in
dem der Katastrophenfall eintrat, Steuern abgeführt haben, können Sie Ihren Verlust auf einem Formular
1040X (Amended U.S. Individual Income Tax Return - Abgeänderte US-Einkommenssteuererklärung) für das
Vorjahr absetzen, anstatt zu warten, bis Sie Ihre Steuererklärung für das laufende Jahr einreichen. Auf diese
Weise können Sie sich Ihre Steuern, die Sie in Verbindung mit Ihrer Steuererklärung des Vorjahres bezahlt
haben, teilweise oder ganz erstatten lassen.
Was das für Sie bedeutet
· Wenn Sie im vorausgegangenen Steuerjahr eine Bundeseinkommenssteuererklärung abgegeben und
Bundessteuern bezahlt haben ...
o können Sie jetzt u. U. eine abgeänderte Steuererklärung einreichen (oder damit bis nächstes Jahr
warten), um Ihren Verlust geltend zu machen und sich die bezahlten Steuern erstatten zu lassen.
o Sie müssen Ihre Abzüge auf Formular 1040, Aufstellung A, einzeln aufschlüsseln.
So machen Sie Ihren Verlust geltend
· Fertigen Sie eine Liste Ihres gesamten verloren gegangenen Besitzes an.
· Stellen Sie die Anschaffungskosten (bzw. die angepasste Steuerbemessungsgrundlage) fest.
· Stellen Sie den Verkehrswert jedes Besitzguts fest.
o Dabei handelt es sich um den Betrag, zu dem das Besitzgut unmittelbar vor Eintreten des
Katastrophenfalls hätte verkauft werden können.
· Stellen Sie den derzeitigen Wert - also nach Eintreten des Katastrophenfalls - fest.
· Stellen Sie Versicherungs- oder andere Erstattungsleistungen fest, die Sie bereits erhalten haben oder
voraussichtlich erhalten werden.
So können Sie sich bei der Geltendmachung von Sachverlusten helfen lassen
. Besorgen Sie sich die IRS-Publikation 2194, Disaster Resource Guide (Hilfreiche Informationen und
Materialien für Katastrophenfälle), für Einzelpersonen und Unternehmen.
· Besorgen Sie sich rechnererstellte Kopien Ihrer letztjährigen Steuererklärung vom IRS.
· Der IRS kann Ihnen beim Erstellen Ihrer abgeänderten Steuererklärungen behilflich sein.
So erhalten Sie weitere Informationen und Unterstützung
. IRS-Hotline für Hilfe im Katastrophenfall - 1-866-562-5227
(Montag bis Freitag von 7.00 bis 19.00 Uhr Ortszeit).
*Bitte treffen Sie vor dem Anruf bei der Hotline-Nummer bei Bedarf selbst Vorkehrungen für einen
Dolmetscher.
. Besuchen Sie die IRS-Website unter www.irs.gov oder
· Wenden Sie sich an Ihren Steuerberater.
Publication 3067 EN-DE (Rev. 10-2017) Catalog Number 53671Z Department of the Treasury Internal Revenue Service www.irs.gov
< 1 of 1 > Q Q @>
IRS-Unterstützung in
IRS Katastrophenfällen
VAV
Von der US-Bundesregierung erklärt
Sie können den vollständigen oder
Kraftfahrzeuge infolge einer Besch
Bundeseinkommenssteuererklärung ab
dem der Katastrophenfall eintrat,
1040X (Amended U.S. Individual Inc
Vorjahr absetzen, anstatt zu warte
Weise können Sie sich Ihre Steuern
haben, teilweise oder ganz erstatt
Was das für Sie bedeutet
· Wenn Sie im vorausgegangenen Ste
Bundessteuern bezahlt haben ...
o können Sie jetzt u. U. eine abge
warten), um Ihren Verlust geltend
o Sie müssen Ihre Abzüge auf Formu
So machen Sie Ihren Verlust gelten
· Fertigen Sie eine Liste Ihres ge
Stellen Sie die Anschaffungskosten
Stellen Sie den Verkehrswert jedes
o Dabei handelt es sich um den Bet
Katastrophenfalls hätte verkauft w
Stellen Sie den derzeitigen Wert -
Stellen Sie Versicherungs- oder an
voraussichtlich erhalten werden.
So können Sie sich bei der Geltend
. Besorgen Sie sich die IRS-Publik
Materialien für Katastrophenfälle)
V
Learn more: read model
Layout analysis
D
B
The Layout analysis model analyzes and extracts text, tables, selection marks, and other structure elements like titles, section headings,
page headers, page footers, and more.
Sample document processed using the Document Intelligence Studio 2 :


<!---- Page 143 ---------------------------------------------------------------------------------------------------------------------------------->
Tuesday
Sep 20,
YYYY
NEWS TODAY
Latest news and bulletin updates
Issuc
#10
Mirjam Nilsson
The scoop of the day
The latest updates
Video provides a powerful way to help you
prove your point. When you click Online
Video, you can paste in the embed code for
the video you want to add. You can also
type a keyword to search online for the
video that best fits your document.
Content
To make your document look
professionally produced, Word provides
header, footer, cover page, and text box
designs that complement each other. For
example, you can add a matching cover
page, header, and sidebar.
Video provides a powerful way to
help you prove your point. When
you click Online Video, you can
paste in the embed code for the
video you want to add. You can also
type a keyword to search online for
the video that best fits your
document.
Click Insert and then choose the elements
you want from the different galleries.
Polygon
59, 195, 262, 195, 262, 275, 59, 275
Themes and styles also help keep your
document coordinated. When you click
Design and choose a new Theme, the
pictures, charts, and SmartArt graphics
change to match your new theme. When
you apply styles, your headings change to
match the new theme.
Picture Caption: To make your document look professionally produced, Word provides header, footer, cover page, and
text box designs that complement each other
Mirjam Nilsson
The scoop of the day
Save time in Word with new buttons that
show up where you need them. To change
The latest updates to get you through the day
Themes and styles also help keep your
<
1
of 1 >
<3
Text
Tables
Selection marks
4
itle
NEWS TODAY
Paragraph
Issue #10
Title
Latest news and bulletin updates
Paragraph
Mirjam Nilsson
SectionHeading
The scoop of the day The latest updates
Paragraph
Video provides a powerful way to help you prove
¥
Learn more: layout model
Health insurance card
6
The health insurance card model combines powerful Optical Character Recognition (OCR) capabilities with deep learning models to analyze
and extract key information from US health insurance cards.
Sample US health insurance card processed using Document Intelligence Studio :
Eu[ Analyze
V
All pages
Range
Fields
Result
Code
DocType: healthInsuranceCard.us
Copays (2) #1
A
1 Amount
$1,500
PREMERA
Benefit
Microsoft
Deductible
BLUE CROSS
2 Amount
Member
ANGEL BROWN
Medical Network HERITAGE
$1,000
Premera Dental
YES
Benefit
Prefix Identification # Suffix
Premera Vision
YES
Coinsurance Max
ABC 123456789
01
GroupNumber
#1
99.50%
Group # 1000000
HEALTH SAVINGS PLAN
1000000
Rx Group # BCAAXYZ
Rx BIN# 987654
Deductible
Shared In and Out of Network
$1.500
Coinsurance Max
$1.000
IdNumber
#1
1
A
BCBS 456
Number
99.50%
123456789
Note: Rx and Medical Cost-Shares are Shared
Rx
Prefix
99.50%
PPC
ABC
Insurer #1
99.50%
PREMERA BLUE CROSS
Member #1
Employer
+
99.50%
Microsoft
<
1
of 1 )
Q Q + 2
IdNumberSuffix
99.50%
Learn more: Health insurance card model
US tax documents


<!---- Page 144 ---------------------------------------------------------------------------------------------------------------------------------->
The US tax document models analyze and extract key fields and line items from a select group of tax documents. The API supports the
analysis of English-language US tax documents of various formats and quality including phone-captured images, scanned documents, and
digital PDFs. The following models are currently supported:
Expand table
Model
Description
ModelID
US Tax W-2
Extract taxable compensation details.
prebuilt-tax.us.w2
US Tax W-4
Extract taxable compensation details.
prebuilt-tax.us.w4
US Tax 1040
Extract mortgage interest details.
prebuilt-tax.us.1040(variations)
US Tax 1095
Extract health insurance details.
prebuilt-tax.us.1095(variations)
US Tax 1098
Extract mortgage interest details.
prebuilt-tax.us.1098(variations)
US Tax 1099
Extract income received from sources other than employer.
prebuilt-tax.us.1099(variations)
Sample W-2 document processed using Document Intelligence Studio:
1 Wages, tips, other comp.
2 Federal Income tax withheld
37160.56
3894.54
3 Social security wages
A Social security tav withheld
2303.95
37160.56
5 Medicare wages and tips
37160.56
6 Medicare tay withheld
538.83
7 Social security tips
8 Allocated tips
9
302.30
874.20
10 Dependent care benefits
11 Nonqualified plans
12a Code See inst. for box 12
9873.20
653.21
DO
5939.68
"i' Analyze
API version: 2022-01-30-preview
Fields
Result
Code
a. Employee's Soc Sec No
123-45-6789
DocType: tax.us.w2
5. Employer ID number (EIN)
98-7654321
AdditionalInfo (4)
#1
E
Employer's name, address and ZIP code
CONTOSO LTD
123 MICROSOFT WAY
REDMOND, WA 98765
AllocatedTips
874.2
#1
100.00%
d Control Number
000086242
#1
100.00%
e Employee's name, address, and ZIP code
ANGEL BROWN
4567 MAIN STREET
BUFFALO, WA 12345
ControlNumber
000086242
DependentCareBenefits
9873.2
#1
100.00%
Employee
#1
=
13 Statutory employee
14 Other
12b Code
B
5432.00
Employer
#1
E
Retirement plan
DISINS
170.85
12c Code
D
876.30
FederalIncome TaxWithheld
#1
100.00%
Third-party sick pay
12d Code
0
123.30
3894.54
PA
87654321
37160.56
1135.65
WA
12345678
9631.20
LocalTaxInfos (2)
#1
E
15 State Employer's state ID number
16 State wages, tips, etc.
19 Local income tax
51.00
17 State income tax
1032.30
18 Local wages, tips, etc.
37160.56
20 Locality name
Cmberland Viy/Mddl
Medicare TaxWithheld
538.83
#1
100.00%
37160.56
594.54
E.Pennsboro/E.Pnns
<
1
of 1 >
MedicareWagesAndTips
#1
100.00%
Learn more: Tax document models
US mortgage documents
%
1008 33
The US mortgage document models analyze and extract key fields including borrower, loan, and property information from a select group
of mortgage documents. The API supports the analysis of English-language US mortgage documents of various formats and quality
including phone-captured images, scanned documents, and digital PDFs. The following models are currently supported:
Expand table
Model
Description
ModelID
1003 End-User License Agreement (EULA)
Extract loan, borrower, property details.
prebuilt-mortgage.us.1003

| Model | Description | ModelID |
| --- | --- | --- |
| US Tax W-2 | Extract taxable compensation details. | prebuilt-tax.us.w2 |
| US Tax W-4 | Extract taxable compensation details. | prebuilt-tax.us.w4 |
| US Tax 1040 | Extract mortgage interest details. | prebuilt-tax.us.1040(variations) |
| US Tax 1095 | Extract health insurance details. | prebuilt-tax.us.1095(variations) |
| US Tax 1098 | Extract mortgage interest details. | prebuilt-tax.us.1098(variations) |
| US Tax 1099 | Extract income received from sources other than employer. | prebuilt-tax.us.1099(variations) |

| 1 Wages, tips, other comp. |  | 2 Federal Income tax withheld |  |
| --- | --- | --- | --- |
|  | 37160.56 |  | 3894.54 |
| 3 Social security wages |  | A Social security tav withheld |  |
|  | 37160.56 |  | 2303.95 |
| 5 Medicare wages and tips | 37160.56 | 6 Medicare tay withheld 538.83 |  |

| 7 Social security tips | 8 Allocated tips |  | 9 |
| --- | --- | --- | --- |
| 302.30 |  | 874.20 |  |
| 10 Dependent care benefits | 11 Nonqualified plans |  | 12a Code See inst. for box 12 |
| 9873.20 |  | 653.21 | DO 5939.68 |

| Model | Description | ModelID |
| --- | --- | --- |
| 1003 End-User License Agreement (EULA) | Extract loan, borrower, property details. | prebuilt-mortgage.us.1003 |


<!---- Page 145 ---------------------------------------------------------------------------------------------------------------------------------->
Model
Description
ModelID
1004 Uniform Residential Appraisal Report
(URAR))
Extract loan, borrower, property details.
prebuilt-mortgage.us.1004
1005 Verification of Employment
Extract loan, borrower, property details.
prebuilt-mortgage.us.1005
1008 Summary document
Extract borrower, seller, property, mortgage, and underwriting
details.
prebuilt-mortgage.us.1008
Closing disclosure
Extract closing, transaction costs, and loan details.
prebuilt-
mortgage.us.closingDisclosure
Sample Closing disclosure document processed using Document Intelligence Studio2 :
Run analysis
Query fields
Analyze options
Closing Disclosure
This form is a statement of final loan terms and closing costs. Compare this
document with your Loan Estimate.
Closing Information
Transaction Information
Loan Information
Date Issued
4/12/2027
Closing Date
Disbursement Date
4/17/2022
Borrower
Gwen Stacy
1634 W Glenoaks Blvd, Glendale,
CA 91201
Loan Term
Purpose
Purchase
Product
Fixed Rate
1/21/2027
Settlement Agent
Bright Title Co
Seller
Bonita Andersor
920 University Blvd, Rexburg.
File #
54-568
1634 W Glenoaks Blvd
Glendale, CA 91201
Loan Type
* Conventional
FHA
Property
CA 94954
OVO
Lender
Skye AU Bank
Loan ID #
MIC #
672047875
147188500
Sale Price
$1,800,000.00
Estimated Total
Loan Terms
Can this amount increase after closing?
Loan Amount
$1,440,000
NO
Interest Rate
7.033%
NO
Monthly Principal & Interest
See Projected Payments below for your
Estimated Total Monthly Payment
$9,609.39
NO
Prepayment Penalty
Does the loan have these features?
NO
Balloon Payment
NO
Projected Payments
Payment Calculation
Years 1-7
Years 8-30
Principal & Interest
$9,609.39
$9,609.39
Mortgage Insurance
+
$100.00
Estimated Escrow
Amount can increase over time
+
$1,775.00
+
$1,775.00
<
1
of 5
Q Q @ 2
Fields
Result
Code
DocType: mortgage.us.closingDisclosure
Closing
#1
E
A
ClosingDate
99.50%
99.50%
DisbursementDate
4/21/2022
FileNumber
99.50%
64-5681
IssueDate
99.50%
4/12/2022
PropertyAddress
99.50%
1634 W Glenoaks Blvd, Glendale, CA 91201
HouseNumber
1634
Road
W Glenoaks Blvd
PostalCode
91201
City
Glendale
State
Learn more: Mortgage document models
Contract
The contract model analyzes and extracts key fields and line items from contractual agreements including parties, jurisdictions, contract ID,
and title. The model currently supports English-language contract documents.
Sample contract processed using Document Intelligence Studio[2:
30 years
4/17/2022

| Model | Description | ModelID |
| --- | --- | --- |
| 1004 Uniform Residential Appraisal Report (URAR)) | Extract loan, borrower, property details. | prebuilt-mortgage.us.1004 |
| 1005 Verification of Employment | Extract loan, borrower, property details. | prebuilt-mortgage.us.1005 |
| 1008 Summary document | Extract borrower, seller, property, mortgage, and underwriting details. | prebuilt-mortgage.us.1008 |
| Closing disclosure | Extract closing, transaction costs, and loan details. | prebuilt- mortgage.us.closingDisclosure |

| Loan Terms |  | Can this amount increase after closing? |
| --- | --- | --- |
| Loan Amount | $1,440,000 | NO |
| Interest Rate | 7.033% | NO |
| Monthly Principal & Interest See Projected Payments below for your Estimated Total Monthly Payment | $9,609.39 | NO |
| Prepayment Penalty |  | Does the loan have these features? NO |
| Balloon Payment |  | NO |

| Projected Payments |  |  |
| --- | --- | --- |
| Payment Calculation | Years 1-7 | Years 8-30 |
| Principal & Interest | $9,609.39 | $9,609.39 |
| Mortgage Insurance | + $100.00 | :selected: |
| Estimated Escrow Amount can increase over time | + $1,775.00 | + :selected: $1,775.00 |


<!---- Page 146 ---------------------------------------------------------------------------------------------------------------------------------->
Ti Run analysis
Analyze options
Fields
Content
Result
Code
DocType: contract
Jurisdictions
#1
WEB HOSTING AGREEMENT
This web Hosting Agreement is entered as of this 15 day of October, 2022
"Effective Date") by and between Contoso Corporation a Washington corporation,
having its principal place of business at 1 Microsoft Way, Redmond!
Washington, 98058 "Contoso" and AdventureWorks Cycles a
Washington corporation, having its principal place of business at 98
NW 76st Street, Suite 54, Bellevue, Washington, 98007
"AdventureWorks" !!
Clause
This Agreement shall be governed by and construed in
accordance with the internal laws of the State of Washington
applicable to agreements made and to be performed entirely
within such state.
This agreement shall void and nullify any and all previous agreements to this
date between Contoso and AdventureWorks.
There shall be no additional fees of any kind paid to Contoso, other than
those stated within this agreement for software usage and/or bandwidth usage.
AdventureWorks agrees to pay Contoso $0.01 (one cent) per access up to
400,000 accesses thereafter payment shall be $0.005 (one-half cent) per access.
AdventureWorks shall send this amount to Contoso by no later than
Wednesday for accesses used from the previous week (Monday thru Sunday).
Parties (2)
#1
E
Region
Washington
Contoso must provide a person(s) to correct any technical problems (Server
being down or inaccessible) 24 hours per day, 7 days per week. This person(s)
must be available by beeper or telephone. AdventureWorks shall provide this
same 24 hour service at the broadcast location.
This Agreement shall be governed by and construed in accordance with the internal
laws of the State of Washington applicable to agreements made and to be performed
entirely within such state.
EffectiveDate
#1
99.90%
15 day of October, 2022
All parties have read and fully agree to all terms and conditions as set forth
in this Web Hosting Agreement.
ExecutionDate
#1
99.90%
Contoso Corporation
By: Angel Brown
Title: CTO
Angel Brown
Adventure Works Cycle
By: Aaron Smith
Title: CEO
Aaron Smith
15 day of October, 2022
Title
#1
99.90%
WEB HOSTING AGREEMENT
Learn more: contract model
US Bank Check
The contract model analyzes and extracts key fields from check including check details, account details, amount, memo, is extracted from
US bank checks.
A bank check sample processed using Document Intelligence Studio :
Contoso Bank
Contoso Ltd.
123 Main St, Redmond, WA 98052
No. 370654
98-2
125
Date: June 20, 2024
Pay
To The
Order Of
22nd Century Insurance
John Doe
$ *** 123,456.00
One Hundred Twenty-Three Thousand Four Hundred Fifty-Six And 00/100
Dollars
Uker
Memo: Fees & Charges
Authorized Signature
370654⑈
⑆125000024⑆
84950⑉55432⑈
WordAmount
Content
One Hundred Twenty-Three
Thousand Four Hundred Fifty-Six
And 00/100 Dollars
Value
123456
Confidence
99.50%
Learn more: contract model
US Bank Statement

| WordAmount :selected: |  |
| --- | --- |
| Content | One Hundred Twenty-Three Thousand Four Hundred Fifty-Six And 00/100 Dollars |
| Value | 123456 |
| Confidence | 99.50% |


<!---- Page 147 ---------------------------------------------------------------------------------------------------------------------------------->
The bank statement model analyzes and extracts key fields and line items from US bank statements account number, bank details,
statement details, and transaction details.
Sample bank statement processed using Document Intelligence Studio :
CONTOSO BANK
Contoso Bank
3513 Grove Stroot
ew York, NY 1001
Account Number
168552031585
Account Type
Simple Checking
Lela R. Duvall
2096 Godfrey Road
New York, NY 10021
Statement Period
Nov 1, 2023 - Nov 30, 2023
Account Summary
EndingBalance
Deposits / Credits
Total Deposits / Credits
$ 2,500.00
Withdrawals / Debits
Service Fees
Page 1 of 1
Classified as Microsoft Confidential
‹
1
of 1
>
Beginning Balance on Nov 1, 2023
$ 3,000.00
Deposits / Credits
+ 2,500.00
Withdrawals / Debits
- 1,200.00
Service Fees
- 10.00
Ending Balance on Nov 30, 2023
$ 4,290.00
Date
Description
Value
11/01/2023
Deposit
Confidence
11/29/2023
Deposit
Content
$ 4,290.00
4290
99.50%
Date
Description
Amount
11/02/2023
Online Payment
5200.00
11/18/2023
Online Transfer To xxxxxxxx1379
-1,000.00
Total Withdrawals / Debits
-$ 1,200.00
Date
Description
Amount
11/02/2023
International Transaction Fee
-10.00
Total Service Fees
$ 10.00
Learn more: contract model
PayStub
The paystub model analyzes and extracts key fields and line items from documents and files with payroll related information.
Sample paystub processed using Document Intelligence Studio :

| Beginning Balance on Nov 1, 2023 | $ 3,000.00 |
| --- | --- |
| Deposits / Credits | + 2,500.00 |
| Withdrawals / Debits | - 1,200.00 |
| Service Fees | - 10.00 |
| Ending Balance on Nov 30, 2023 | $ 4,290.00 |

| Date | Description | Value |
| --- | --- | --- |
| 11/01/2023 | Deposit | Confidence |
| 11/29/2023 | Deposit |  |

| Content | $ 4,290.00 |
| --- | --- |
|  | 4290 |
|  | 99.50% |

| Date | Description | Amount |
| --- | --- | --- |
| 11/02/2023 | Online Payment | 5200.00 |
| 11/18/2023 | Online Transfer To xxxxxxxx1379 | -1,000.00 |
| Total Withdrawals / Debits |  | -$ 1,200.00 |

| Date | Description | Amount |
| --- | --- | --- |
| 11/02/2023 | International Transaction Fee | -10.00 |
| Total Service Fees |  | $ 10.00 |


<!---- Page 148 ---------------------------------------------------------------------------------------------------------------------------------->
EARNINGS
HOURS
RATE
CURRENT
YTD
Regular Hours
56.00
24.00
$
1,344.00
$
5,000.00
Overtime
2.00
36.00
S
72.00
$
500.00
Sick Pay
8.00
24.00
S
192.00
$
500.00
Vacation Pay
8.00
24.00
$
192.00
$
500.00
Holiday Pay
8.00
24.00
S
192.00
$
500.00
82.00
S
1,992.00
$
7,000.00
DocType: payStub.us
CONTOSO LTD
EARNINGS STATEMENT
CurrentPeriodDeductions
#1
433
Pay Date
Pay Start
Pay End
1/12/2024
7/1/2024
17/2024
CurrentPeriodGrossPay
#1
1992
Carl Anderson
203 Denver Blvd
Seattle, WA 98040
CurrentPeriodNetPay
#1
1246.61
SSN:
997-60-1247
DOB: 01-09-1997
CurrentPeriod Taxes
#1
312.39
EmployeeAddress
#1
203 Denver Blvd Seattle, WA 98040
HouseNumber
203
Road
Denver Blvd
PostalCode
98040
City
Seattle
I
,246.61
3,785.78
State
WA
StreetAddress
203 Denver Blvd
CONTOSO LTD
1/12/2024
1001
123 Main Street
Redmond, WA 98052
EmployeeName
₩1
Carl Anderson
DEDUCTIONS
RATE
CURRENT
YTD
Aftertax Health Ins
S
120.00
$
500.00
Aftertax Dental Ins
$
35.00
$
400.00
Aftertax 401k
S
78.00
$
200.00
Child Support
S
155.00
$
200.00
Garnishment
$
45.00
S
433.00
$
500.00
S
1,800.00
TAXES
RATE
CURRENT
YTD
Federal Income Tax
S
100.00
$
500.00
Social Security
0.0620
$
123.50
$
475.22
Medicare
0.0145
S
28.88
$
199.00
State Income Tax
$
50.00
$
200.00
Local Income Tax
$
10.00
$
40.00
3
312 3
S
1,414.22
NET PAY
CURRENT
YTD
Direct Deposited to Account of:
Routing Number:
Account Number
Amount:
Carl Anderson
485066128
8300567112
$
1,246.61
AAAA
Learn more: contract model
Invoice
{ $)
The invoice model automates processing of invoices to extracts customer name, billing address, due date, and amount due, line items, and
other key data.
Sample invoice processed using Document Intelligence Studio[2:
Tu] Analyze
API version: 2021-09-30-preview
Values
Result
Code
CONTOSO LTD
INVOICE
AmountDue
#1
97.30%
onioso Headquarter
INVOICE:
23 456 St
New York, NY. 1000
INVOICE DATE
DUE DATE
114112018
9/11170
610
CUSTOMER NAME
SERVICE PERIOD
BISESFOIN
REALIZAIN
CUSTOMER ID
KINOPASS
MICHOSOIR COFF
23 Other St,
BillingAddress
#1
94.70%
edmond WA 9805
BILL TO:
SHIP TO:
SERVICE ADDRESS:
KIERAN STERN
123 Bill St, Redmond WA, 98052
Mitcrochetaktar
MIRTOJON DNIMVETY
123 Bill St,
Redmond WA. 98057
23 Ship St,
Redmond WA GRO5
23 Service St
edmond WA 9805
BillingAddressRecipient
#1
95.70%
Microsoft Finance
CustomerAddress
#1
94.70%
123 Other St, Redmond WA, 98052
CustomerAddressRecipient
#1
95.60%
Microsoft Corp
THANK YOU FOR YOUR BUSINESS!
Customerld #1
96.40%
REMIT TO:
oriolo
125 Hemit St
New York, NY. 10001
CID-12345
+
SALESPERSON
P.O. NUMBER
REQUISITIONER
SHIPPED VIA
F.O.B. POINT
TERMS
WEEEEE
DATE
ITEM CODE
DESCRIPTION
QTY
UM
PRICE
TAX
AMOUNT
FLY22
BFA
KRIZEMIG
I
INIna
$10010
HE
GRILO
1AZAZ
AT MEN
RT@00420
31000
-
17AU
WINNERS
-
SKO
50.00
SUBTOTAL
SALES TAX
TOTAL
PREVIOUS UNPAID BALANCE
AMOUNT DUE
100000
500
50 00
0100
56 10 00
Learn more: invoice model

| EARNINGS | HOURS | RATE | CURRENT | YTD |
| --- | --- | --- | --- | --- |
| Regular Hours | 56.00 | 24.00 | $ 1,344.00 | $ 5,000.00 |
| Overtime | 2.00 | 36.00 | S 72.00 | $ 500.00 |
| Sick Pay | 8.00 | 24.00 | S 192.00 | $ 500.00 |
| Vacation Pay | 8.00 | 24.00 | $ 192.00 | $ 500.00 |
| Holiday Pay | 8.00 | 24.00 | S 192.00 | $ 500.00 |
|  | 82.00 |  | S 1,992.00 | $ 7,000.00 |

| DEDUCTIONS | RATE | CURRENT | YTD |
| --- | --- | --- | --- |
| Aftertax Health Ins |  | S 120.00 | $ 500.00 |
| Aftertax Dental Ins |  | $ 35.00 | $ 400.00 |
| Aftertax 401k |  | S 78.00 | $ 200.00 |
| Child Support |  | S 155.00 | $ 200.00 |
| Garnishment |  | $ 45.00 S 433.00 | $ 500.00 S 1,800.00 |

| TAXES | RATE | CURRENT | YTD |
| --- | --- | --- | --- |
| Federal Income Tax |  | S 100.00 | $ 500.00 |
| Social Security | 0.0620 | $ 123.50 | $ 475.22 |
| Medicare | 0.0145 | S 28.88 | $ 199.00 |
| State Income Tax |  | $ 50.00 | $ 200.00 |
| Local Income Tax |  | $ 10.00 | $ 40.00 |
|  |  | 3 312 3 | S 1,414.22 |
| NET PAY |  | CURRENT | YTD |

| Direct Deposited to Account of: | Routing Number: | Account Number | Amount: |
| --- | --- | --- | --- |
| Carl Anderson | 485066128 | 8300567112 | $ 1,246.61 |

|  | SALESPERSON | P.O. NUMBER | REQUISITIONER | SHIPPED VIA | F.O.B. POINT | TERMS |
| --- | --- | --- | --- | --- | --- | --- |
|  |  | WEEEEE |  |  |  |  |

| DATE | ITEM CODE | DESCRIPTION | QTY | UM | PRICE | TAX | AMOUNT |
| --- | --- | --- | --- | --- | --- | --- | --- |
| FLY22 | :unselected: BFA | KRIZEMIG |  | I INIna | $10010 HE |  | GRILO |
| 1AZAZ | AT MEN | RT@00420 |  |  | 31000 - |  |  |
|  |  |  |  |  |  |  |  |
| 17AU |  | WINNERS | - |  | SKO 50.00 |  |  |

| SUBTOTAL SALES TAX TOTAL PREVIOUS UNPAID BALANCE AMOUNT DUE | 100000 |
| --- | --- |
|  | 500 |
|  | 50 00 |
|  | 0100 |
|  | 56 10 00 |


<!---- Page 149 ---------------------------------------------------------------------------------------------------------------------------------->
Receipt
Use the receipt model to scan sales receipts for merchant name, dates, line items, quantities, and totals from printed and handwritten
receipts. The version v3.0 also supports single-page hotel receipt processing.
Sample receipt processed using Document Intelligence Studio :
In Analyze
API version: 2021-09-30-preview
Values
Result
Code
Contoso
Items (2)
#1
1 Name
98.40%
123 Main Street
Redmond, WA 9805:
Surface Pro 6
Quantity
96.90%
987-654-3210
1
TotalPrice
98.50%
6/10/2019 13:59
999
Sales Associate: Paul
2 Name
96.90%
1
Surface Pro (
$999.00
SurfacePen
1
SurfacePer
$99.99
Quantity
97.00%
1
Sub-Total
$1098.99
Tax
$104.40
TotalPrice
98.50%
Total
99.99
$1203.39
+
Locale
98.70%
Learn more: receipt model
Identity document (ID)
0
Use the Identity document (ID) model to process U.S. Driver's Licenses (all 50 states and District of Columbia) and biographical pages from
international passports (excluding visa and other travel documents) to extract key fields.
Sample U.S. Driver's License processed using Document Intelligence Studio 2:


<!---- Page 150 ---------------------------------------------------------------------------------------------------------------------------------->
Analyze
ov
Values
Result
Code
Address
#1
A
87.20%
123 STREET ADDRESS YOUR CITY WA 99999-1234
CountryRegion
USA
99.50%
WA
USA
WASHINGTON
DRIVER LICENSE
FEDERAL LIMITS APPLY
DateOfBirth
#1
98.90%
4d LIC#WDLABCD456DG
9CLASS B DONOR
1958-01-06
20 1234567XX1101
TALBOT
2LIAM R.
DateOfExpiration
#1
98.60%
2020-08-12
3 DOB 01/06/1958
4a ISS 01/06/2015
123 STREET ADDRESS
YOUR CITY WA 99999-1234
DocumentNumber
#1
97.50%
WDLABCD456DG
15 SEX M
16 HGT 5'-08"
18 EYES BLU
17 WGT 165 lb
Endorsements
#1
98.40%
12 RESTRICTIONS
B
9a END L.
4b EXP 08/12/2020
L
5 DDWDLABCD456DG1234567XX1101
Veteran
REV 07/01/2018
FirstName
#1
84.60%
LIAM R.
LastName
#1
93.10%
TALBOT
Locale
100.00%
+
<
1
of 1
>
en-US
Learn more: identity document model
Marriage certificate
Use the marriage certificate model to process U.S. marriage certificates to extract key fields including the individuals, date, and location.
Sample U.S. marriage certificate processed using Document Intelligence Studio :
Run analysis
Query fields
Analyze options
O
Fields
Result
Code
STATE OF MICHIGAN
CERTIFICATION OF VITAL RECORD.
DocType: marriageCertificate.us
A
er prired ebay plated by typewriter
COUNTY OF KENT
STATE OF MICHIGAN
IssueDate
#1
99.50%
Except for signature, spaces left
24245482
Marriage License
State of Michigan
March 22, 2017
91517
All of the following appears as of record
MarriageDate
#1
98.60%
and Recorded in Liber 8 of RECORD OF MARRIAGE on page 53 Record Number: 311
Luke Eli Harrison
between
and
Myla Kate Calderon
FULL NAME OF FEMALE (FIRST, WODLE, LAST)
19th March AD. 2017
VAL NAME OF MALE (FIRST, MIDDLE, LAST
30
American Indian
26
AST ROME BREF OFNI FRET MANHEG. W DIFFERENT
American Indian
MarriagePlace
#1
99.50%
Detroit, Michigan
AGE
Lansing, Michigar
CITY. STATE
1
CITY, STATE
None
By Authority of MCL 333 2813
Detroit Wayne MICHIGAN,
PRIVECURLY MARNES
Detroit, Michigar
Wyoming, Michigar
Dentist
Accountant
Mike Harrison
Gary Calderon
Spouse 1Address
#1
Detroit, Michigan
99.50%
Jaida Mayer
Helen Larson
HOTHERS FULL BARRE
An affidavit has been filed in this office by which it appears that side statements are true. This marriage
authorized to perform a matrace celere under the above the State of Michigan by any person
marriage ceremony laws of the State of Michigan.
Date of Application
March 12, 2017
City
Date Recorded
Ivan Reese
This Space Reserved for Binding
Wayne
COUNTY
COUNTY CLERK
Annabelle Nash
Detroit
DEPUTY GLERE
Cerificate of Marriage
State
Between Mr.
Luke
El Harrisor
and M
Myla Kate Calderor
In accordance with the above license, the persons herein mentioned were joined in
Detroit
county of
Wayne
MICHIGAN
Michigan
marriage, in
on the
19th
dav of
March
AD. 2017
in the presence of
StreetAddress
Abby Watts
Brian Watts
Detroit, Michigan
Detroit, Michigan
Taylor Hensley
Celebrant
In war service against Germany
Spouse 1Age
#1
99.50%
STATE OF MICHIGAN
I.MAMY HOLLINRAKE. CLERK OF KENT COUNTY DO HEREBY CERTIFY but the foregoing in a trat and exact
orpy of the original document en file in the office of the County Clerk.
m
MAST HOLLINHAKE
CUỐNTY CLEAK
30
March 22. 2017
Spouse 1BirthPlace
#1
99.50%
<
1
of 1 >
Detroit, Michigan


<!---- Page 151 ---------------------------------------------------------------------------------------------------------------------------------->
Learn more: identity document model
Credit card
18
1)
Use the credit card model to process credit and debit cards to extract key fields.
Sample credit card processed using Document Intelligence Studio:
Ti Run analysis
Query fields
Analyze options
Fields
Result
Code
DocType: creditCard
CardHolderName
#1
99.50%
Contoso Bank:
E
›))
5412 1234 5656 8888
ADAM SMITH
CardNumber
#1
99.50%
5412 1234 5656 8888
VALID
01/28
CardVerificationValue
123
#1
99.50%
THRU
ADAM SMITH
mastercont
CustomerServicePhoneNumbers (2)
#1
E
For customer servies, ca
+1 200-345-6789
200-000-888
1 +1 200-345-6789
2 +1 200-000-8888
ExpirationDate
#1
123
99.50%
01/28
NOT VALID UNLESS SIGNED
Lorem ipsum dolor sit amet, consectetuer adipiscing elit, sed diam nonummy nibh
euismod tincidunt ut laoreet dolore magna aliquam erat volutpat.
IssuingBank
#1
99.50%
Contoso Bank
PaymentNetwork
#1
99.10%
mastercard
+
<
1
of 1
Learn more: identity document model
Custom models
FIT
Custom models can be broadly classified into two types. Custom classification models that support classification of a "document type" and
custom extraction models that can extract a defined schema from a specific document type.


<!---- Page 152 ---------------------------------------------------------------------------------------------------------------------------------->
Custom
models
Extraction
Classification
Neural
Template
Custom document models analyze and extract data from forms and documents specific to your business. They recognize form fields within
your distinct content and extract key-value pairs and table data. You only need one example of the form type to get started.
Version v3.0 and later custom models support signature detection in custom template (form) and cross-page tables in both template and
neural models. Signature detection looks for the presence of a signature, not the identity of the person who signs the document. If the
model returns unsigned for signature detection, the model didn't find a signature in the defined field.
Sample custom template processed using Document Intelligence Studio ? :
Invoice Number
Invoice Date
Invoice Due Date
Charges
VAT ID
34278587
6/18/2017
6/24/2017
$56,651.49
PT
Label data
Region
Train
-
Contoso
Address:
1 Redmond way Suite
6000 Redmond, WA
99243
Invoice For: Microsoft
1020 Enterprise Way
Sunnayvale, CA 87659
Receipt No
Invoice_1.pdf
2468
Sold To
Fabrikam Residences
Invoice_2.pdf
ID #
1197531
Live Delivery?
unselected
Invoice_3.pdf
Online Delivery?
selected
Video Delivery?
selected
Invoice_4.pdf
Learn more: custom model
Custom extraction

| Invoice Number | Invoice Date | Invoice Due Date | Charges | VAT ID |
| --- | --- | --- | --- | --- |
| 34278587 :unselected: | 6/18/2017 | 6/24/2017 :unselected: | $56,651.49 | PT :unselected: |


<!---- Page 153 ---------------------------------------------------------------------------------------------------------------------------------->
Custom extraction model can be one of two types, custom template, custom neural. To create a custom extraction model, label a dataset
of documents with the values you want extracted and train the model on the labeled dataset. You only need five examples of the same
form or document type to get started.
Sample custom extraction processed using Document Intelligence Studio :
Custom extraction models
Extract information from forms and documents with custom extraction models. Train a model by labeling as few as 5 example documents. (The same labeled dataset can train all types of custom extraction models.) Learn more about custom extraction models.
Template (Custom form) models
Template models work well when the target documents share a common visual layout. Training only takes a few minutes, and
more than 100 languages are supported.
Form
W-9
Request for Taxpayer
Identification Number and Certification
Give Form to the
requester. Do not
send to the IRS.
(Plow. October 2018)
Horas Revenue tardes
>> Go to www.irs.gov/FormWVS for instructions and the latest information.
1 Name (as shown on your income tax return). Name is required on this Ine; do not leave this line blank.
JORT 19%
2 lhusnatt hametinregarded aritty nima, & cafferent from above
Arcter Ing
Print of type
totowing seven DORes
ungio-mombar LLC
lee Specific Instructions on page 3.
Other (une instructionm) >
3 Check appropriate box tor tedersi tax clemsitoation of the person whose name is entered on ine 1. Check only one of the
4 Exactions (codos apply only to
Instructions on page 2):
Individual/sole proprietor or
C Corporation
Partnership
Inunstastata
Eximpt payue code (If any)
6
Limnad saniity company. Enter the tax classification (C.C corporation, Sus corporation, PuPartnership(>>
Note: Check the appropriate box in the line above for the tax classification of the single-member owner. Do not check
LLC t tre LLC is classified as a single-mamiter LLC that is disregarded from the owner unless the owner of the LLC is
anomer LLC that is not disregarded trem the owner tor U.S. federal tax purposes. Omerwise, a single-member LLC tha
is disregarded from the owner should check the appropriate box for the tax classification of es owner.
E
Exemption from FATGA Reporting
code (if any)
B
100 TAS Address
Jamie Doe
Neural (Custom document) models
Neural models can flexibly handle both structured and unstructured documents. Training takes up to half an hour, and currently
only English language documents are supported. The current version can extract inline field data and checkboxes.
Neural models are available only in select regions. Click here for details.
HOUSE RENTAL AGREEMENT
This House Rental Agreement ("Agreement," "rental agreement," or "lease") is entered
Opayı
G
Harry
Byme
(Tenants). If more than one person is named as Tenant they shall be jointly and severally liable
and responsible under the terms of this Agreement. This lease Agreement involves a residential
house, yard, and related facilities located at ( Bellewir Dr. Sal are City Hai EESN (the
"premises"). The date of this Agreement is
Entiary
5
1.
Landlord rents to Tenant, unfurnished, the premises on a month to month basis,
terminable by either party at the end of any calendar month on at least 30 days notice to the other
party. Tenant shall be entitled to possession of the premises and rent shall commence on[
April
Tenant shall not assign, sublease, or allow anyone other than persons permitted under this
lease to at any time he in nossession of anv nortion of the nremises. Landlord will provide five (5)
Learn more: custom template model
Learn more: custom neural model
Custom classifier
The custom classification model enables you to identify the document type before invoking the extraction model. The classification model
is available starting with the 2023-07-31 (GA) API. Training a custom classification model requires at least two distinct classes and a
minimum of five samples per class.
Learn more: custom classification model
Composed models
A composed model is created by taking a collection of custom models and assigning them to a single model built from your form types.
You can assign multiple custom models to a composed model called with a single model ID. You can assign up to 200 trained custom
models to a single composed model.
Composed model dialog window in Document Intelligence Studio 2:


<!---- Page 154 ---------------------------------------------------------------------------------------------------------------------------------->
Cognitive Services | Form Recognizer Studio - Preview
«
Form Recognizer Studio > Custom Form > composed > Models
= Custom Form
Models
composed
Compose
Test
Download
Delete
Label data
Model ID
Model Description
Models
8aa 16866-16fe-44ca-b13a-8bfc6ad1d
Test
773fb140-f173-47a2-8aa9-a5fce1ceb
Settings
4c493f98-87c3-4f6d-b0d8-3a1aab49e
Learn more: custom model
Input requirements
Supported file formats:
[] Expand table
Model
PDF
JPEG/JPG, PNG, BMP, TIFF, HEIF
Image:
Microsoft Office:
Word (DOCX ), Excel (XLSX), PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom classification
✔
✔
✔
. For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription, only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free (FO) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x 10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel image. This dimension corresponds to about 8
point text at 150 dots per inch (DPI).
. For custom model training, the maximum number of pages for training data is 500 for the custom template model and 50,000 for the
custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB with a maximum of 10,000 pages. For 2024-11-30
(GA), the total size of training data is 2 GB with a maximum of 10,000 pages.
Note
The Sample Labeling tool [ doesn't support the BMP file format. The limitation is derived from the tool not the Document
Intelligence Service.
Version migration

| Model | PDF | JPEG/JPG, | Image: PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel (XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- | --- |
| Read | :selected: ✔ | :selected: ✔ :unselected: |  | :selected: ✔ |
| Layout | :selected: ✔ | :selected: ✔ :unselected: |  | ✔ :selected: |
| General Document | :selected: ✔ | :selected: ✔ :unselected: |  |  |
| Prebuilt | :selected: ✔ | :selected: ✔ |  |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |  |
| Custom classification | :selected: ✔ | :selected: ✔ |  | :selected: ✔ |


<!---- Page 155 ---------------------------------------------------------------------------------------------------------------------------------->
Learn how to use Document Intelligence v3.0 in your applications by following our Document Intelligence v3.1 migration guide
Next steps
. Try processing your own forms and documents with the Document Intelligence Studio [2.
· Complete a Document Intelligence quickstart and get started creating a document processing app in the development language of
your choice.
Feedback
Was this page helpful?
Yes
No
Provide product feedback E | Get help at Microsoft Q&A


<!---- Page 156 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence bank check
model
Article · 12/11/2024
The Document Intelligence bank check model combines powerful Optical Character
Recognition (OCR) capabilities with deep learning models to analyze and extract data
from US bank checks. The API analyzes printed checks; extracts key information, and
returns a structured JSON data representation. The latest version 4.0 for bank check
supports signature detection on bank checks.
[] Expand table
Feature
version
Model ID
Check model
v4.0: 2024-11-30 (GA)
prebuilt-check. us
Check data extraction
A check is a secure way to transfer amount from payee's account to receiver's account.
Businesses use check to pay their vendors as a signed document to instruct the bank for
payment. See how data, including check details, account details, amount, memo, is
extracted from bank check US. You need the following resources:
. An Azure subscription-you can create one for free
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.

| Feature | version | Model ID |
| --- | --- | --- |
| Check model | v4.0: 2024-11-30 (GA) | prebuilt-check. us |


<!---- Page 157 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
P Search
«
C
Regenerate Key1
Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
X Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
P
& Encryption
KEY 2
Pricing tier
P
Networking
Location/Region
Identity
westus2
₼
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
0
Locks
Monitoring
> Automation
Ls
Help
Document Intelligence Studio
9
Note
Document Intelligence Studio is available with v3.1 and v3.0 APIs.
1. On the Document Intelligence Studio home page , select check.
2. You can analyze the sample check or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options :
Run analysis
Analyze options
Try Document Intelligence Studio
Input requirements
· Supported file formats:


<!---- Page 158 ---------------------------------------------------------------------------------------------------------------------------------->
[] Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
. The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
· If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | :selected: ✔ | :selected: ✔ | ✔ :selected: |
| General Document | ✔ :selected: | ✔ :selected: |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |
| Custom classification | :selected: ✔ | :selected: ✔ | ✔ :selected: |
|  |  |  |  |


<!---- Page 159 ---------------------------------------------------------------------------------------------------------------------------------->
Supported languages and locales
For a complete list of supported languages, see our prebuilt model language support
page.
Field extractions
For supported document extraction fields, see the bank check model schema _ page in
our GitHub sample repository.
Supported locales
The prebuilt-check. us version 2024-11-30 (GA) supports the en-us locale.
Next steps
· Try processing your own forms and documents with the Document Intelligence
Studio
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
& Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 160 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence bank statement
model
Article · 12/11/2024
The Document Intelligence bank statement model combines powerful Optical Character
Recognition (OCR) capabilities with deep learning models to analyze and extract data
from US bank statements. The API analyzes printed bank statements; extracts key
information such as account number, bank details, statement details, transaction details,
and fees; and returns a structured JSON data representation. With V4.0 GA, you can now
extract check tables in the US bank statements.
[ Expand table
Feature
version
Model ID
Bank statement model
v4.0: 2024-11-30 (GA)
prebuilt-bankStatement.us
Bank statement data extraction
A bank statement helps review account's activities during a specified period. It's an
official statement that helps in detecting fraud, tracking expenses, accounting errors and
record the period's activities. See how data is extracted using the prebuilt-
bankStatement. us model. You need the following resources:
. An Azure subscription-you can create one for free
. A Document Intelligence instance [ in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.

| Feature | version | Model ID |
| --- | --- | --- |
| Bank statement model | v4.0: 2024-11-30 (GA) | prebuilt-bankStatement.us |


<!---- Page 161 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
P Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
r
& Encryption
KEY 2
Pricing tier
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
> Automation
Lo
> Help
Document Intelligence Studio
1. On the Document Intelligence Studio home page , select bank statements.
2. You can analyze the sample bank statement or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options :
Run analysis
Analyze options
Try Document Intelligence Studio
Input requirements
· Supported file formats:
[ Expand table
Model
PDF
Image:
Microsoft Office:
Word (DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
JPEG/JPG, PNG,
TIFF , HEIF
BMP ,
Read
✔
✔
✔

| Model | PDF | Image: |  | Microsoft Office: Word (DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- | --- |
|  |  | JPEG/JPG, TIFF , | PNG, BMP , HEIF |  |
| Read | :selected: ✔ | :selected: ✔ |  | :selected: ✔ |


<!---- Page 162 ---------------------------------------------------------------------------------------------------------------------------------->
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Supported languages and locales
For a complete list of supported languages, see our prebuilt model language support
page.

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Layout | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | ✔ :selected: | ✔ :selected: |  |
| Custom classification | :selected: ✔ | :selected: ✔ | ✔ :selected: |
|  |  |  |  |


<!---- Page 163 ---------------------------------------------------------------------------------------------------------------------------------->
Field extractions
For supported document extraction fields, see the bank statement model schema
page in our GitHub sample repository.
Supported locales
The prebuilt-bankStatement.us version 2027-11-30 supports the en-us locale.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 164 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence contract model
Article · 12/11/2024
This content applies to:
v4.0 (GA) | Previous version:
v3.1 (GA) ::: moniker-end
The Document Intelligence contract model uses powerful Optical Character Recognition
(OCR) capabilities to analyze and extract key fields and line items from a select group of
important contract entities. Contracts can be of various formats and quality including
phone-captured images, scanned documents, and digital PDFs. The API analyzes
document text; extracts key information such as Parties, Jurisdictions, Contract ID, and
Title; and returns a structured JSON data representation. The model currently supports
English-language document formats.
Automated contract processing
Automated contract processing is the process of extracting key contract fields from
documents. Historically, the contract analysis process is achieved manually and, hence,
very time consuming. Accurate extraction of key data from contracts is typically the first
and one of the most critical steps in the contract automation process.
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
[] Expand table
Feature
Resources
Model ID
Contract model
. Document Intelligence Studio EZ
· REST API
prebuilt-contract
· C# SDK
· Python SDK
· Java SDK
· JavaScript SDK
Input requirements
· Supported file formats:

| Feature | Resources | Model ID |
| --- | --- | --- |
| Contract model | . Document Intelligence Studio EZ · REST API | prebuilt-contract |
|  | · C# SDK |  |
|  | · Python SDK |  |
|  | · Java SDK |  |
|  | · JavaScript SDK |  |


<!---- Page 165 ---------------------------------------------------------------------------------------------------------------------------------->
[] Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF , HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX ),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
. The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
· If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF , HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX ), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | :selected: ✔ | :selected: ✔ | ✔ :selected: |
| General Document | ✔ :selected: | ✔ :selected: |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |
| Custom classification | :selected: ✔ | :selected: ✔ | ✔ :selected: |
|  |  |  |  |


<!---- Page 166 ---------------------------------------------------------------------------------------------------------------------------------->
Try contract document data extraction
See how data, including customer information, vendor details, and line items, is
extracted from contracts. You need the following resources:
. An Azure subscription-you can create one for free .
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
Search
«
Regenerate Key1 Regenerate Key2
Overview
Activity log
Access control (IAM)
i
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
Diagnose and solve problems
Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
I
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
Automation
Lo
Help
Document Intelligence Studio
1. On the Document Intelligence Studio home page , select Tax Documents.
2. You can analyze the sample tax documents or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options:
Run analysis
Analyze options
Try Document Intelligence Studio


<!---- Page 167 ---------------------------------------------------------------------------------------------------------------------------------->
Supported languages and locales
For a complete list of supported languages, see our Language Support-prebuilt models
page.
Field extraction
. For supported document extraction fields, see the contract model schema page
in our GitHub sample repository.
. The contract key-value pairs and line items extracted are in the documentResults
section of the JSON output.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
No
Provide product feedback z | Get help at Microsoft Q&A


<!---- Page 168 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence credit card
model
Article · 12/11/2024
This content applies to:
v4.0
The Document Intelligence credit/debit card model uses powerful Optical Character
Recognition (OCR) capabilities to analyze and extract key fields from credit and debit
cards. Credit cards and debit cards can be of various formats and quality including
phone-captured images, scanned documents, and digital PDFs. The API analyzes
document text; extracts key information such as card number, issuing bank, and
expiration date; and returns a structured JSON data representation. The model currently
supports English-language document formats.
Automated card processing
Automated Credit/Debit card processing is the process of extracting key fields from
bank cards. Historically, bank card analysis process is achieved manually and, hence, very
time consuming. Accurate extraction of key data from bank cards is typically the first
and one of the most critical steps in the contract automation process.
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
[] Expand table
Feature
Resources
Model ID
Contract model
· Document Intelligence Studio
· REST API
· C# SDK
prebuilt-creditCard
· Python SDK
· Java SDK
· JavaScript SDK
Input requirements
· Supported file formats:

| Feature | Resources | Model ID |
| --- | --- | --- |
| Contract model | · Document Intelligence Studio · REST API · C# SDK | prebuilt-creditCard |
|  | · Python SDK |  |
|  | · Java SDK |  |
|  | · JavaScript SDK |  |


<!---- Page 169 ---------------------------------------------------------------------------------------------------------------------------------->
[] Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF , HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX ),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
. The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
· If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF , HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX ), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | :selected: ✔ | :selected: ✔ | ✔ :selected: |
| General Document | ✔ :selected: | ✔ :selected: |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |
| Custom classification | :selected: ✔ | :selected: ✔ | ✔ :selected: |
|  |  |  |  |


<!---- Page 170 ---------------------------------------------------------------------------------------------------------------------------------->
Try credit card data extraction
To see how data extraction works for the credit/debit card service, you need the
following resources:
. An Azure subscription-you can create one for free .
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
P Search
«
Regenerate Key1 Regenerate Key2
Overview
Activity log
Access control (IAM)
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
Diagnose and solve problems
Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
I
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
Automation
Lo
Help
Document Intelligence Studio
1. On the Document Intelligence Studio home page , select Credit/Debit Card.
2. You can analyze the sample credit/debit documents or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options :
Run analysis
Analyze options
Try Document Intelligence Studio


<!---- Page 171 ---------------------------------------------------------------------------------------------------------------------------------->
Supported languages and locales
For a complete list of supported languages, see our prebuilt model language support
page.
Field extraction
. For supported document extraction fields, see the credit card model schema
page in our GitHub sample repository.
. The bank cards key-value pairs and line items extracted are in the documentResults
section of the JSON output.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio Z .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
P
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 172 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence health insurance
card model
Article · 12/11/2024
This content applies to:
v4.0 | Previous versions:
v3.1 (GA)
v3.0 (GA) :::
moniker-end
The Document Intelligence health insurance card model combines powerful Optical
Character Recognition (OCR) capabilities with deep learning models to analyze and
extract key information from US health insurance cards. A health insurance card is a key
document for care processing. It can be digitally analyzed for patient onboarding,
financial coverage information, cashless payments, and insurance claim processing. The
health insurance card model analyzes health card images; extracts key information such
as insurer, member, prescription, and group number; and returns a structured JSON
representation. Health insurance cards can be presented in various formats and quality
including phone-captured images, scanned documents, and digital PDFs.
Sample health insurance card processed using Document Intelligence Studio
Analyze
API version: 2022-03-31-preview V
Fields
Result
Code
GroupNumber
#1
80.00%
PREMERA
1000000
BLUE CROSS
Microsoft
IdNumber
Member
Medical Network HERITAGE
ANGEL BROWN
Premera Dental
YES
Number
87.00%
Prefix Identification # Suffix
Premera Vision
YES
ABC 123456789
123456789
01
Group # 1000000
HEALTH SAVINGS PLAN
Insurer
#1
87.00%
Rx Group # BCAAXYZ
Rx BINE 987654
Deductible
Shared In and Out of Network
$1.500
PREMERA
Coinsurance Mas
$1,000
BCBS 456
Member
·1
Note: Rx and Medical Cost-Shares are Shared
Rx
Name
100.00%
PPO
ANGEL BROWN
PrescriptionInfo
#1
<
1
of 1
>
®
Q
2
RxBIN
80.00%
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
[] Expand table


<!---- Page 173 ---------------------------------------------------------------------------------------------------------------------------------->
Feature
Resources
Model ID
Health insurance card
· Document Intelligence
prebuilt-
model
Studio Z
· REST API
healthInsuranceCard.us
· C# SDK
· Python SDK
· Java SDK
· JavaScript SDK
Input requirements
· Supported file formats:
[] Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX ),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.

| Feature | Resources | Model ID |
| --- | --- | --- |
| Health insurance card | · Document Intelligence | prebuilt- |
| model | Studio Z · REST API | healthInsuranceCard.us |
|  | · C# SDK |  |
|  | · Python SDK |  |
|  | · Java SDK |  |
|  | · JavaScript SDK |  |

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX ), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | ✔ :selected: | :selected: ✔ | ✔ :selected: |
| Layout | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | :selected: ✔ | :selected: ✔ |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |
| Custom classification | :selected: ✔ | :selected: ✔ | :selected: ✔ |


<!---- Page 174 ---------------------------------------------------------------------------------------------------------------------------------->
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Try Document Intelligence Studio
See how data is extracted from health insurance cards using the Document Intelligence
Studio. You need the following resources:
. An Azure subscription-you can create one for free z.
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier ( F0 ) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
Search
«
Regenerate Key1 Regenerate Key2
Overview
Activity log
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
X Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
Encryption
KEY 2
Pricing tier
D
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
₼
Locks
Monitoring
Lo
Automation
Help


<!---- Page 175 ---------------------------------------------------------------------------------------------------------------------------------->
!
Note
Document Intelligence Studio is available with API version v3.0.
1. On the Document Intelligence Studio home page , select Health insurance cards.
2. You can analyze the sample insurance card document or select the + Add button
to upload your own sample.
3. Select the Run analysis button and, if necessary, configure the Analyze options :
Run analysis
Analyze options
Try Document Intelligence Studio
.
Supported languages and locales
For a complete list of supported languages, see our prebuilt model language support
page.
Field extraction
For supported document extraction fields, see the health insurance card model
schema [ page in our GitHub sample repository.
Migration guide and REST API v3.1
. Follow our Document Intelligence v3.1 migration guide to learn how to use the
v3.1 version in your applications and workflows.
. Explore our REST API to learn more about the v3.1 version and new capabilities.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio [7 .


<!---- Page 176 ---------------------------------------------------------------------------------------------------------------------------------->
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 177 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence ID document
model
Article · 02/10/2025
This content applies to:
v3.0 (GA)
v4.0 (GA) | Previous versions:
v3.1 (GA)
v2.1 (GA)
::: moniker-end
Document Intelligence Identity document (ID) model combines Optical Character
Recognition (OCR) with deep learning models to analyze and extract key information
from identity documents. The API analyzes identity documents (including the following)
and returns a structured JSON data representation.
[] Expand table
Region
Document types
Worldwide
Passport Book, Passport Card
United
Driver License, Identification Card, Residency Permit (Green card), Social Security
States
Card, Military ID
Europe
Driver License, Identification Card, Residency Permit
India
Driver License, PAN Card, Aadhaar Card
Canada
Driver License, Identification Card, Residency Permit (Maple Card)
Australia
Driver License, Photo Card, Key-pass ID (including digital version)
Identity document processing
Identity document processing involves extracting data from identity documents either
manually or by using OCR-based technology. ID document processing is an important
step in any business operation that requires proof of identity. Examples include
customer verification in banks and other financial institutions, mortgage applications,
medical visits, claim processing, hospitality industry, and more. Individuals provide some
proof of their identity via driver licenses, passports, and other similar documents so that
the business can efficiently verify them before providing services and benefits.
Sample U.S. Driver's License processed with Document Intelligence Studio

| Region | Document types |
| --- | --- |
| Worldwide | Passport Book, Passport Card |
| United | Driver License, Identification Card, Residency Permit (Green card), Social Security |
| States | Card, Military ID |
| Europe | Driver License, Identification Card, Residency Permit |
| India | Driver License, PAN Card, Aadhaar Card |
| Canada | Driver License, Identification Card, Residency Permit (Maple Card) |
| Australia | Driver License, Photo Card, Key-pass ID (including digital version) |


<!---- Page 178 ---------------------------------------------------------------------------------------------------------------------------------->
[[ Analyze
0
V
Values
Result
Code
Address
#1
4
87.20%
123 STREET ADDRESS YOUR CITY WA 99999-1234
CountryRegion
USA
99.50%
WA
USA
WASHINGTON
DRIVER LICENSE
FEDERAL LIMITS APPLY
DateOfBirth
#1
98.90%
1958-01-06
20 1234567XX1101
4d LIC# WDLABCD456DG
9CLASS B DONOR
1
TALBOT
2
LIAM R.
DateOfExpiration
#1
98.60%
2020-08-12
3 DOB
01/06/1958
4a ISS 01/06/2015
123 STREET ADDRESS
YOUR CITY WA 99999-1234
DocumentNumber
#1
97.50%
WDLABCD456DG
15 SEX MM
16 HGT 5'-08"
18 EYES BLU
17 WGT 165 lb
Endorsements
#1
98.40%
12 RESTRICTIONS
B
9a END I.
4b EXP
08/12/2020
L
5 DDWDLABCD456DG1234567XX1101
Veteran
REV 07/01/2018
FirstName
#1
84.60%
LIAM R.
LastName
#1
93.10%
TALBOT
Locale
100.00%
< 1
of 1
>
en-US
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
0
Expand table
Feature
Resources
Model ID
ID document model
· Document Intelligence Studio [
· REST API
· C# SDK
· Python SDK
· Java SDK
prebuilt-idDocument
· JavaScript SDK
Input requirements
Supported file formats:
0
Expand table

| Feature | Resources | Model ID |
| --- | --- | --- |
| ID document model | · Document Intelligence Studio [ · REST API · C# SDK · Python SDK · Java SDK | prebuilt-idDocument |
|  |  |  |
|  | · JavaScript SDK |  |


<!---- Page 179 ---------------------------------------------------------------------------------------------------------------------------------->
Model
PDF
Image:
Microsoft Office:
Word (DOCX ), Excel (XLSX ), PowerPoint
( PPTX ), HTML
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
· If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
ID document model data extraction

| Model | PDF | Image: | Microsoft Office: Word (DOCX ), Excel (XLSX ), PowerPoint ( PPTX ), HTML |
| --- | --- | --- | --- |
|  |  | JPEG/JPG, PNG, BMP, TIFF, HEIF |  |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | ✔ :selected: | :selected: ✔ | ✔ :selected: |
| General Document | ✔ :selected: | ✔ :selected: |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | ✔ :selected: | :selected: ✔ |  |
| Custom classification | ✔ :selected: | ✔ :selected: | :selected: ✔ |


<!---- Page 180 ---------------------------------------------------------------------------------------------------------------------------------->
Extract data, including name, birth date, and expiration date, from ID documents. You
need the following resources:
. An Azure subscription-you can create one for free .
. A Document Intelligence instance [ in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
Diagnose and solve problems
Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
C
<> Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
Automation
Ls
Help
1 Note
Document Intelligence Studio is available with v3.1 and v3.0 APIs and later versions.
1. On the Document Intelligence Studio home page , select Identity documents.
2. You can analyze the sample invoice or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options:
Run analysis
Analyze options
Try Document Intelligence Studio
.


<!---- Page 181 ---------------------------------------------------------------------------------------------------------------------------------->
Field extractions
For supported document extraction fields, see the ID document model schema page
in our GitHub sample repository.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio Z .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
. Find more samples on GitHub.
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 182 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence invoice model
Article · 12/11/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
::: moniker-end
The Document Intelligence invoice model uses powerful Optical Character Recognition
(OCR) capabilities to analyze and extract key fields and line items from sales invoices,
utility bills, and purchase orders. Invoices can be of various formats and quality including
phone-captured images, scanned documents, and digital PDFs. The API analyzes invoice
text; extracts key information such as customer name, billing address, due date, and
amount due; and returns a structured JSON data representation. The model currently
supports invoices in 27 languages.
Supported document types:
· Invoices
· Utility bills
· Sales orders
· Purchase orders
Automated invoice processing
Automated invoice processing is the process of extracting key accounts payable fields
from billing account documents. Extracted data includes line items from invoices
integrated with your accounts payable (AP) workflows for reviews and payments.
Historically, the accounts payable process is performed manually and, hence, very time
consuming. Accurate extraction of key data from invoices is typically the first and one of
the most critical steps in the invoice automation process.
Sample invoice processed with Document Intelligence Studio :


<!---- Page 183 ---------------------------------------------------------------------------------------------------------------------------------->
I
Analyze
&
V
Values
Result
Code
AmountDue
#1
97.30%
4
CONTOSO LTD]
INVOICE
610
BillingAddress
#1
94.70%
Contoso Headquarter
INVOICE: DUSIT
23 456 51
lew York NY. 1000
INVOICE DATE
DUE DATE:
123 Bill St, Redmond WA, 98052
CUSTOMER NAME:
SERVICE PERIOD:
CUSTOMER ID
Microsoft Com
23 Other St.
edmond WA 9805
BillingAddressRecipient
#1
95.70%
Microsoft Finance
BILL TO:
SHIP TO:
SERVICE ADDRESS:
123 Bill St,
Redmond WA 98057
23 Ship St,
edmond WA 98052
23 Service St,
Redmond WA 98057
CustomerAddress
#1
94.70%
123 Other St, Redmond WA, 98052
CustomerAddressRecipient
#1
95.60%
Microsoft Corp
Customerld
#1
96.40%
CID-12345
CustomerName
#1
94.90%
MICROSOFT CORPORATION
THANK YOU FOR YOUR BUSINESS!
REMIT TO:
ontoso Bli
123 Remit St
New York, NY. 10001
DueDate #1
97.30%
2019-12-15
++
InvoiceDate
#1
97.20%
<
1
of 1 >
Q Q @ 2
2019-11-15
SALESPERSON
P.O. NUMBER
REQUISITIONER
SHIPPED VIA
F.O.B. POINT
TERMS
DATE
ITEM CODE
DESCRIPTION
QTY
UM
PRICE
TAX
AMOUNT
O
-
DL
-
O
-
-
SUBTOTAL
SALES TAX
51000
TOTAL
50.00
PREVIOUS UNPAID BALANCE
5300 0
AMOUNT DUE
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
0
Expand table
Feature
Resources
Model ID
Invoice model
· Document Intelligence Studio z
· REST API
· C# SDK
· Python SDK
· Java SDK
· JavaScript SDK
prebuilt-invoice
Input requirements
· Supported file formats:
Expand table
Model
PDF
Image:
Microsoft Office:
Word ( DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
JPEG/JPG, PNG,
BMP ,
TIFF , HEIF
Read
✔
✔
✔

| :unselected: | SALESPERSON | P.O. NUMBER | REQUISITIONER | SHIPPED VIA | F.O.B. POINT | TERMS |
| --- | --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |

| DATE | ITEM CODE | DESCRIPTION | QTY | UM | PRICE | TAX | AMOUNT |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  |  |  | O |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |
|  | - |  | DL |  |  |  |  |  |
|  | - |  | O | - |  | - |  |  |

| :unselected: |  |  | SUBTOTAL |  |
| --- | --- | --- | --- | --- |
|  |  |  | SALES TAX | 51000 |
|  |  |  | TOTAL | 50.00 |
|  |  | PREVIOUS UNPAID BALANCE |  | 5300 0 |
|  |  |  | AMOUNT DUE |  |

| Feature | Resources | Model ID |
| --- | --- | --- |
| Invoice model | · Document Intelligence Studio z · REST API · C# SDK · Python SDK · Java SDK · JavaScript SDK | prebuilt-invoice |
|  |  |  |
|  |  |  |

| Model | PDF | Image: | Microsoft Office: Word ( DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
|  |  | JPEG/JPG, PNG, BMP , TIFF , HEIF |  |
| Read | :selected: ✔ | :selected: ✔ | ✔ :selected: |


<!---- Page 184 ---------------------------------------------------------------------------------------------------------------------------------->
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Invoice model data extraction
See how data, including customer information, vendor details, and line items, is
extracted from invoices. You need the following resources:

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Layout | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | ✔ :selected: | ✔ :selected: |  |
| Custom classification | :selected: ✔ | :selected: ✔ | ✔ :selected: |
|  |  |  |  |


<!---- Page 185 ---------------------------------------------------------------------------------------------------------------------------------->
. An Azure subscription-you can create one for free .
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
×
Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
................................
D
& Encryption
KEY 2
Pricing tier
................................
P
<- > Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
Le
Automation
> Help
1. On the Document Intelligence Studio home page , select Invoices.
2. You can analyze the sample invoice or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options :
Run analysis
Analyze options
Try Document Intelligence Studio
Supported languages and locales
For a complete list of supported languages, see our prebuilt model language support
page.


<!---- Page 186 ---------------------------------------------------------------------------------------------------------------------------------->
Field extraction
. For supported document extraction fields, see the invoice model schema [ page
in our GitHub sample repository.
. The invoice key-value pairs and line items extracted are in the documentResults
section of the JSON output.
Key-value pairs
The prebuilt invoice model supports the optional return of key-value pairs. By default,
the return of key-value pairs is disabled. Key-value pairs are specific spans within the
invoice that identify a label or key and its associated response or value. In an invoice,
these pairs could be the label and the value the user entered for that field or telephone
number. The AI model is trained to extract identifiable keys and values based on a wide
variety of document types, formats, and structures.
Keys can also exist in isolation when the model detects that a key exists, with no
associated value or when processing optional fields. For example, a middle name field
can be left blank on a form in some instances. Key-value pairs are always spans of text
contained in the document. For documents where the same value is described in
different ways, for example, customer/user, the associated key is either customer or user
(based on context).
JSON output
The JSON output has three parts:
. "readResults" node contains all of the recognized text and selection marks. Text is
organized via page, then by line, then by individual words.
. "pageResults" node contains the tables and cells extracted with their bounding
boxes, confidence, and a reference to the lines and words in readResults.
. "documentResults" node contains the invoice-specific values and line items that
the model discovered. It's where to find all the fields from the invoice such as
invoice ID, ship to, bill to, customer, total, line items and lots more.
Migration guide
. Follow our Document Intelligence v3.1 migration guide to learn how to use the
v3.0 version in your applications and workflows.


<!---- Page 187 ---------------------------------------------------------------------------------------------------------------------------------->
::: moniker-end
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio Z .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
. Find more samples on GitHub.
Feedback
Was this page helpful?
Yes
No
Provide product feedback @ | Get help at Microsoft Q&A


<!---- Page 188 ---------------------------------------------------------------------------------------------------------------------------------->
What is Document Intelligence layout
model?
Article · 05/05/2025
v2.1
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
(GA)
Document Intelligence layout model is an advanced machine-learning based document
analysis API available in the Document Intelligence cloud. It enables you to take documents in
various formats and return structured data representations of the documents. It combines an
enhanced version of our powerful Optical Character Recognition (OCR) capabilities with deep
learning models to extract text, tables, selection marks, and document structure.
Document structure layout analysis
Document structure layout analysis is the process of analyzing a document to extract regions
of interest and their inter-relationships. The goal is to extract text and structural elements from
the page to build better semantic understanding models. There are two types of roles in a
document layout:
· Geometric roles: Text, tables, figures, and selection marks are examples of geometric
roles.
· Logical roles: Titles, headings, and footers are examples of logical roles of texts.
The following illustration shows the typical components in an image of a sample page.


<!---- Page 189 ---------------------------------------------------------------------------------------------------------------------------------->
Page header
This is the header of the document.
Title
This is title
3. Others
Section heading
1. Text
Paragraph
Latin refers to an ancient Italic language
originating in the region of Latium in
ancient Rome.
Al Document Intelligence is an Al service
that applies advanced machine learning
to extract text, key-value pairs, tables,
and structures from documents
automatically and accurately:
2. Page Objects
clear
Subsection heading
2.1 Table
precise
Here's a sample table below, designed to
be simple for easy understand and quick
reference.
vague
Selection marks
coherent
Table
Incomprehensible
Table caption
Turn documents into usable data and
shift your focus to acting on information
rather than compiling it. Start with
prebuilt models or create custom models
tailored to your documents both on
premises and in the cloud with the Al
Document Intelligence studio or SDK.
Table 1: This is a dummy table
Figure caption
2.2. Figure
Figure 1: Here is a figure with text
Values
500
450
Figure
400
400
350
Learn how to accelerate your business
processes by automating text extraction
with Al Document Intelligence. This
webinar features hands-on demos for key
use cases such as document processing,
knowledge mining, and industry-specific
Al model customization.
300
300
250
200
200
100
0
Jan
Feb
Mar
Apr
May
Jun
Months
Page number
Page footer
This is the footer of the document.
1 | Page
Name
Corp
Remark
Foo
Bar
Microsoft
Dummy
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications, and
libraries:
[ Expand table
Feature
Resources
Model ID
Layout model
· Document Intelligence Studio
· REST API
· C# SDK
· Python SDK
prebuilt-layout

| Name | Corp | Remark |
| --- | --- | --- |
| Foo |  |  |
| Bar | Microsoft | Dummy |

| Feature | Resources | Model ID |
| --- | --- | --- |
| Layout model | · Document Intelligence Studio · REST API · C# SDK · Python SDK | prebuilt-layout |


<!---- Page 190 ---------------------------------------------------------------------------------------------------------------------------------->
Feature
Resources
Model ID
· Java SDK
· JavaScript SDK
Supported languages
See Language Support-document analysis models for a complete list of supported languages.
Supported file types
Document Intelligence v4.0: 2024-11-30 (GA) layout model supports the following file formats:
Expand table
Model PDF
Image:
Microsoft Office:
JPEG/JPG, PNG, BMP, TIFF,
Word (DOCX ), Excel (XLSX ), PowerPoint (PPTX ),
HEIF
HTML
Layout
✔
✔
✔
Input requirements
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription, only
the first two pages are processed).
. If your PDFs are password-locked, you must remove the lock before submission.
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free (FO)
tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x 10,000
pixels.
. The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel image.
This dimension corresponds to about 8 point text at 150 dots per inch (DPI).
. For custom model training, the maximum number of pages for training data is 500 for the
custom template model and 50,000 for the custom neural model.

| Feature | Resources | Model ID |
| --- | --- | --- |
|  | · Java SDK |  |
|  | · JavaScript SDK |  |

| Model PDF | Image: |  |  | Microsoft Office: |
| --- | --- | --- | --- | --- |
|  | JPEG/JPG, | PNG, | BMP, TIFF, | Word (DOCX ), Excel (XLSX ), PowerPoint (PPTX ), |
|  |  | HEIF |  | HTML |
| Layout :selected: ✔ |  | ✔ :selected: |  | ✔ :selected: |


<!---- Page 191 ---------------------------------------------------------------------------------------------------------------------------------->
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB with a
maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training data is 2 GB
with a maximum of 10,000 pages.
For more information on model usage, quotas, and service limits, see service limits.
Get started with Layout model
See how data, including text, tables, table headers, selection marks, and structure information
is extracted from documents using Document Intelligence. You need the following resources:
. An Azure subscription-you can create one for free .
. A Document Intelligence instance [ in the Azure portal. You can use the free pricing tier
(F0) to try the service. After your resource deploys, select Go to resource to get your key
and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
Search
«
Regenerate Key1
Regenerate Key2
Overview
Activity log
& Access control (IAM)
i
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
· Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
Encryption
KEY 2
Pricing tier
T
Networking
Location/Region
Identity
westus2
0
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
> Automation
Ls
Help
After you retrieve your key and endpoint, use the following development options to build and
deploy your Document Intelligence applications:
REST API


<!---- Page 192 ---------------------------------------------------------------------------------------------------------------------------------->
· Document Intelligence REST API
· How to guide
Data extraction
The layout model extracts structural elements from your documents. To follow are descriptions
of these structural elements with guidance on how to extract them from your document input:
· Pages
· Paragraphs
· Text, lines, and words
· Selection marks
· Tables
· Output response to markdown
· Figures
· Sections
Run the sample layout document analysis within Document Intelligence Studio, then
navigate to the results tab and access the full JSON output.
Content
Result
Code
JSON V
1
{
2
"status": "succeeded",
3
"createdDateTime": "2025-03-27T19:41: 38Z",
4
"lastUpdatedDateTime": "2025-03-27T19:41:39Z",
5
"analyzeResult": {
6
"apiVersion": "2024-11-30",
7
"modelId": "prebuilt-layout",
8
"stringIndexType": "utf16CodeUnit",
9
"content": "This is the header of the document. \nThis is title\n1.
10
"pages": [
Pages
The pages collection is a list of pages within the document. Each page is represented
sequentially within the document and includes the orientation angle indicating if the page is
rotated and the width and height (dimensions in pixels). The page units in the model output
are computed as shown:

| Content |  | Result | Code |  |
| --- | --- | --- | --- | --- |
| JSON V |  |  |  |  |
| 1 | { |  |  |  |
| 2 |  | "status": "succeeded", |  |  |
| 3 | "createdDateTime": "2025-03-27T19:41: 38Z", |  |  |  |
| 4 | "lastUpdatedDateTime": "2025-03-27T19:41:39Z", |  |  |  |
| 5 | "analyzeResult": { |  |  |  |
| 6 |  | "apiVersion": "2024-11-30", |  |  |
| 7 |  | "modelId": "prebuilt-layout", |  |  |
| 8 |  | "stringIndexType": "utf16CodeUnit", |  |  |
| 9 |  | "content": "This is the header of the document. \nThis is title\n1. |  |  |
| 10 |  | "pages": | [ |  |


<!---- Page 193 ---------------------------------------------------------------------------------------------------------------------------------->
0
Expand table
File format
Computed page unit
Total pages
Images (JPEG/JPG, PNG,
BMP, HEIF)
Each image = 1 page unit
Total images
PDF
Each page in the PDF = 1 page unit
Total pages in the PDF
TIFF
Each image in the TIFF = 1 page unit
Total images in the TIFF
Word (DOCX)
Up to 3,000 characters = 1 page unit, embedded
or linked images not supported
Total pages of up to 3,000
characters each
Excel (XLSX)
Each worksheet = 1 page unit, embedded or
linked images not supported
Total worksheets
PowerPoint (PPTX)
Each slide = 1 page unit, embedded or linked
images not supported
Total slides
HTML
Up to 3,000 characters = 1 page unit, embedded
or linked images not supported
Total pages of up to 3,000
characters each
Sample code
Python
# Analyze pages.
for page in result.pages:
print(f" ---- Analyzing layout from page #{page. page_number} ---- ")
print(f"Page has width: {page.width} and height: {page.height}, measured with
unit: {page.unit}")
View samples on GitHub.
Extract selected pages
For large multi-page documents, use the pages query parameter to indicate specific page
numbers or page ranges for text extraction.
Paragraphs
The Layout model extracts all identified blocks of text in the paragraphs collection as a top
level object under analyzeResults. Each entry in this collection represents a text block and

| File format | Computed page unit | Total pages |
| --- | --- | --- |
| Images (JPEG/JPG, PNG, BMP, HEIF) | Each image = 1 page unit | Total images |
| PDF | Each page in the PDF = 1 page unit | Total pages in the PDF |
| TIFF | Each image in the TIFF = 1 page unit | Total images in the TIFF |
| Word (DOCX) | Up to 3,000 characters = 1 page unit, embedded or linked images not supported | Total pages of up to 3,000 characters each |
| Excel (XLSX) | Each worksheet = 1 page unit, embedded or linked images not supported | Total worksheets |
| PowerPoint (PPTX) | Each slide = 1 page unit, embedded or linked images not supported | Total slides |
| HTML | Up to 3,000 characters = 1 page unit, embedded or linked images not supported | Total pages of up to 3,000 characters each |


<!---- Page 194 ---------------------------------------------------------------------------------------------------------------------------------->
.. /includes the extracted text as content and the bounding polygon coordinates. The span
information points to the text fragment within the top level content property that contains the
full text from the document.
JSON
"paragraphs": [
{
"spans": [],
"boundingRegions": [],
"content": "While healthcare is still in the early stages of its Al
journey, we are seeing pharmaceutical and other life sciences organizations making
major investments in Al and related technologies. \" TOM LAWRY | National Director
for Al, Health and Life Sciences | Microsoft"
}
]
Paragraph roles
The new machine-learning based page object detection extracts logical roles like titles, section
headings, page headers, page footers, and more. The Document Intelligence Layout model
assigns certain text blocks in the paragraphs collection with their specialized role or type
predicted by the model. It's best to use paragraph roles with unstructured documents to help
understand the layout of the extracted content for a richer semantic analysis. The following
paragraph roles are supported:
0
'Expand table
Predicted role
Description
Supported file types
title
The main headings in the page
pdf, image, docx, pptx, xlsx, html
sectionHeading
One or more subheadings on the page
pdf, image, docx, xlsx, html
footnote
Text near the bottom of the page
pdf, image
pageHeader
Text near the top edge of the page
pdf, image, docx
pageFooter
Text near the bottom edge of the page
pdf, image, docx, pptx, html
pageNumber
Page number
pdf, image
JSON
{
"paragraphs": [

| Predicted role | Description | Supported file types |
| --- | --- | --- |
| title | The main headings in the page | pdf, image, docx, pptx, xlsx, html |
| sectionHeading | One or more subheadings on the page | pdf, image, docx, xlsx, html |
| footnote | Text near the bottom of the page | pdf, image |
| pageHeader | Text near the top edge of the page | pdf, image, docx |
| pageFooter | Text near the bottom edge of the page | pdf, image, docx, pptx, html |
| pageNumber | Page number | pdf, image |


<!---- Page 195 ---------------------------------------------------------------------------------------------------------------------------------->
{
"spans": [],
"boundingRegions": [],
"role": "title",
"content": "NEWS TODAY"
},
{
"spans": [],
"boundingRegions": [],
"role": "sectionHeading",
"content": "Mirjam Nilsson"
}
]
}
Text, lines, and words
The document layout model in Document Intelligence extracts print and handwritten style text
as lines and words . The styles collection includes any handwritten style for lines if detected
along with the spans pointing to the associated text. This feature applies to supported
handwritten languages.
For Microsoft Word, Excel, PowerPoint, and HTML, Document Intelligence v4.0 2024-11-30 (GA)
Layout model extract all embedded text as is. Texts are extracted as words and paragraphs.
Embedded images aren't supported.
Sample code
Python
# Analyze lines.
if page. lines:
for line_idx, line in enumerate(page.lines) :
words = get_words (page, line)
print(
f" ... Line # {line_idx} has word count {len(words)} and text
'{line. content} ' "
f"within bounding polygon ' {line. polygon} '"
)
# Analyze words.
for word in words:
print(f" ...... Word ' {word. content}' has a confidence of
{word. confidence} ")


<!---- Page 196 ---------------------------------------------------------------------------------------------------------------------------------->
View samples on GitHub.
Handwritten style for text lines
The response .. /includes classifying whether each text line is of handwriting style or not, along
with a confidence score. For more information. See Handwritten language support. The
following example shows an example JSON snippet.
JSON
"styles": [
{
"confidence": 0.95,
"spans": [
{
"offset": 509,
"length": 24
}
"isHandwritten": true
]
}
If you enable the font/style addon capability, you also get the font/style result as part of the
styles object.
Selection marks
The Layout model also extracts selection marks from documents. Extracted selection marks
appear within the pages collection for each page. They include the bounding polygon,
confidence, and selection state (selected/unselected). The text representation (that is,
: selected: and : unselected) is also included as the starting index ( offset ) and length that
references the top level content property that contains the full text from the document.
Sample code
Python
# Analyze selection marks.
if page. selection_marks:
for selection_mark in page. selection_marks:
print(
f"Selection mark is ' {selection_mark. state}' within bounding
polygon "
f"' {selection_mark. polygon}' and has a confidence of


<!---- Page 197 ---------------------------------------------------------------------------------------------------------------------------------->
{selection_mark. confidence}"
)
View samples on GitHub.
Tables
Extracting tables is a key requirement for processing documents containing large volumes of
data typically formatted as tables. The Layout model extracts tables in the pageResults section
of the JSON output. Extracted table information .. /includes the number of columns and rows,
row span, and column span. Each cell with its bounding polygon is output along with
information whether the area is recognized as a columnHeader or not. The model supports
extracting tables that are rotated. Each table cell contains the row and column index and
bounding polygon coordinates. For the cell text, the model outputs the span information
containing the starting index ( offset ). The model also outputs the length within the top-level
content that contains the full text from the document.
Here are a few factors to consider when using the Document Intelligence bale extraction
capability:
· Is the data that you want to extract presented as a table, and is the table structure
meaningful?
· Can the data fit in a two-dimensional grid if the data isn't in a table format?
· Do your tables span multiple pages? If so, to avoid having to label all the pages, split the
PDF into pages before sending it to Document Intelligence. After the analysis, post-
process the pages to a single table.
. Refer to Tabular fields if you're creating custom models. Dynamic tables have a variable
number of rows for each column. Fixed tables have a constant number of rows for each
column.
1 Note
· Table analysis isn't supported if the input file is XLSX.
· For 2024-11-30 (GA), the bounding regions for figures and tables cover only the core
content and exclude associated caption and footnotes.


<!---- Page 198 ---------------------------------------------------------------------------------------------------------------------------------->
Sample code
Python
if result. tables:
for table_idx, table in enumerate(result. tables) :
print(f"Table # {table_idx} has {table. row_count} rows and " f"
{table. column_count} columns")
if table.bounding_regions:
for region in table.bounding_regions:
print(f"Table # {table_idx} location on page:
{region. page_number} is {region. polygon}")
# Analyze cells.
for cell in table.cells:
print(f". . . Cell[{cell.row_index} ] [{cell. column_index} ] has text
'{cell. content} '")
if cell.bounding_regions:
for region in cell.bounding_regions:
print(f" ... content on page {region.page_number} is within
bounding polygon '{region. polygon} '")
View samples on GitHub.
Output response to markdown format
The Layout API can output the extracted text in markdown format. Use the
outputContentFormat=markdown to specify the output format in markdown. The markdown
content is output as part of the content section.
1 Note
For v4.0 2024-11-30 (GA), the representation of tables is changed to HTML tables to
enable rendering of merged cells, multi-row headers, etc. Another related change is to use
Unicode checkbox characters
and
for selection marks instead of : selected: and
: unselected: . This update means that the content of selection mark fields contains
: selected: even though their spans refer to Unicode characters in the top-level span.
Refer to the Markdown Output Format for full definition of Markdown elements.
Sample code
Python


<!---- Page 199 ---------------------------------------------------------------------------------------------------------------------------------->
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
poller = document_intelligence_client.begin_analyze_document (
"prebuilt-layout",
AnalyzeDocumentRequest(url_source=url),
output_content_format=ContentFormat. MARKDOWN,
)
View samples on GitHub.
Figures
Figures (charts, images) in documents play a crucial role in complementing and enhancing the
textual content, providing visual representations that aid in the understanding of complex
information. The figures object detected by the Layout model has key properties like
boundingRegions (the spatial locations of the figure on the document pages, including the page
number and the polygon coordinates that outline the figure's boundary), spans (details the
text spans related to the figure, specifying their offsets and lengths within the document's text.
This connection helps in associating the figure with its relevant textual context), elements (the
identifiers for text elements or paragraphs within the document that are related to or describe
the figure) and caption if there's any.
When output=figures is specified during the initial analyze operation, the service generates
cropped images for all detected figures that can be accessed via
/analyeResults/{resultId}/figures/{figureId}. FigureId is included in each figure object,
following an undocumented convention of {pageNumber} . {figureIndex} where figureIndex
resets to one per page.
1 Note
For v4.0 2024-11-30 (GA), the bounding regions for figures and tables cover only the core
content and exclude associated caption and footnotes.
Sample code
Python
# Analyze figures.
if result. figures:
for figures_idx, figures in enumerate(result. figures) :


<!---- Page 200 ---------------------------------------------------------------------------------------------------------------------------------->
print(f"Figure # {figures_idx} has the following spans:
{figures . spans } ")
for region in figures.bounding_regions:
print(f"Figure # {figures_idx} location on page:
{region. page_number} is within bounding polygon ' {region. polygon}'")
View samples on GitHub.
Sections
Hierarchical document structure analysis is pivotal in organizing, comprehending, and
processing extensive documents. This approach is vital for semantically segmenting long
documents to boost comprehension, facilitate navigation, and improve information retrieval.
The advent of retrieval-augmented generation (RAG) in document generative AI underscores
the significance of hierarchical document structure analysis. The Layout model supports
sections and subsections in the output, which identifies the relationship of sections and object
within each section. The hierarchical structure is maintained in elements of each section. You
can use output response to markdown format to easily get the sections and subsections in
markdown.
Sample code
Python
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
poller = document_intelligence_client.begin_analyze_document(
"prebuilt-layout",
AnalyzeDocumentRequest(url_source=url),
output_content_format=ContentFormat. MARKDOWN,
)
View samples on GitHub.
Next steps
. Learn how to process your own forms and documents with the Document Intelligence
Studio z .


<!---- Page 201 ---------------------------------------------------------------------------------------------------------------------------------->
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
. Find more samples on GitHub.


<!---- Page 202 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence marriage
certificate model
Article · 12/11/2024
This content applies to:
v4.0 (GA)
The Document Intelligence Marriage Certificate model uses powerful Optical Character
Recognition (OCR) capabilities to analyze and extract key fields from Marriage
Certificates. Marriage certificates can be of various formats and quality including phone-
captured images, scanned documents, and digital PDFs. The API analyzes document
text; extracts key information such as Spouse names, Issue date, and marriage place; and
returns a structured JSON data representation. The model currently supports English-
language document formats.
Automated marriage certificate processing
Automated marriage certificate processing is the process of extracting key fields from
Marriage certificates. Historically, the marriage certificate analysis process is achieved
manually and, hence, very time consuming. Accurate extraction of key data from
marriage certificates is typically the first and one of the most critical steps in the
marriage certificate automation process.
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
[] Expand table
Feature
Resources
Model ID
prebuilt-
· Document Intelligence
prebuilt-
marriageCertificate.us
Studio
· REST API
marriageCertificate.us
· C# SDK
· Python SDK
· Java SDK
. JavaScript SDK

| Feature | Resources | Model ID |
| --- | --- | --- |
| prebuilt- | · Document Intelligence | prebuilt- |
| marriageCertificate.us | Studio · REST API | marriageCertificate.us |
|  | · C# SDK |  |
|  | · Python SDK |  |
|  | · Java SDK |  |
|  | . JavaScript SDK |  |


<!---- Page 203 ---------------------------------------------------------------------------------------------------------------------------------->
Input requirements
· Supported file formats:
[] Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word ( DOCX ), Excel ( XLSX),
PowerPoint ( PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
. The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word ( DOCX ), Excel ( XLSX), PowerPoint ( PPTX ), HTML |
| --- | --- | --- | --- |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | :selected: ✔ | :selected: ✔ |  |
| Custom extraction | ✔ :selected: | ✔ :selected: |  |
| Custom classification | :selected: ✔ | :selected: ✔ | ✔ :selected: |


<!---- Page 204 ---------------------------------------------------------------------------------------------------------------------------------->
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Try marriage certificate document data
extraction
To see how data extraction works for the marriage certificate card service, you need the
following resources:
. An Azure subscription-you can create one for free .
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
×
Search
«
Regenerate Key1
Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
Diagnose and solve problems
Resource Management
Show Keys
Keys and Endpoint
KEY 1
Encryption
KEY 2
Pricing tier
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
> Automation
La
Help
Document Intelligence Studio
1. On the Document Intelligence Studio home page , select Marriage Certificate.
2. You can analyze the sample Marriage certificates or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options:


<!---- Page 205 ---------------------------------------------------------------------------------------------------------------------------------->
Run analysis
Analyze options
Try Document Intelligence Studio
Supported languages and locales
For a complete list of supported languages, see our prebuilt model language support
page.
Field extraction
. For supported document extraction fields, see the marriage certificate model
schema _ page in our GitHub sample repository.
. The marriage certificate key-value pairs and line items extracted are in the
documentResults section of the JSON output.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio 2 .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 206 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence mortgage
document models
Article · 12/11/2024
This content applies to:
v4.0 (GA)
The Document Intelligence Mortgage models use powerful Optical Character
Recognition (OCR) capabilities and deep learning models to analyze and extract key
fields from mortgage documents. Mortgage documents can be of various formats and
quality. The API analyzes mortgage documents and returns a structured JSON data
representation. The models currently support English-language documents only. With
the latest V4.0, you can now extract signatures from mortgage applications and forms.
Supported document types:
· Uniform Residential Loan Application (Form 1003)
· Uniform Residential Appraisal Report (Form 1004)
· Verification of employment form (Form 1005)
· Uniform Underwriting and Transmittal Summary (Form 1008)
· Closing Disclosure form
Development options
Document Intelligence v4.0 (2024-11-30-GA) supports the following tools, applications,
and libraries:
Expand table
Feature
Resources
Model ID
Mortgage
· Document Intelligence
· prebuilt-mortgage.us.1003
model
Studio [
· prebuilt-mortgage.us.1004
· REST API
· prebuilt-mortgage.us.1005
· C# SDK
· prebuilt-mortgage.us.1008
· Python SDK
· prebuilt-
· Java SDK
· JavaScript SDK
mortgage.us.closingDisclosure
Input requirements
· Supported file formats:

| Feature | Resources | Model ID |
| --- | --- | --- |
| Mortgage | · Document Intelligence | · prebuilt-mortgage.us.1003 |
| model | Studio [ | · prebuilt-mortgage.us.1004 |
|  | · REST API | · prebuilt-mortgage.us.1005 |
|  | · C# SDK | · prebuilt-mortgage.us.1008 |
|  | · Python SDK | · prebuilt- |
|  | · Java SDK · JavaScript SDK | mortgage.us.closingDisclosure |


<!---- Page 207 ---------------------------------------------------------------------------------------------------------------------------------->
[] Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF , HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX ),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
. The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
· If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF , HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX ), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | :selected: ✔ | :selected: ✔ | ✔ :selected: |
| General Document | ✔ :selected: | ✔ :selected: |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |
| Custom classification | :selected: ✔ | :selected: ✔ | ✔ :selected: |
|  |  |  |  |


<!---- Page 208 ---------------------------------------------------------------------------------------------------------------------------------->
Try mortgage documents data extraction
To see how data extraction works for the mortgage documents service, you need the
following resources:
. An Azure subscription-you can create one for free .
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
Search
«
Regenerate Key1
2
Regenerate Key2
Overview
Activity log
Access control (IAM)
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
Diagnose and solve problems
Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
I
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
Automation
Lo
Help
Document Intelligence Studio
1. On the Document Intelligence Studio home page , select Mortgage.
2. You can analyze the sample mortgage documents or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options:
Run analysis
Analyze options
Try Document Intelligence Studio


<!---- Page 209 ---------------------------------------------------------------------------------------------------------------------------------->
Supported languages and locales
See our Language Support-prebuilt models page for a complete list of supported
languages.
Field extraction
For supported document extraction fields, see the mortgage document model schema
pages in our GitHub sample repository.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio [7 .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
P No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 210 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence payStub model
Article · 12/11/2024
The Document Intelligence payStub model combines powerful Optical Character
Recognition (OCR) capabilities with deep learning models to analyze and extract
compensation and earnings data from pay slips. The API analyzes documents and files
with payroll related information; extracts key information and returns a structured JSON
data representation.
[ Expand table
Feature
version
Model ID
payStub model
v4.0: 2024-11-30 (GA)
prebuilt-payStub. us
Try payStub data extraction
Pay stubs are essential documents issued by employers to employees, providing
earnings, deductions, and net pay information for a specific pay period. See how data is
extracted using prebuilt-payStub. us model. You need the following resources:
. An Azure subscription-you can create one for free
. A Document Intelligence instance [ in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.

| Feature | version | Model ID |
| --- | --- | --- |
| payStub model | v4.0: 2024-11-30 (GA) | prebuilt-payStub. us |


<!---- Page 211 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
P Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
& Encryption
KEY 2
Pricing tier
r
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
> Automation
Los
> Help
Document Intelligence Studio
1. On the Document Intelligence Studio home page z, select payStub.
2. You can analyze the sample pay stub or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options :
Input requirements
· Supported file formats:
[] Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX ),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX ), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| Layout | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | :selected: ✔ | :selected: ✔ |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |


<!---- Page 212 ---------------------------------------------------------------------------------------------------------------------------------->
Model
PDF
Image:
JPEG/JPG, PNG,
BMP
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
. The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Supported languages and locales
For a complete list of supported languages, see our prebuilt model language support
page.
Field extractions
For supported document extraction fields, see the payStub model schema [ page in our
GitHub sample repository.

| Model | PDF | Image: JPEG/JPG, PNG, BMP TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Custom classification | ✔ :selected: | ✔ :selected: | ✔ :selected: |


<!---- Page 213 ---------------------------------------------------------------------------------------------------------------------------------->
Supported locales
The prebuilt-payStub.us version supports the en-us locale.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 214 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence read model
Article · 02/19/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
1 Note
To extract text from external images like labels, street signs, and posters, use the
Azure AI Image Analysis v4.0 Read feature optimized for general (not document)
images with a performance-enhanced synchronous API. This capability makes it
easier to embed OCR in real-time user experience scenarios.
Document Intelligence Read Optical Character Recognition (OCR) model runs at a higher
resolution than Azure AI Vision Read and extracts print and handwritten text from PDF
documents and scanned images. It also includes support for extracting text from
Microsoft Word, Excel, PowerPoint, and HTML documents. It detects paragraphs, text
lines, words, locations, and languages. The Read model is the underlying OCR engine for
other Document Intelligence prebuilt models like Layout, General Document, Invoice,
Receipt, Identity (ID) document, Health insurance card, W2 in addition to custom
models.
What is Optical Character Recognition?
Optical Character Recognition (OCR) for documents is optimized for large text-heavy
documents in multiple file formats and global languages. It includes features like higher-
resolution scanning of document images for better handling of smaller and dense text;
paragraph detection; and fillable form management. OCR capabilities also include
advanced scenarios like single character boxes and accurate extraction of key fields
commonly found in invoices, receipts, and other prebuilt scenarios.
Development options (v4)
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
[] Expand table


<!---- Page 215 ---------------------------------------------------------------------------------------------------------------------------------->
Feature
Resources
Model ID
Read OCR model
· Document Intelligence Studio
· REST API
· C# SDK
prebuilt-read
· Python SDK
· Java SDK
· JavaScript SDK
Input requirements (v4)
Supported file formats:
[] Expand table
Model
PDF
Image:
Microsoft Office:
Word (DOCX ), Excel (XLSX ), PowerPoint
(PPTX ), HTML
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
. The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
. The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch

| Feature | Resources | Model ID |
| --- | --- | --- |
| Read OCR model | · Document Intelligence Studio · REST API · C# SDK | prebuilt-read |
|  | · Python SDK · Java SDK |  |
|  |  |  |
|  | · JavaScript SDK |  |

| Model | PDF | Image: |  | Microsoft Office: Word (DOCX ), Excel (XLSX ), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- | --- |
|  |  | JPEG/JPG, PNG, BMP, TIFF, HEIF |  |  |
| Read | ✔ :selected: | ✔ :selected: |  | ✔ :selected: |
| Layout | ✔ :selected: | :selected: ✔ |  | ✔ :selected: |
| General Document | :selected: ✔ | :selected: ✔ |  |  |
| Prebuilt | ✔ :selected: | :selected: ✔ |  |  |
| Custom extraction | ✔ :selected: | :selected: ✔ |  |  |
| Custom classification | :selected: ✔ | :selected: ✔ |  | :selected: ✔ |


<!---- Page 216 ---------------------------------------------------------------------------------------------------------------------------------->
(DPI).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Get started with Read model (v4)
Try extracting text from forms and documents using the Document Intelligence Studio.
You need the following assets:
. An Azure subscription-you can create one for free [7.
. A Document Intelligence instance @ in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
Search
«
Regenerate Key1
Regenerate Key2
Overview
Activity log
Access control (IAM)
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
P
Encryption
KEY 2
Pricing tier
<- > Networking
Location/Region
Identity
westus2
₪
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
Automation
Lo
> Help
!
Note


<!---- Page 217 ---------------------------------------------------------------------------------------------------------------------------------->
Currently, Document Intelligence Studio doesn't support Microsoft Word, Excel,
PowerPoint, and HTML file formats.
Sample document processed with Document Intelligence Studio
Read
Service resource:
+ Add
8
in[ Analyze
API version: 2022-01-30-preview V
8V
Sample
Introduction
Visual Studio 2019 provides a rich, integrated development environment for creating stunning applications for Windows,
Android, and iOS, as well as modem web applications and cloud services. Visual Studio 2019 also provides a
comprehensive, highly flexible set of application lifecycle management (ALM) tools. Visual Studio subscriptions ofer
customers high-value subscriber benefits such as 31 development/test use rights for Microsoft platform software like SQL
Server/Windows/Windows Server, monthly Microsoft Azure credits, a developer account for publishing apps to the
Windows Store and an Office 365 Developer subscription.
read-vs ... aper.png
The remainder of this paper provides an overview of the Visual Studio product line and the licensing requirements for
those products in common deployment scenarios. If you're a volume licensing customer, the definitive guide to licensing
terms and conditions is the Microsoft Licensing Product Terms and your licensing program agreement. For retail
customers the license terms are specified in the Retail Software License Terms included with your product.
Sample
Visual Studio 2019 Licensing Overview
read-german.png
With the primary Visual Studio 2019 offerings there are essentially two things for which you purchase licenses:
1. Users
2. The Visual Studio Azure DevOps Server environment
Additionaly, you can purchase Azure DevOps for your team, which is billed along with other Microsoft Azure services.
The primary way to license users is by purchasing the appropriate level Visual Studio subscription for each user who will be
participating in software development projects. The software, services, and support induded with Visual Studio
subscriptions varies by level, so you shouldi consult the Viquei Sausto subscription comparison to determine the right level
for the needs of each team member. The Visual Studio software and other Microsoft software that the individual
subscriber can install and run is defined by what is available for that Visual Studio subscription level in Subscriber
Downloads while the user's subscription is active.
Users
Visual Studio subscription options:
A. Standard subscriptions (sold via the Microsoft Store and Volume Licensing resellers):
. Visual Studio Enterprise Subscription (formerly MSDN)
. Visual Studio Test Professional Subscription (formerly MSDN0
. Visual Studio Professional Subscription (formerly MSDN)
. MSDN Mettons
2019 Mierssett Corporation All right Printynd
Microsoft
1. On the Document Intelligence Studio home page z, select Read.
2. You can analyze the sample document or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options:
Run analysis
Analyze options
Try Document Intelligence Studio
.
Supported languages and locales (v4)
See our Language Support-document analysis models page for a complete list of
supported languages.
Data extraction (v4)
1 Note


<!---- Page 218 ---------------------------------------------------------------------------------------------------------------------------------->
Microsoft Word and HTML file are supported in v4.0. The following capabilities are
currently not supported:
· No angle, width/height, and unit returned with each page object.
· No bounding polygon or bounding region for each object detected.
· No page range ( pages ) as a parameter returned.
· No lines object.
Searchable PDFs
The searchable PDF capability enables you to convert an analog PDF, such as scanned-
image PDF files, to a PDF with embedded text. The embedded text enables deep text
search within the PDF's extracted content by overlaying the detected text entities on top
of the image files.
1 Important
· Currently, only the Read OCR model prebuilt-read supports the searchable
PDF capability. When using this feature, specify the modelId as prebuilt-read.
Other model types return an error for this preview version.
· Searchable PDF is included with the 2024-11-30 GA prebuilt-read model with
no added cost for generating a searchable PDF output.
Use searchable PDFs
To use searchable PDF, make a POST request using the Analyze operation and specify
the output format as pdf :
Bash
POST {endpoint}/documentintelligence/documentModels/prebuilt-
read: analyze ?_ overload=analyzeDocument&api-version=2024-11-30&output=pdf
{ ... }
202
Poll for completion of the Analyze operation. Once the operation is complete, issue a
GET request to retrieve the PDF format of the Analyze operation results.


<!---- Page 219 ---------------------------------------------------------------------------------------------------------------------------------->
Upon successful completion, the PDF can be retrieved and downloaded as
application/pdf. This operation allows direct downloading of the embedded text form
of PDF instead of Base64-encoded JSON.
Bash
// Monitor the operation until completion.
GET /documentModels/prebuilt-read/analyzeResults/{resultId}
200
{ ... }
// Upon successful completion, retrieve the PDF as application/pdf.
GET {endpoint}/documentintelligence/documentModels/prebuilt-
read/analyzeResults/{resultId}/pdf?api-version=2024-11-30
URI Parameters
Name
In
Required
Type
Description
endpoint
string
path
True
uri
The Document Intelligence service endpoint.
modelId
path
True
string
Unique document model name.
Regex pattern: ^[a-zA-Z0-9][a-zA-Z0-9 ._~- ]{1,63}$
resultId
path
True
string
uuid
Analyze operation result ID.
api-version
query
True
string
The API version to use for this operation.
Responses
Name
Type
Description
200 OK
file
The request has succeeded.
Media Types: "application/pdf", "application/json"
Other Status Codes
DocumentIntelligenceErrorResponse

| Name In | Required |  | Type | Description |
| --- | --- | --- | --- | --- |
| endpoint string | path | True |  |  |
|  |  |  |  |  |

| Name | Type | Description |
| --- | --- | --- |
| 200 OK |  |  |
| file |  |  |


<!---- Page 220 ---------------------------------------------------------------------------------------------------------------------------------->
An unexpected error response.
Media Types: "application/pdf", "application/json"
Security
Ocp-Apim-Subscription-Key
Type: apiKey
In: header
OAuth2Auth
Type: oauth2
Flow: accessCode
Authorization URL: https://login.microsoftonline.com/common/oauth2/authorize
Token URL: https://login.microsoftonline.com/common/oauth2/token
Scopes
Name
Description
https://cognitiveservices.azure.com/.default
Examples
Get Analyze Document Result PDF
Sample request
HTTP
HTTP
Copy
GET
https://myendpoint.cognitiveservices.azure.com/documentintelligence/document
Models/prebuilt-invoice/analyzeResults/3b31320d-8bab-4f88-b19c-
2322a7f11034/pdf?api-version=2024-11-30
Sample response
Status code:
200
JSON
Copy
"{pdfBinary}"
Definitions
Name
Description
DocumentIntelligenceError
The error object.
DocumentIntelligenceErrorResponse
Error response object.
DocumentIntelligenceInnerError
An object containing more specific information about the error.
DocumentIntelligenceError
The error object.
Name
Type
Description
code
string
One of a server-defined set of error codes.


<!---- Page 221 ---------------------------------------------------------------------------------------------------------------------------------->
details
DocumentIntelligenceError []
An array of details about specific errors that led to this reported error.
innererror
DocumentIntelligenceInnerError
An object containing more specific information than the current object about
the error.
message
string
A human-readable representation of the error.
target
string
The target of the error.
DocumentIntelligenceErrorResponse
Error response object.
Name
Type
Description
error
DocumentIntelligenceError
Error info.
DocumentIntelligenceInnerError
An object containing more specific information about the error.
Name
Type
Description
code
string
One of a server-defined set of error codes.
innererror
DocumentIntelligenceInnerError
Inner error.
message
string
A human-readable representation of the error.
In this article
URI Parameters
Responses
Security
Examples


<!---- Page 222 ---------------------------------------------------------------------------------------------------------------------------------->
200 OK
Content-Type: application/pdf
Pages parameter
The pages collection is a list of pages within the document. Each page is represented
sequentially within the document and includes the orientation angle indicating if the
page is rotated and the width and height (dimensions in pixels). The page units in the
model output are computed as shown:
[] Expand table
File format
Computed page unit
Total pages
Images (JPEG/JPG,
PNG, BMP, HEIF)
Each image = 1 page unit
Total images
PDF
Each page in the PDF = 1 page unit
Total pages in the PDF
TIFF
Each image in the TIFF = 1 page unit
Total images in the TIFF
Word (DOCX)
Up to 3,000 characters = 1 page unit,
embedded or linked images not supported
Total pages of up to 3,000
characters each
Excel (XLSX)
Each worksheet = 1 page unit, embedded
or linked images not supported
Total worksheets
PowerPoint (PPTX)
Each slide = 1 page unit, embedded or
linked images not supported
Total slides
HTML
Up to 3,000 characters = 1 page unit,
embedded or linked images not supported
Total pages of up to 3,000
characters each
Sample code
Python
# Analyze pages.
for page in result.pages:
print(f" ---- Analyzing document from page #{page. page_number} ----
")
print(f"Page has width: {page.width} and height: {page. height},
measured with unit: {page. unit}")

| File format | Computed page unit | Total pages |
| --- | --- | --- |
| Images (JPEG/JPG, PNG, BMP, HEIF) | Each image = 1 page unit | Total images |
| PDF | Each page in the PDF = 1 page unit | Total pages in the PDF |
| TIFF | Each image in the TIFF = 1 page unit | Total images in the TIFF |
| Word (DOCX) | Up to 3,000 characters = 1 page unit, embedded or linked images not supported | Total pages of up to 3,000 characters each |
| Excel (XLSX) | Each worksheet = 1 page unit, embedded or linked images not supported | Total worksheets |
| PowerPoint (PPTX) | Each slide = 1 page unit, embedded or linked images not supported | Total slides |
| HTML | Up to 3,000 characters = 1 page unit, embedded or linked images not supported | Total pages of up to 3,000 characters each |


<!---- Page 223 ---------------------------------------------------------------------------------------------------------------------------------->
View samples on GitHub.
Use pages for text extraction
For large multi-page PDF documents, use the pages query parameter to indicate specific
page numbers or page ranges for text extraction.
Paragraph extraction
The Read OCR model in Document Intelligence extracts all identified blocks of text in
the paragraphs collection as a top level object under analyzeResults . Each entry in this
collection represents a text block and includes the extracted text as content and the
bounding polygon coordinates. The span information points to the text fragment within
the top-level content property that contains the full text from the document.
JSON
"paragraphs": [
{
"spans": [],
"boundingRegions": [],
"content": "While healthcare is still in the early stages of its
Al journey, we are seeing pharmaceutical and other life sciences
organizations making major investments in Al and related technologies. \" TOM
LAWRY | National Director for Al, Health and Life Sciences | Microsoft"
}
]
Text, lines, and words extraction
The Read OCR model extracts print and handwritten style text as lines and words . The
model outputs bounding polygon coordinates and confidence for the extracted words.
The styles collection includes any handwritten style for lines if detected along with the
spans pointing to the associated text. This feature applies to supported handwritten
languages.
For Microsoft Word, Excel, PowerPoint, and HTML, Document Intelligence Read model
v3.1 and later versions extracts all embedded text as is. Texts are extrated as words and
paragraphs. Embedded images aren't supported.
Sample code


<!---- Page 224 ---------------------------------------------------------------------------------------------------------------------------------->
Python
# Analyze lines.
if page. lines:
for line_idx, line in enumerate(page.lines):
words = get_words (page, line)
print(
f" ... Line # {line_idx} has {len(words)} words and text
'{line. content} ' within bounding polygon ' {line. polygon} '"
)
# Analyze words.
for word in words:
print(f" ...... Word ' {word. content}' has a confidence of
{word. confidence}")
View samples on GitHub.
Handwritten style extraction
The response includes classifying whether each text line is of handwriting style or not,
along with a confidence score. For more information, see handwritten language support.
The following example shows an example JSON snippet.
JSON
"styles": [
{
"confidence": 0.95,
"spans": [
{
"offset": 509,
"length": 24
}
"isHandwritten": true
]
}
If you enabled the font/style addon capability, you also get the font/style result as part
of the styles object.
Next steps v4.0
Complete a Document Intelligence quickstart:


<!---- Page 225 ---------------------------------------------------------------------------------------------------------------------------------->
V REST API
V C# SDK
V Python SDK
V Java SDK
JavaScript
Explore our REST API:
Document Intelligence API v4.0
Find more samples on GitHub:
Read model.
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 226 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence receipt model
Article · 12/11/2024
This content applies to:
v3.0 (GA)
v4.0 (GA) | Previous versions:
v3.1 (GA)
v2.1 (GA)
::: moniker-end
The Document Intelligence receipt model combines powerful Optical Character
Recognition (OCR) capabilities with deep learning models to analyze and extract key
information from sales receipts. Receipts can be of various formats and quality including
printed and handwritten receipts. The API extracts key information such as merchant
name, merchant phone number, transaction date, tax, and transaction total and returns
structured JSON data. Receipt model v4.0 (GA) also supports other fields including
ReceiptType, TaxDetails. NetAmount, TaxDetails. Description, TaxDetails. Rate and
CountryRegion .
Supported receipt types:
· Meal
· Supplies
· Hotel
· Fuel&Energy
· Transportation
· Communication
· Subscriptions
· Entertainment
· Training
· Healthcare
Receipt data extraction
Receipt digitization encompasses the transformation of various types of receipts,
including scanned, photographed, and printed copies, into a digital format for
streamlined downstream processing. Examples include expense management, consumer
behavior analysis, tax automation, etc. Using Document Intelligence with OCR (Optical
Character Recognition) technology can extract and interpret data from these diverse
receipt formats. Document Intelligence processing simplifies the conversion process but
also significantly reduces the time and effort required, thus facilitating efficient data
management, and retrieval.


<!---- Page 227 ---------------------------------------------------------------------------------------------------------------------------------->
Sample receipt processed with Document Intelligence Studio :
Analyze
8
Values
Result
Code
*
Contos
123 Main Street
Redmond, WA 98052
987-654-3210
6/10/2019 13:59
Sales Associate: Paul
El
Surface Pro 6
$999.00
1
SurfacePer
$99.99
Sub-Total
$1098.99
Tax
$104.40
Total
$1203.39
<
1
of 1
>
Q Q @ Q
Items (2)
#1
E
1 Name
98.40%
Surface Pro 6
Quantity
96.90%
1
TotalPrice
98.50%
999
2 Name
96.90%
SurfacePen
Quantity
97.00%
1
TotalPrice
98.50%
99.99
Locale
98.70%
en-US
MerchantAddress
#1
94.50%
123 Main Street Redmond, WA 98052
MerchantName
#1
95.00%
Contoso
++
MerchantPhoneNumber
#1
+19876543210
99.00%
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
0
Expand table
Feature
Resources
Model ID
Receipt model
· Document Intelligence Studio
· REST API
· C# SDK
· Python SDK
· Java SDK
prebuilt-receipt
· JavaScript SDK
Input requirements
· Supported file formats:
17
is
Expand table

| Items (2) #1 E :selected: |  |
| --- | --- |
| 1 Name :unselected: | 98.40% |
| Surface Pro 6 |  |
| Quantity :unselected: | 96.90% |
| 1 |  |
| TotalPrice :unselected: | 98.50% |
| 999 |  |
| 2 Name :unselected: | 96.90% |
| SurfacePen :unselected: |  |
| Quantity :unselected: | 97.00% |
| 1 |  |
| TotalPrice :unselected: | 98.50% |
| 99.99 |  |
| Locale :selected: | 98.70% |
| en-US |  |
| MerchantAddress #1 :selected: | 94.50% |
| 123 Main Street Redmond, WA 98052 :unselected: |  |
| MerchantName #1 :selected: | 95.00% |
| Contoso :unselected: | ++ |
| MerchantPhoneNumber #1 +19876543210 :selected: | 99.00% |

| Feature | Resources | Model ID |
| --- | --- | --- |
| Receipt model | · Document Intelligence Studio · REST API · C# SDK · Python SDK · Java SDK | prebuilt-receipt |
|  |  |  |
|  | · JavaScript SDK |  |


<!---- Page 228 ---------------------------------------------------------------------------------------------------------------------------------->
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Receipt model data extraction

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | :selected: ✔ | :selected: ✔ | ✔ :selected: |
| General Document | ✔ :selected: | ✔ :selected: |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | ✔ :selected: | :selected: ✔ |  |
| Custom classification | ✔ :selected: | :selected: ✔ | ✔ :selected: |
|  |  |  |  |


<!---- Page 229 ---------------------------------------------------------------------------------------------------------------------------------->
See how Document Intelligence extracts data, including time and date of transactions,
merchant information, and amount totals from receipts. You need the following
resources:
. An Azure subscription-you can create one for free .
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
×
Search
«
C' Regenerate Key1 & Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
Encryption
KEY 2
Pricing tier
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
> Monitoring
> Automation
Ls
Help
1 Note
Document Intelligence Studio is available with v3.1 and v3.0 APIs and later versions.
1. On the Document Intelligence Studio home page , select Receipts.
2. You can analyze the sample receipt or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options:
Run analysis
Analyze options


<!---- Page 230 ---------------------------------------------------------------------------------------------------------------------------------->
Try Document Intelligence Studio
.
Supported languages and locales
For a complete list of supported languages, see our prebuilt models language support
page.
Field extraction
For supported document extraction fields, refer to the receipt model schemat page in
our GitHub sample repository
Next steps
· Try processing your own forms and documents with the Document Intelligence
Studio [7 .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
. Find more samples on GitHub. [
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 231 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence US tax document
models
Article · 12/11/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA) ::: moniker-end
The Document Intelligence tax model uses powerful Optical Character Recognition
(OCR) capabilities to analyze and extract key fields and line items from a select group of
tax documents. Tax documents can be of various formats like 1099, 1098, W2, 1040,
1095A, 1095C, W-4, 1099-SSA. Input format can include phone-captured images,
scanned documents, and digital PDFs. The API analyzes document text; extracts key
information and returns a structured JSON data representation. The model currently
supports certain English tax document formats.
Supported tax form types:
· Unified tax US
· W-2
· 1098
· 1098-E
· 1098-T
· 1099 and variations (added 1099-SSA)
· 1040 and variations
· 1095A, 1095C
· W-4
Automated tax document processing
Automated tax document processing is the process of extracting key fields from tax
documents. Historically, tax documents were processed manually. This model allows for
the easy automation of tax scenarios.
Unified Tax US
The Unified US Tax prebuilt model automatically detects and extracts data from W2,
1098, 1040, and 1099 tax forms in submitted documents. These documents can be
composed of many tax or non-tax-related documents. The model only processes the
forms it supports.


<!---- Page 232 ---------------------------------------------------------------------------------------------------------------------------------->
1040
· TaxPayer details
....
2023 Tax Return
· Social Security
wages: 10 USD
....
W2
Document Intelligence Al
Unified Tax prebuilt
1099-
INT
· Total tax withheld: 10
USD
....
Single file with
multiple tax
and non-tax
forms
1098
· Mortgage Insurance
Premium: 1000 USD
....
Other
· Brokerage/Mortgage, etc
· Any non-tax document
....
Development options
Document Intelligence v4.0: 2024-11-30 (GA) supports the following tools, applications,
and libraries:
0
Expand table
Feature
Resources
Model ID
US tax form models
· Document Intelligence Studio
· prebuilt-tax.us
· REST API
· C# SDK
· prebuilt-tax.us.W-2
· prebuilt-tax.us.W-4
· Python SDK
· Java SDK
· prebuilt-tax.us.1095A
· prebuilt-tax.us.1095C
· JavaScript SDK
· prebuilt-tax.us.1098
· prebuilt-tax.us.1098E
· prebuilt-tax.us.1098T
· prebuilt-tax.us.1099A
· prebuilt-tax.us.1099B
· prebuilt-tax.us.1099C
· prebuilt-tax.us.1099CAP
· prebuilt-tax.us.1099Combo
· prebuilt-tax.us.1099DIV
· prebuilt-tax.us.1099G
· prebuilt-tax.us.1099H
· prebuilt-tax.us.1099INT
· prebuilt-tax.us.1099K
· prebuilt-tax.us.1099LS
· prebuilt-tax.us.1099LTC
· prebuilt-tax.us.1099MISC
· prebuilt-tax.us.1099NEC
· prebuilt-tax.us.1099OID
· prebuilt-tax.us.1099PATR

| Feature | Resources | Model ID |
| --- | --- | --- |
| US tax form models | · Document Intelligence Studio | · prebuilt-tax.us |
|  | · REST API · C# SDK | · prebuilt-tax.us.W-2 · prebuilt-tax.us.W-4 |
|  | · Python SDK · Java SDK | · prebuilt-tax.us.1095A · prebuilt-tax.us.1095C |
|  | · JavaScript SDK | · prebuilt-tax.us.1098 · prebuilt-tax.us.1098E · prebuilt-tax.us.1098T · prebuilt-tax.us.1099A · prebuilt-tax.us.1099B · prebuilt-tax.us.1099C · prebuilt-tax.us.1099CAP · prebuilt-tax.us.1099Combo · prebuilt-tax.us.1099DIV · prebuilt-tax.us.1099G · prebuilt-tax.us.1099H · prebuilt-tax.us.1099INT · prebuilt-tax.us.1099K · prebuilt-tax.us.1099LS · prebuilt-tax.us.1099LTC · prebuilt-tax.us.1099MISC · prebuilt-tax.us.1099NEC · prebuilt-tax.us.1099OID · prebuilt-tax.us.1099PATR |


<!---- Page 233 ---------------------------------------------------------------------------------------------------------------------------------->
Feature
Resources
Model ID
· prebuilt-tax.us.1099Q
· prebuilt-tax.us.1099QA
· prebuilt-tax.us.1099R
· prebuilt-tax.us.1099S
· prebuilt-tax.us.1099SA
· prebuilt-tax.us.1099SB
· prebuilt-tax.us.1099SSA
· prebuilt-tax.us.1040
· prebuilt-tax.us.1040Schedule1
· prebuilt-tax.us.1040Schedule2
· prebuilt-tax.us.1040Schedule3
· prebuilt-tax.us.1040Schedule8812
· prebuilt-tax.us.1040ScheduleA
· prebuilt-tax.us.1040ScheduleB
· prebuilt-tax.us.1040ScheduleC
· prebuilt-tax.us.1040ScheduleD
· prebuilt-tax.us.1040ScheduleE
· prebuilt-tax.us.1040ScheduleEIC
· prebuilt-tax.us.1040ScheduleF
· prebuilt-tax.us.1040ScheduleH
· prebuilt-tax.us.1040ScheduleJ
· prebuilt-tax.us.1040ScheduleR
· prebuilt-tax.us.1040ScheduleSE
· prebuilt-tax.us.1040Senior
Input requirements
· Supported file formats:
[ Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX ),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
✔
✔
✔

| Feature | Resources | Model ID |
| --- | --- | --- |
|  |  | · prebuilt-tax.us.1099Q · prebuilt-tax.us.1099QA · prebuilt-tax.us.1099R · prebuilt-tax.us.1099S · prebuilt-tax.us.1099SA · prebuilt-tax.us.1099SB · prebuilt-tax.us.1099SSA · prebuilt-tax.us.1040 · prebuilt-tax.us.1040Schedule1 · prebuilt-tax.us.1040Schedule2 · prebuilt-tax.us.1040Schedule3 · prebuilt-tax.us.1040Schedule8812 · prebuilt-tax.us.1040ScheduleA · prebuilt-tax.us.1040ScheduleB · prebuilt-tax.us.1040ScheduleC · prebuilt-tax.us.1040ScheduleD · prebuilt-tax.us.1040ScheduleE · prebuilt-tax.us.1040ScheduleEIC · prebuilt-tax.us.1040ScheduleF · prebuilt-tax.us.1040ScheduleH · prebuilt-tax.us.1040ScheduleJ · prebuilt-tax.us.1040ScheduleR · prebuilt-tax.us.1040ScheduleSE · prebuilt-tax.us.1040Senior |

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX ), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| Layout | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | :selected: ✔ | :selected: ✔ |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |
| Custom | :selected: ✔ | :selected: ✔ | :selected: ✔ |


<!---- Page 234 ---------------------------------------------------------------------------------------------------------------------------------->
Model
PDF
Image:
Microsoft Office:
JPEG/JPG, PNG, BMP,
Word (DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
TIFF , HEIF
classification
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Try tax document data extraction
See how data, including customer information, vendor details, and line items, is
extracted from invoices. You need the following resources:
. An Azure subscription-you can create one for free .
. A Document Intelligence instance in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.

| Model | PDF | Image: |  | Microsoft Office: |
| --- | --- | --- | --- | --- |
|  |  | JPEG/JPG, | PNG, BMP, | Word (DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
|  |  | TIFF , HEIF |  |  |
| classification |  |  |  |  |


<!---- Page 235 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
P Search
«
C
Regenerate Key1
Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
X Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
P
& Encryption
KEY 2
Pricing tier
P
Networking
Location/Region
Identity
westus2
₪
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
0
Locks
Monitoring
> Automation
Ls
Help
Document Intelligence Studio
1. On the Document Intelligence Studio home page, select the supported tax
document model.
2. You can analyze a sample tax document or upload your own files.
3. Select the Run analysis button and, if necessary, configure the Analyze options :
Run analysis
Analyze options
Try Document Intelligence Studio
Supported languages and locales
See our Language Support-prebuilt models page for a complete list of supported
languages.
Field extraction


<!---- Page 236 ---------------------------------------------------------------------------------------------------------------------------------->
For supported document extraction fields, see the tax document model schema
pages in our GitHub sample repository.
The tax documents key-value pairs and line items extracted are in the documentResults
section of the JSON output.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Find more samples on GitHub.
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 237 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence custom models
Article · 12/11/2024
This content applies to:
v3.0 (GA)
v4.0 (GA) | Previous versions:
v3.1 (GA)
v2.1 (GA)
::: moniker-end
Document Intelligence uses advanced machine learning technology to identify
documents, detect and extract information from forms and documents, and return the
extracted data in a structured JSON output. With Document Intelligence, you can use
document analysis models, pre-built/pre-trained, or your trained standalone custom
models.
Custom models now include custom classification models for scenarios where you need
to identify the document type before invoking the extraction model. Classifier models
are available starting with the 2023-07-31 (GA) API. A classification model can be paired
with a custom extraction model to analyze and extract fields from forms and documents
specific to your business. Standalone custom extraction models can be combined to
create composed models.
Custom document model types
Custom document models can be one of two types, custom template or custom form
and custom neural or custom document models. The labeling and training process for
both models is identical, but the models differ as follows:
Custom extraction models
To create a custom extraction model, label a dataset of documents with the values you
want extracted and train the model on the labeled dataset. You only need five examples
of the same form or document type to get started.
Custom neural model
1 Important
Document Intelligence v4.0 2024-11-30 (GA) API supports custom neural model
overlapping fields, signature detection and table, row and cell level confidence.


<!---- Page 238 ---------------------------------------------------------------------------------------------------------------------------------->
The custom neural (custom document) model uses deep learning models and base
model trained on a large collection of documents. This model is then fine-tuned or
adapted to your data when you train the model with a labeled dataset. Custom neural
models support extracting key data fields from structured, semi-structured, and
unstructured documents. When you're choosing between the two model types, start
with a neural model to determine if it meets your functional needs. See neural models to
learn more about custom document models.
Custom template model
The custom template or custom form model relies on a consistent visual template to
extract the labeled data. Variances in the visual structure of your documents affect the
accuracy of your model. Structured forms such as questionnaires or applications are
examples of consistent visual templates.
Your training set consists of structured documents where the formatting and layout are
static and constant from one document instance to the next. Custom template models
support key-value pairs, selection marks, tables, signature fields, and regions. Template
models and can be trained on documents in any of the supported languages. For more
information, see custom template models.
If the language of your documents and extraction scenarios supports custom neural
models, we recommend that you use custom neural models over template models for
higher accuracy.
? Tip
To confirm that your training documents present a consistent visual template,
remove all the user-entered data from each form in the set. If the blank forms are
identical in appearance, they represent a consistent visual template.
For more information, see Interpret and improve accuracy and confidence for
custom models.
Input requirements
· For best results, provide one clear photo or high-quality scan per document.
· Supported file formats:
[ Expand table


<!---- Page 239 ---------------------------------------------------------------------------------------------------------------------------------->
Model
PDF
Image:
jpeg/jpg, png, bmp,
tiff, heif
Microsoft Office:
Word (docx), Excel (xlsx),
PowerPoint (pptx)
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
* Microsoft Office files are currently not supported for other models or versions.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 x 50 pixels and 10,000 px x 10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 -point text at 150 dots per inch.
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
. For custom extraction model training, the total size of training data is 50 MB for
template model and 1G-MB for the neural model.
. For custom classification model training, the total size of training data is 1GB with a
maximum of 10,000 pages.
Optimal training data
Training input data is the foundation of any machine learning model. It determines the
quality, accuracy, and performance of the model. Therefore, it's crucial to create the best
training input data possible for your Document Intelligence project. When you use the

| Model | PDF | Image: jpeg/jpg, png, bmp, tiff, heif | Microsoft Office: Word (docx), Excel (xlsx), PowerPoint (pptx) |
| --- | --- | --- | --- |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | ✔ :selected: | ✔ :selected: | :selected: ✔ |
| General Document | ✔ :selected: | ✔ :selected: |  |
| Prebuilt | ✔ :selected: | ✔ :selected: |  |
| Custom extraction | ✔ :selected: | ✔ :selected: |  |
| Custom classification | ✔ :selected: | :selected: ✔ | ✔ :selected: |


<!---- Page 240 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence custom model, you provide your own training data. Here are a
few tips to help train your models effectively:
. Use text-based instead of image-based PDFs when possible. One way to identify
an image*based PDF is to try selecting specific text in the document. If you can
select only the entire image of the text, the document is image based, not text
based.
· Organize your training documents by using a subfolder for each format (JPEG/JPG,
PNG, BMP, PDF, or TIFF).
. Use forms that have all of the available fields completed.
· Use forms with differing values in each field.
· Use a larger dataset (more than five training documents) if your images are low
quality.
· Determine if you need to use a single model or multiple models composed into a
single model.
· Consider segmenting your dataset into folders, where each folder is a unique
template. Train one model per folder, and compose the resulting models into a
single endpoint. Model accuracy can decrease when you have different formats
analyzed with a single model.
· Consider segmenting your dataset to train multiple models if your form has
variations with formats and page breaks. Custom forms rely on a consistent visual
template.
. Ensure that you have a balanced dataset by accounting for formats, document
types, and structure.
Build mode
The build custom model operation adds support for the template and neural custom
models. Previous versions of the REST API and client libraries only supported a single
build mode that is now known as the template mode.
. Template models only accept documents that have the same basic page structure
-a uniform visual appearance-or the same relative positioning of elements
within the document.


<!---- Page 241 ---------------------------------------------------------------------------------------------------------------------------------->
. Neural models support documents that have the same information, but different
page structures. Examples of these documents include United States W2 forms,
which share the same information, but vary in appearance across companies.
This table provides links to the build mode programming language SDK references and
code samples on GitHub:
[] Expand table
Programming language
SDK reference
Code sample
C#/.NET
DocumentBuildMode Struct
Sample_BuildCustomModelAsync.cs [
Java
DocumentBuildMode Class
BuildModel.java z
JavaScript
DocumentBuildMode type
buildModel.js
Python
DocumentBuildMode Enum
sample_build_model.py[
Compare model features
The following table compares custom template and custom neural features:
[ Expand table
Feature
Custom template (form)
Custom neural (document)
Document
structure
Template, form, and structured
Structured, semi-structured,
and unstructured
Training time
1 to 5 minutes
20 minutes to 1 hour
Data extraction
Key-value pairs, tables, selection marks,
coordinates, and signatures
Key-value pairs, selection
marks, and tables
Overlapping
fields
Not supported
Supported
Document
variations
Requires a model per each variation
Uses a single model for all
variations
Language
support
Language support custom template
Language support custom
neural
Custom classification model

| Programming language | SDK reference | Code sample |
| --- | --- | --- |
| C#/.NET | DocumentBuildMode Struct | Sample_BuildCustomModelAsync.cs [ |
| Java | DocumentBuildMode Class | BuildModel.java z |
| JavaScript | DocumentBuildMode type | buildModel.js |
| Python | DocumentBuildMode Enum | sample_build_model.py[ |

| Feature | Custom template (form) | Custom neural (document) |
| --- | --- | --- |
| Document structure | Template, form, and structured | Structured, semi-structured, and unstructured |
| Training time | 1 to 5 minutes | 20 minutes to 1 hour |
| Data extraction | Key-value pairs, tables, selection marks, coordinates, and signatures | Key-value pairs, selection marks, and tables |
| Overlapping fields | Not supported | Supported |
| Document variations | Requires a model per each variation | Uses a single model for all variations |
| Language support | Language support custom template | Language support custom neural |


<!---- Page 242 ---------------------------------------------------------------------------------------------------------------------------------->
Document classification is a new scenario supported by Document Intelligence with the
2023-07-31 (v3.1 GA) API. The document classifier API supports classification and
splitting scenarios. Train a classification model to identify the different types of
documents your application supports. The input file for the classification model can
contain multiple documents and classifies each document within an associated page
range. To learn more, see custom classification models.
1 Note
The v4.0 2024-11-30 (GA) document classification model supports Office
document types for classification. This API version also introduces incremental
training for the classification model.
Custom model tools
Document Intelligence v3.1 and later models support the following tools, applications,
and libraries, programs, and libraries:
Expand table
Feature
Resources
Model ID
Custom model
· Document Intelligence Studio z
· REST API
· C# SDK
· Python SDK
custom-model-id
Custom model life cycle
The life cycle of a custom model depends on the API version that is used to train it. If
the API version is a general availability (GA) version, the custom model has the same life
cycle as that version. The custom model isn't available for inference when the API
version is deprecated. If the API version is a preview version, the custom model has the
same life cycle as the preview version of the API.
Build a custom model
Extract data from your specific or unique documents using custom models. You need
the following resources:

| Feature | Resources | Model ID |
| --- | --- | --- |
| Custom model | · Document Intelligence Studio z · REST API · C# SDK · Python SDK | custom-model-id |


<!---- Page 243 ---------------------------------------------------------------------------------------------------------------------------------->
. An Azure subscription. You can create one for free .
. A Document Intelligence instance [ in the Azure portal. You can use the free
pricing tier (F0) to try the service. After your resource deploys, select Go to
resource to get your key and endpoint.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
Search
«
Regenerate Key1 ' Regenerate Key2
Overview
Activity log
i
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
r
& Encryption
KEY 2
Pricing tier
I
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
Automation
Los
Help
Document Intelligence Studio
1 Note
Document Intelligence Studio is available with v3.1 and v3.0 APIs.
1. On the Document Intelligence Studio home page, select Custom extraction
models.
2. Under My Projects, select Create a project.
3. Complete the project details fields.
4. Configure the service resource by adding your Storage account and Blob
container to Connect your training data source.
5. Review and create your project.
6. Add your sample documents to label, build, and test your custom model.


<!---- Page 244 ---------------------------------------------------------------------------------------------------------------------------------->
Try Document Intelligence Studio
For a detailed walkthrough to create your first custom extraction model, see How to
create a custom extraction model.
Custom model extraction summary
This table compares the supported data extraction areas:
[ Expand table
Model
Form
fields
Selection
marks
Structured
fields (Tables)
Signature
Region
labeling
Overlapping
fields
Custom
template
✔
✔
✔
✔
✔
n/a
Custom
neural
✔
✔
✔
✔
*
✔
Table symbols:
V-Supported
** n/a-Currently unavailable;
*- Behaves differently depending upon model. With template models, synthetic data is
generated at training time. With neural models, exiting text recognized in the region is
selected.
? Tip
When choosing between the two model types, start with a custom neural model if
it meets your functional needs. See custom neural to learn more about custom
neural models.
Custom model development options
The following table describes the features available with the associated tools and client
libraries. As a best practice, ensure that you use the compatible tools listed here.
" Expand table

| Model | Form fields | Selection marks | Structured fields (Tables) | Signature | Region labeling | Overlapping fields |
| --- | --- | --- | --- | --- | --- | --- |
| Custom template | ✔ :selected: | ✔ :selected: | ✔ :selected: | ✔ :selected: | ✔ :selected: | n/a |
| Custom neural | ✔ :selected: | :selected: ✔ | :selected: ✔ | ✔ :selected: | * | ✔ :selected: |


<!---- Page 245 ---------------------------------------------------------------------------------------------------------------------------------->
Document type
REST API
SDK
Label and Test
Models
Custom template v 4.0
v3.1 v3.0
Document Intelligence
Document
Document Intelligence
3.1
Intelligence SDK
Studio z
Custom neural v4.0
Document Intelligence
Document
Document Intelligence
v3.1 v3.0
3.1
Intelligence SDK
Studio 2
Custom form v2.1
Document Intelligence
2.1 GA API
Document
Intelligence SDK
Sample labeling tool Z
1 Note
Custom template models trained with the 3.0 API will have a few improvements
over the 2.1 API stemming from improvements to the OCR engine. Datasets used
to train a custom template model using the 2.1 API can still be used to train a new
model using the 3.0 API.
· For best results, provide one clear photo or high-quality scan per document.
· Supported file formats are JPEG/JPG, PNG, BMP, TIFF, and PDF (text-embedded or
scanned). Text-embedded PDFs are best to eliminate the possibility of error in
character extraction and location.
· For PDF and TIFF files, up to 2,000 pages can be processed. With a free tier
subscription, only the first two pages are processed.
· The file size must be less than 500 MB for paid (SO) tier and 4 MB for free (FO) tier.
· Image dimensions must be between 50 x 50 pixels and 10,000 x 10,000 pixels.
· PDF dimensions are up to 17 x 17 inches, corresponding to Legal or A3 paper size,
or smaller.
· The total size of the training data is 500 pages or less.
· If your PDFs are password-locked, you must remove the lock before submission.
? Tip
Training data:
o If possible, use text-based PDF documents instead of image-based
documents. Scanned PDFs are handled as images.

| Document type | REST API | SDK | Label and Test Models |
| --- | --- | --- | --- |
| Custom template v 4.0 v3.1 v3.0 | Document Intelligence | Document | Document Intelligence |
|  | 3.1 | Intelligence SDK | Studio z |
| Custom neural v4.0 | Document Intelligence | Document | Document Intelligence |
| v3.1 v3.0 | 3.1 | Intelligence SDK | Studio 2 |
| Custom form v2.1 | Document Intelligence 2.1 GA API | Document Intelligence SDK | Sample labeling tool Z |


<!---- Page 246 ---------------------------------------------------------------------------------------------------------------------------------->
o Please supply only a single instance of the form per document.
o For filled-in forms, use examples that have all their fields filled in.
o Use forms with different values in each field.
o If your form images are of lower quality, use a larger dataset. For example,
use 10 to 15 images.
Supported languages and locales
See our Language Support-custom models page for a complete list of supported
languages.
Next steps
. Try processing your own forms and documents with the Document Intelligence
Studio 2 .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 247 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence composed
custom models
Article · 02/27/2025
v2.1 (GA)
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
::: moniker-end
1 Important
The v4.0 2024-11-30 (GA) model compose operation adds an explicitly trained
classifier instead of an implicit classifier for analysis. For the previous composed
model version, see Composed custom models v3.1. If you're currently using
composed models, consider upgrading to the latest implementation.
What is a composed model?
With composed models, you can group multiple custom models into a composed
model called with a single model ID. For example, your composed model might include
custom models trained to analyze your supply, equipment, and furniture purchase
orders. Instead of manually trying to select the appropriate model, you can use a
composed model to determine the appropriate custom model for each analysis and
extraction.
Some scenarios require classifying the document first and then analyzing the document
with the model best suited to extract the fields from the model. Such scenarios can
include ones where a user uploads a document but the document type isn't explicitly
known. Another scenario can be when multiple documents are scanned together into a
single file and the file is submitted for processing. Your application then needs to
identify the component documents and select the best model for each document.
In previous versions, the model compose operation performed an implicit classification to
decide which custom model best represents the submitted document. The 2024-11-30
(GA) implementation of the model compose operation replaces the implicit classification
from the earlier versions with an explicit classification step and adds conditional routing.
Benefits of the new model compose operation


<!---- Page 248 ---------------------------------------------------------------------------------------------------------------------------------->
The new model compose operation requires you to train an explicit classifier and provides
several benefits.
· Continual incremental improvement. You can consistently improve the quality of
the classifier by adding more samples and incrementally improving classification.
This fine tuning ensures your documents are always routed to the right model for
extraction.
· Complete control over routing. By adding confidence-based routing, you provide
a confidence threshold for the document type and the classification response.
· Ignore document specific document types during the operation. Earlier
implementations of the model compose operation selected the best analysis model
for extraction based on the confidence score even if the highest confidence scores
were relatively low. By providing a confidence threshold or explicitly not mapping a
known document type from classification to an extraction model, you can ignore
specific document types.
· Analyze multiple instances of the same document type. When paired with the
splitMode option of the classifier, the model compose operation can detect multiple
instances of the same document in a file and split the file to process each
document independently. Using splitMode enables the processing of multiple
instances of a document in a single request.
. Support for add on features. Add on features like query fields or barcodes can
also be specified as a part of the analysis model parameters.
· Assigned custom model maximum expanded to 500. The new implementation of
the model compose operation allows you to assign up to 500 trained custom
models to a single composed model.
How to use model compose
· Start by collecting samples of all your needed documents including samples with
information that should be extracted or ignored.
. Train a classifier by organizing the documents in folders where the folder names
are the document type you intend to use in your composed model definition.
. Finally, train an extraction model for each of the document types you intend to
use.


<!---- Page 249 ---------------------------------------------------------------------------------------------------------------------------------->
. Once your classification and extraction models are trained, use the Document
Intelligence Studio, client libraries, or the REST API to compose the classification
and extraction models into a composed model.
Use the splitMode parameter to control the file splitting behavior:
. None. The entire file is treated as a single document.
· perPage. Each page in the file is treated as a separate document.
. auto. The file is automatically split into documents.
Billing and pricing
Composed models are billed the same as individual custom models. The pricing is based
on the number of pages analyzed by the downstream analysis model. Billing is based on
the extraction price for the pages routed to an extraction model. With the addition of
the explicit classification charges are incurred for the classification of all pages in the
input file. For more information, see the Document Intelligence pricing page .
Development options
Document Intelligence v4.0:2024-11-30 (GA) supports the following tools, applications,
and libraries:
" Expand table
Feature
Resources
Custom model
· Document Intelligence Studio
· REST API
· C# SDK
· Java SDK
· JavaScript SDK
· Python SDK
Composed model
· Document Intelligence Studio
· REST API
· C# SDK
· Java SDK
· JavaScript SDK
· Python SDK
Next steps

| Feature | Resources |
| --- | --- |
| Custom model | · Document Intelligence Studio |
|  | · REST API |
|  | · C# SDK |
|  | · Java SDK |
|  | · JavaScript SDK |
|  | · Python SDK |
| Composed model | · Document Intelligence Studio |
|  | · REST API |
|  | · C# SDK |
|  | · Java SDK |
|  | · JavaScript SDK |
|  | · Python SDK |


<!---- Page 250 ---------------------------------------------------------------------------------------------------------------------------------->
Learn to create and compose custom models:
Build a custom model
Compose custom models
Feedback
Was this page helpful?
& Yes
No
Provide product feedback 7 | Get help at Microsoft Q&A


<!---- Page 251 ---------------------------------------------------------------------------------------------------------------------------------->
Use Document Intelligence incremental
classifiers
Article · 02/27/2025
This content applies to:
v4.0 (GA)
Azure AI Document Intelligence is a cloud-based Azure AI service that enables you to
build intelligent document processing solutions. Document Intelligence APIs analyze
images, PDFs, and other document files to extract and detect various content, layout,
style, and semantic elements.
Document Intelligence custom classification models are deep-learning-model types that
combine layout and language features to accurately detect and identify documents you
process within your applications. Custom classification models perform classification of
input files one page at a time to identify the documents within and can also identify
multiple documents or multiple instances of a single document within an input file.
Document Intelligence document classifiers identify known document types in files.
When processing an input file with multiple document types or when you don't know
the document type, use a classifier to identify the document. Classifiers should be
periodically updated whenever the following changes occur:
· You add new templates for an existing class.
· You add new document types for recognition.
· Classifier confidence is low.
In some scenarios, you can no longer have the original set of documents used to train
the classifier. With incremental training, you can update the classifier with just the new
labeled samples.
1 Note
Incremental training only applies to document classifier models and not custom
models.
Incremental training is useful when you want to improve the quality of a custom
classifier. Adding new training samples for existing classes improves the confidence of
the model for existing document types. For instance, if a new version of an existing form
is added or there's a new document type. An example can be when your application
starts supporting a new document type as a valid input.


<!---- Page 252 ---------------------------------------------------------------------------------------------------------------------------------->
Getting started with incremental training
· Incremental training doesn't introduce any new API endpoints.
. The documentClassifiers: build request payload is modified to support
incremental training.
. Incremental training results in a new classifier model being created with the
existing classifier left untouched.
. The new classifier has all the document samples and types of the old classifier
along with the newly provided samples. You need to ensure your application is
updates to work with the newly trained classifier.
1 Note
Copy operation for classifiers is currently unavailable.
Create an incremental classifier build request
The incremental classifier build request is similar to the classify document build request
but includes the new baseClassifierId property. The baseClassifierId is set to the
existing classifier that you want to extend. You also need to provide the docTypes for the
different document types in the sample set. By providing a docType that exists in the
baseClassifier, the samples provided in the request are added to the samples provided
when the base classifier was trained. New docType values added in the incremental
training are only added to the new classifier. The process to specify the samples remains
unchanged. For more information, see training a classifier model.
Sample POST request
Sample POST request to build an incremental document classifier
POST {your-endpoint}/documentintelligence/documentClassifiers:build?api-
version=2024-11-30
JSON
{
"classifierId": "myAdaptedClassifier",
"description": "Classifier description",
"baseClassifierId": "myOriginalClassifier",


<!---- Page 253 ---------------------------------------------------------------------------------------------------------------------------------->
"docTypes": {
"formA": {
"azureBlobSource": {
"containerUrl":
"https: //myStorageAccount. blob. core.windows. net/myContainer?mySasToken",
"prefix": "formADocs/"
}
},
"formB": {
"azureBlobFileListSource": {
"containerUrl":
"https://myStorageAccount. blob. core. windows . net/myContainer?mySasToken",
"fileList": "formB.jsonl"
}
}
}
}
POST response
All Document Intelligence APIs are asynchronous, polling the returned operation
location provides a status on the build operation. Classifiers are fast to train and your
classifier can be ready to use in a minute or two.
Upon successful completion:
. The successful POST method returns a 202 OK response code indicating that the
service created the request.
. The translated documents are located in your target container.
· The POST request also returns response headers including Operation-Location. The
value of this header contains a resultId that can be queried to get the status of
the asynchronous operation and retrieve the results using a GET request with your
same resource subscription key.
Sample GET request
Sample GET request to retrieve the result of an incremental document classifier
GET {your-
endpoint}/documentintelligence/documentClassifiers/{classifierId}/analyzeResults/{re
sultId}?api-version=2024-11-30
JSON
{


<!---- Page 254 ---------------------------------------------------------------------------------------------------------------------------------->
"classifierId": "myAdaptedClassifier",
"description": "Classifier description",
"createdDateTime": "2022-07-30T00:00:00Z",
"expirationDateTime": "2023-01-01T00:00:00Z",
"apiVersion": "2024-11-30",
"baseClassifierId": "myOriginalClassifier",
"docTypes": {
"formA": {
"azureBlobSource": {
"containerUrl":
"https://myStorageAccount . blob. core. windows. net/myContainer",
"prefix": "formADocs/"
}
},
"formB": {
"azureBlobFileListSource": {
"containerUrl":
"https: //myStorageAccount . blob. core. windows. net/myContainer",
"fileList": "formB. jsonl"
}
}
}
}
GET response
The GET response from an incrementally trained classifier differs from the standard
classifier GET response. The incrementally trained classifier doesn't return all the
document types supported. It returns the document types added or updated in the
incremental training step and the extended base classifier. To get a complete list of
document types, the base classifier must be listed. Deleting a base classifier doesn't
impact the use of an incrementally trained classifier.
Limits
. Incremental training only works when the base classifier and the incrementally
trained classifier are both trained on the same API version. As a result, the
incrementally trained classifier has the same model lifecycle as the base classifier.
. Training dataset size limits for the incremental classifier are the same as for other
classifier model. See service limits for a complete list of applicable limits.
Next steps


<!---- Page 255 ---------------------------------------------------------------------------------------------------------------------------------->
. Learn more about document classification
Feedback
Was this page helpful?
Yes
P
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 256 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence custom template
model
Article · 12/11/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Custom template (formerly custom form) is an easy-to-train document model that
accurately extracts labeled key-value pairs, selection marks, tables, regions, and
signatures from documents. Template models use layout cues to extract values from
documents and are suitable to extract fields from highly structured documents with
defined visual templates.
Custom template models share the same labeling format and strategy as custom neural
models, with support for more field types and languages.
Model capabilities
Custom template models support key-value pairs, selection marks, tables, signature
fields, and selected regions.
0
Expand table
Form
fields
Selection
marks
Tabular fields
(Tables)
Signature
Selected
regions
Overlapping
fields
Supported
Supported
Supported
Supported
Supported
Not supported
Tabular fields
With the release of API versions v3.0 and later, custom template models add support for
cross page tabular fields (tables):
. To label a table that spans multiple pages, label each row of the table across the
different pages in a single table.
. As a best practice, ensure that your dataset contains a few samples of the expected
variations. For example, include samples where the entire table is on a single page
and where tables span two or more pages if you expect to see those variations in
documents.

| Form fields | Selection marks | Tabular fields (Tables) | Signature | Selected regions | Overlapping fields |
| --- | --- | --- | --- | --- | --- |
| Supported | Supported | Supported | Supported | Supported | Not supported |


<!---- Page 257 ---------------------------------------------------------------------------------------------------------------------------------->
Tabular fields are also useful when extracting repeating information within a document
that isn't recognized as a table. For example, a repeating section of work experiences in
a resume can be labeled and extracted as a tabular field.
Dealing with variations
Template models rely on a defined visual template, changes to the template results in
lower accuracy. In those instances, split your training dataset to include at least five
samples of each template and train a model for each of the variations. You can then
compose the models into a single endpoint. For subtle variations, like digital PDF
documents and images, it's best to include at least five examples of each type in the
same training dataset.
Input requirements
· For best results, provide one clear photo or high-quality scan per document.
· Supported file formats:
@ Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX), Excel (XLSX),
PowerPoint (PPTX), and HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom
✔
✔
* Microsoft Office files are currently not supported for other models or versions.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 x 50 pixels and 10,000 px x 10,000 pixels.

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX), Excel (XLSX), PowerPoint (PPTX), and HTML |
| --- | --- | --- | --- |
| Read | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| Layout | ✔ :selected: | :selected: ✔ | ✔ :selected: |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | :selected: ✔ | :selected: ✔ |  |
| Custom | :selected: ✔ | :selected: ✔ |  |


<!---- Page 258 ---------------------------------------------------------------------------------------------------------------------------------->
. If your PDFs are password-locked, you must remove the lock before submission.
. The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 -point text at 150 dots per inch
(DPI ).
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
. For custom extraction model training, the total size of training data is 50 MB for
template model and 1G-MB for the neural model.
. For custom classification model training, the total size of training data is 1GB with a
maximum of 10,000 pages.
Training a model
Custom template models are generally available starting with v2.0 API and later versions.
If you're starting with a new project or have an existing labeled dataset, use the v3.1 or
v3.0 API with Document Intelligence Studio to train a custom template model.
[] Expand table
Model
REST API
SDK
Label and Test Models
Custom template
v3.1 API
Document Intelligence SDK
Document Intelligence Studio [
With the v3.0 and later APIs, the build operation to train model supports a new
buildMode property, to train a custom template model, set the buildMode to template.
REST
https://{endpoint}/documentintelligence/documentModels:build?api-
version=2024-11-30
{
"modelId": "string",
"description": "string",
"buildMode": "template",
"azureBlobSource":
{
"containerUrl": "string",
"prefix": "string"
}
}

| Model | REST API | SDK | Label and Test Models |
| --- | --- | --- | --- |
| Custom template | v3.1 API | Document Intelligence SDK | Document Intelligence Studio [ |


<!---- Page 259 ---------------------------------------------------------------------------------------------------------------------------------->
Supported languages and locales
See our Language Support-custom models page for a complete list of supported
languages.
Next steps
Learn to create and compose custom models:
Build a custom model
Compose custom models
Feedback
Was this page helpful?
Yes
P No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 260 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence custom neural
model
Article · 01/15/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
Custom neural document models or neural models are a deep learned model type that
combines layout and language features to accurately extract labeled fields from
documents. The base custom neural model is trained on various document types that
makes it suitable to be trained for extracting fields from structured and semi-structured
documents. Custom neural models are available in the v3.0 and later models With V4.0,
custom neural model now supports signature detection. The table below lists common
document types for each category:
@ Expand table
Documents
Examples
Structured
surveys, questionnaires
Semi-structured
invoices, purchase orders
Custom neural models share the same labeling format and strategy as custom template
models. Currently custom neural models only support a subset of the field types
supported by custom template models.
Model capabilities
1 Important
Custom neural v4.0 2024-11-30 (GA) model supports signature detection, table cell
confidence, and overlapping fields.
Custom neural models currently support key-value pairs and selection marks and
structured fields (tables).
Expand table

| Documents | Examples |
| --- | --- |
| Structured | surveys, questionnaires |
| Semi-structured | invoices, purchase orders |


<!---- Page 261 ---------------------------------------------------------------------------------------------------------------------------------->
Form
fields
Selection
marks
Tabular
fields
Signature
Region
labeling
Overlapping
fields
Supported
Supported
Supported
Supported
Supported 1
Supported 2
1 Region labels in custom neural models use the results from the Layout API for
specified region. This feature is different from template models where, if no value is
present, text is generated at training time.
2 Overlapping fields are supported with REST API version 2024-11-30 (GA). Overlapping
fields have some limits. For more information, see overlapping fields.
Build mode
The Build operation supports template and neural custom models. Previous versions of
the REST API and client libraries only supported a single build mode that is now known
as the template mode.
Neural models support documents that have the same information, but different page
structures. Examples of these documents include United States W2 forms, which share
the same information, but can vary in appearance across companies. For more
information, see Custom model build mode.
Signature detection
Custom neural v4.0 2024-11-30 (GA) model supports signature detection. To label a
signature, use field type as Signature and draw the regions for signature. Signature field
only supports one draw region per field. To train a custom neural model with signature
detection, you need to use at least five samples with signature labeled along with
variations to get the most accurate results.
Tabular fields
Custom neural v4.0 2024-11-30 (GA) model supports tabular fields (tables) to analyze
table, row, and cell data with added confidence:
· Models trained with API version 2022-06-30-preview, or later will accept tabular
field labels.
· Documents analyzed with custom neural models using API version 2022-06-30-
preview or later will produce tabular fields aggregated across the tables.
. The results can be found in the analyzeResult object's documents array that is
returned following an analysis operation.

| Form fields | Selection marks | Tabular fields | Signature | Region labeling | Overlapping fields |
| --- | --- | --- | --- | --- | --- |
| Supported | Supported | Supported | Supported | Supported 1 | Supported 2 |


<!---- Page 262 ---------------------------------------------------------------------------------------------------------------------------------->
Tabular fields support cross page tables by default:
· To label a table that spans multiple pages, label each row of the table across the
different pages in a single table.
. As a best practice, ensure that your dataset contains a few samples of the expected
variations. For example, include samples where the entire table is on a single page
and where tables span two or more pages.
Tabular fields are also useful when extracting repeating information within a document
that isn't recognized as a table. For example, a repeating section of work experiences in
a resume can be labeled and extracted as a tabular field.
Tabular fields provide table, row and cell confidence with the 2024-11-30 (GA) API:
· Fixed or dynamic tables add confidence support for the following elements:
o Table confidence, a measure of how accurately the entire table is recognized.
o Row confidence, a measure of recognition of an individual row.
o Cell confidence, a measure of recognition of an individual cell.
· The recommended approach is to review the accuracy in a top-down manner
starting with the table first, followed by the row and then the cell. See confidence
and accuracy scores to learn more about table, row, and cell confidence.
Overlapping fields
Custom neural v4.0 2024-11-30 (GA) model supports overlapping fields:
To use the overlapping fields, your dataset needs to contain at least one sample with the
expected overlap. To label an overlap, use region labeling to designate each of the
spans of content (with the overlap) for each field. Labeling an overlap with field selection
(highlighting a value) fails in the Studio as region labeling is the only supported labeling
tool for indicating field overlaps. Overlap support includes:
· Complete overlap. The same set of tokens are labeled for two different fields.
. Partial overlap. Some tokens belong to both fields, but there are tokens that are
only part of one field or the other.
Overlapping fields have some limits:
. Any token or word can only be labeled as two fields.
· overlapping fields in a table can't span table rows.
· Overlapping fields can only be recognized if at least one sample in the dataset
contains overlapping labels for those fields.


<!---- Page 263 ---------------------------------------------------------------------------------------------------------------------------------->
To use overlapping fields, label your dataset with the overlaps and train the model with
the API version ** 2024-11-30 (GA) **.
Supported languages and locales
See our Language Support-custom models for a complete list of supported languages.
Supported regions
As of October 18, 2022, Document Intelligence custom neural model training will only
be available in the following Azure regions until further notice:
· Australia East
· Brazil South
· Canada Central
· Central India
· Central US
· East Asia
· East US
. East US2
· France Central
· Japan East
· South Central US
· Southeast Asia
· UK South
· West Europe
· West US2
· US Gov Arizona
· US Gov Virginia
? Tip
You can copy a model trained in one of the select regions listed to any other
region and use it accordingly.
Use the REST API or Document Intelligence Studio [ to copy a model to another
region.
Input requirements


<!---- Page 264 ---------------------------------------------------------------------------------------------------------------------------------->
· For best results, provide one clear photo or high-quality scan per document.
· Supported file formats:
[] Expand table
Model
PDF
Image:
jpeg/ jpg, png, bmp,
tiff, heif
Microsoft Office:
Word (docx), Excel (xlsx),
PowerPoint (pptx), and HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom neural
✔
✔
* Microsoft Office files are currently not supported for other models or versions.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 x 50 pixels and 10,000 px x 10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 -point text at 150 dots per inch.
· For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
. For custom extraction model training, the total size of training data is 50 MB for
template model and 1G-MB for the neural model.
· For custom classification model training, the total size of training data is 1GB with a
maximum of 10,000 pages.
Best practices

| Model | PDF | Image: jpeg/ jpg, png, bmp, tiff, heif | Microsoft Office: Word (docx), Excel (xlsx), PowerPoint (pptx), and HTML |
| --- | --- | --- | --- |
| Read | ✔ :selected: | ✔ :selected: | ✔ :selected: |
| Layout | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| General Document | ✔ :selected: | ✔ :selected: |  |
| Prebuilt | ✔ :selected: | :selected: ✔ |  |
| Custom neural | ✔ :selected: | :selected: ✔ |  |


<!---- Page 265 ---------------------------------------------------------------------------------------------------------------------------------->
Custom neural models differ from custom template models in a few different ways. The
custom template or model relies on a consistent visual template to extract the labeled
data. Custom neural models support structured, and semi-structured to extract fields.
When you're choosing between the model types, start with a neural model, and test to
determine if it supports your functional needs.
. Dealing with variations - Custom neural models can generalize across different
formats of a single document type. As a best practice, create a single model for all
variations of a document type. Add at least five labeled samples for each of the
different variations to the training dataset.
· Field naming - When you label the data, labeling the field relevant to the value
improves the accuracy of the key-value pairs extracted. For example, for a field
value containing the supplier ID, consider naming the field supplier_id. Field names
should be in the language of the document.
· Labeling contiguous values - Value tokens/words of one field must be either:
o In a consecutive sequence in natural reading order, without interleaving with
other fields
o In a region that don't cover any other fields
· Representative data - Values in training cases should be diverse and
representative. For example, if a field is named date, values for this field should be
a date. Synthetic value like a random string can affect model performance.
Current Limitations
· Custom neural model doesn't recognize values split across page boundaries.
· Custom neural unsupported field types are ignored if a dataset labeled for custom
template models is used to train a custom neural model.
. Custom neural models are limited to 20 build operations per month for versions
3.x. Open a support request if you need the limit increased. For more information,
see Document Intelligence service quotas and limits.
Training a model
Custom neural models are available in the v3.0 and later models.
[] Expand table


<!---- Page 266 ---------------------------------------------------------------------------------------------------------------------------------->
Document
Type
REST API
SDK
Label and Test Models
Custom
Document Intelligence
Document Intelligence
Document Intelligence
document
3.1
SDK
Studio z
The Build operation to train model supports a new buildMode property, to train a
custom neural model, set the buildMode to neural .
Bash
https://{endpoint}/documentintelligence/documentModels:build?api-
version=2024-11-30
{
"modelId": "string",
"description": "string",
"buildMode": "neural",
"azureBlobSource":
{
"containerUrl": "string",
"prefix": "string"
}
}
Billing
With version v4.0 2024-11-30 (GA), you can train your custom neural model for longer
durations than the standard 30 minutes. Previous versions are limited to 30 minutes per
training instance, with a total of 20 free training instances per month. With version v4.0
2024-11-30 (GA), you can receive 10 hours of free model training, and train a model for
as long as 10 hours.
You can choose to spend all of 10 free hours on a single model build with a large set of
data, or utilize it across multiple builds by adjusting the maximum duration value for the
build operation by specifying maxTrainingHours :
Bash
POST https://{endpoint}/documentintelligence/documentModels:build?api-
version=2024-11-30
{
"modelId": "string",
"description": "string",
"buildMode": "neural",

| Document Type | REST API | SDK | Label and Test Models |
| --- | --- | --- | --- |
| Custom | Document Intelligence | Document Intelligence | Document Intelligence |
| document | 3.1 | SDK | Studio z |


<!---- Page 267 ---------------------------------------------------------------------------------------------------------------------------------->
... ,
"maxTrainingHours": 10
}
1 Important
. If you would like to train more neural models or train models for a longer time
period that exceed 10 hours, billing charges apply. For details on the billing
charges, refer to the pricing_page [ .
. You can opt in for this paid training service by setting the maxTrainingHours to
the desired maximum number of hours. API calls with no budget but with the
maxTrainingHours set to over 10 hours fail.
· Each build takes a different amount of time depending on the type and size of
the training dataset. Billing is calculated for the actual time spent training the
neural model with a minimum of 30 minutes per training job.
· This paid training feature enables you to train larger data sets for longer
durations with flexibility in the training hours.
Bash
GET /documentModels/{myCustomModel}
{
"modelId": "myCustomModel",
"trainingHours": 0.23,
"docTypes": { ... },
...
}
0
Note
For Document Intelligence versions v3.1 (2023-07-31) and v3.0 (2022-08-31),
custom neural model's paid training isn't enabled. For the two older versions,
there's a maximum of 30-minutes training duration per model. If you would like to
train more than 20 model instances, you can create an Azure support ticket to
increase in the training limit.
Next steps


<!---- Page 268 ---------------------------------------------------------------------------------------------------------------------------------->
Learn to create and compose custom models:
Build a custom model
Compose custom models
Feedback
Was this page helpful?
& Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 269 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence custom
classification model
Article · 02/27/2025
This content applies to:
v4.0 (GA) | Previous version:
v3.1 (GA)
1 Important
· The v4.0 2024-11-30 (GA) API, custom classification model doesn't split
documents by default during the analyzing process.
. You need to explicitly set the splitMode property to auto to preserve the
behavior from previous releases. The default for splitMode is none.
· If your input file contains multiple documents, you need to enable splitting by
setting the splitMode to auto.
Azure AI Document Intelligence is a cloud-based Azure AI service that enables you to
build intelligent document processing solutions. Document Intelligence APIs analyze
images, PDFs, and other document files to extract and detect various content, layout,
style, and semantic elements.
Custom classification models are deep-learning-model types that combine layout and
language features to accurately detect and identify documents you process within your
application. Custom classification models perform classification of an input file one page
at a time to identify the documents within and can also identify multiple documents or
multiple instances of a single document within an input file.
Model capabilities
1 Note
· Custom classification v4.0 2024-11-30 (GA) models support incremental
training. You can add new samples to existing classes or add new classes by
referencing an existing classifier.
· Custom classification v3.1 2023-07-31 (GA) model doesn't support model
copy. To use the model copy feature, train the model using the latest v4.0
(GA) model.


<!---- Page 270 ---------------------------------------------------------------------------------------------------------------------------------->
Custom classification models can analyze a single- or multi-file documents to identify if
any of the trained document types are contained within an input file. Here are the
currently supported scenarios:
. A single file containing one document type, such as a loan application form.
. A single file containing multiple document types. For instance, a loan application
package that contains a loan application form, payslip, and bank statement.
. A single file containing multiple instances of the same document. For instance, a
collection of scanned invoices.
Training a custom classifier requires at least two distinct classes and a minimum of
five document samples per class. The model response contains the page ranges for
each of the classes of documents identified.
The maximum allowed number of classes is 1,000. The maximum allowed number of
document samples per class is 100 .
The model classifies each page of the input document, unless specified, to one of the
classes in the labeled dataset. You can specify the page numbers to analyze in the input
document as well. To set the threshold for your application, use the confidence score
from the response.
Incremental training
With custom models, you need to maintain access to the training dataset to update
your classifier with new samples for an existing class, or add new classes. Classifier
models now support incremental training where you can reference an existing classifier
and append new samples for an existing class or add new classes with samples.
Incremental training enables scenarios where data retention is a challenge and the
classifier needs to be updated to align with changing business needs. Incremental
training is supported with models trained with API version v4.0 2024-11-30 (GA) .
1 Important
Incremental training is only supported with models trained with the same API
version. If you're trying to extend a model, use the API version the original model
was trained with to extend the model. Incremental training is only supported with
API version v4.0 2024-11-30 (GA) or later.


<!---- Page 271 ---------------------------------------------------------------------------------------------------------------------------------->
Incremental training requires that you provide the original model ID as the
baseClassifierId. See incremental training to learn more about how to use incremental
training.
Office document type support
You can now train classifiers to recognize document types in various formats including
PDF, images, Word, PowerPoint, and Excel. When assembling your training dataset, you
can add documents of any of the supported types. The classifier doesn't require you to
explicitly label specific types. As a best practice, ensure your training dataset has at least
one sample of each format to improve the overall accuracy of the model.
Compare custom classification and composed models
A custom classification model can replace a composed model in some scenarios but
there are a few differences to be aware of:
[ Expand table
Capability
Custom classifier process
Composed model process
Analyze a single document of
unknown type belonging to
one of the types trained for
extraction model processing.
· Requires multiple calls.
· Requires a single call to a
composed model containing
the model corresponding to
the input document type.
· Call the classification model
based on the document class.
This step allows for a
confidence-based check before
invoking the extraction model
analysis.
· Invoke the extraction model.
Analyze a single document of
unknown type belonging to
several types trained for
extraction model processing.
· Requires multiple calls.
. Make a call to the classifier
that ignores documents not
matching a designated type for
extraction.
· Invoke the extraction model.
· Requires a single call to a
composed model. The service
selects a custom model within
the composed model with the
highest match.
. A composed model can't
ignore documents.
Analyze a file containing
multiple documents of
known or unknown type
belonging to one of the
types trained for extraction
model processing.
· Requires multiple calls.
. Call the extraction model for
each identified document in the
input file.
. Invoke the extraction model.
· Requires a single call to a
composed model.
. The composed model
invokes the component model
once on the first instance of
the document.

| Capability | Custom classifier process | Composed model process |
| --- | --- | --- |
| Analyze a single document of unknown type belonging to one of the types trained for extraction model processing. | · Requires multiple calls. | · Requires a single call to a composed model containing the model corresponding to the input document type. |
|  | · Call the classification model based on the document class. This step allows for a confidence-based check before invoking the extraction model analysis. |  |
|  | · Invoke the extraction model. |  |
| Analyze a single document of unknown type belonging to several types trained for extraction model processing. | · Requires multiple calls. . Make a call to the classifier that ignores documents not matching a designated type for extraction. · Invoke the extraction model. | · Requires a single call to a composed model. The service selects a custom model within the composed model with the highest match. . A composed model can't ignore documents. :selected: |
| Analyze a file containing multiple documents of known or unknown type belonging to one of the types trained for extraction model processing. | · Requires multiple calls. . Call the extraction model for each identified document in the input file. . Invoke the extraction model. | · Requires a single call to a composed model. . The composed model invokes the component model once on the first instance of the document. :selected: :selected: :unselected: |


<!---- Page 272 ---------------------------------------------------------------------------------------------------------------------------------->
Capability
Custom classifier process
Composed model process
. The remaining documents
are ignored.
Language support
Classification models can now be trained on documents of different languages. See
supported languages for a complete list.
Input requirements
Supported file formats:
[ Expand table
Model
PDF
Image:
Microsoft Office:
Word (docx), Excel (xlxs),
PowerPoint (pptx)
jpeg/jpg, png, bmp, tiff,
heif
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
V (not supported in the studio)
· For best results, provide five clear photos or high-quality scans per document type.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 x 50 pixels and 10,000 px x 10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.

| Capability | Custom classifier process | Composed model process |
| --- | --- | --- |
|  |  | . The remaining documents are ignored. |

| Model | PDF | Image: |  |  | Microsoft Office: Word (docx), Excel (xlxs), PowerPoint (pptx) |
| --- | --- | --- | --- | --- | --- |
|  |  | jpeg/jpg, | png, bmp, tiff, heif |  |  |
| Read | :selected: ✔ |  | :selected: ✔ |  | ✔ :selected: |
| Layout | :selected: ✔ |  | :selected: ✔ |  | ✔ :selected: |
| General Document | :selected: ✔ |  | :selected: ✔ |  |  |
| Prebuilt | :selected: ✔ |  | :selected: ✔ |  |  |
| Custom extraction | :selected: ✔ |  | :selected: ✔ |  |  |
| Custom classification | :selected: ✔ |  | :selected: ✔ |  | V (not supported in the studio) :selected: |


<!---- Page 273 ---------------------------------------------------------------------------------------------------------------------------------->
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 -point text at 150 dots per inch
(DPI).
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
. For custom extraction model training, the total size of training data is 50 MB for
template model and 1G-MB for the neural model.
. For custom classification model training, the total size of training data is 2 GB with
a maximum of 25,000 pages.
Document splitting
When you have more than one document in a file, the classifier can identify the different
document types contained within the input file. The classifier response contains the
page ranges for each of the identified document types contained within a file. This
response can include multiple instances of the same document type.
The analyze operation now includes a splitMode property that gives you granular
control over the splitting behavior.
. To treat the entire input file as a single document for classification set the
splitMode to none. When you do so, the service returns just one class for the entire
input file.
· To classify each page of the input file, set the splitMode to perPage. The service
attempts to classify each page as an individual document.
. Set the splitMode to auto and the service identifies the documents and associated
page ranges.
Best practices
Custom classification models require a minimum of five samples per class to train. If the
classes are similar, adding extra training samples improves model accuracy.
The classifier attempts to assign each document to one of the classes, if you expect the
model to see document types not in the classes that are part of the training dataset, you
should plan to set a threshold on the classification score or add a few representative
samples of the document types to an "other" class. Adding an "other" class ensures
that unneeded documents don't affect your classifier quality.


<!---- Page 274 ---------------------------------------------------------------------------------------------------------------------------------->
Training a model
Custom classification models are supported by the v4.0 2024-11-30 (GA) API. Document
Intelligence Studio provides a no-code user interface to interactively train a custom
classifier. Follow the how to guide to get started.
When using the REST API, if you organize your documents by folders, you can use the
azureBlobSource property of the request to train a classification model.
JSON
https://{endpoint}/documentintelligence/documentClassifiers:build?api-
version=2024-11-30
{
"classifierId": "demo2.1",
"description": "",
"docTypes": {
"car-maint": {
"azureBlobSource": {
"containerUrl": "SAS URL to container",
"prefix": "sample1/car-maint/"
}
},
"cc-auth": {
"azureBlobSource": {
"containerUrl": "SAS URL to container",
"prefix": "sample1/cc-auth/"
}
},
"deed-of-trust": {
"azureBlobSource": {
"containerUrl": "SAS URL to container",
"prefix": "sample1/deed-of-trust/"
}
}
}
}
Alternatively, if you have a flat list of files or only plan to use a few select files within
each folder to train the model, you can use the azureBlobFileListSource property to
train the model. This step requires a file list in JSON Lines [ format. For each class,
add a new file with a list of files to be submitted for training.
JSON


<!---- Page 275 ---------------------------------------------------------------------------------------------------------------------------------->
{
"classifierId": "demo2",
"description": "",
"docTypes": {
"car-maint": {
"azureBlobFileListSource": {
"containerUrl": "SAS URL to container",
"fileList": "{path to dataset root}/car-maint.jsonl"
}
},
"cc-auth": {
"azureBlobFileListSource": {
"containerUrl": "SAS URL to container",
"fileList": "{path to dataset root}/cc-auth.jsonl"
}
},
"deed-of-trust": {
"azureBlobFileListSource": {
"containerUrl": "SAS URL to container",
"fileList": "{path to dataset root}/deed-of-trust.jsonl"
}
}
}
}
As an example, the file list car-maint. jsonl contains the following files.
JSON
{"file": "classifier/car-maint/Commercial Motor Vehicle - Adatum. pdf" }
{"file": "classifier/car-maint/Commercial Motor Vehicle - Fincher. pdf" }
{"file": "classifier/car-maint/Commercial Motor Vehicle - Lamna.pdf" }
{"file": "classifier/car-maint/Commercial Motor Vehicle - Liberty. pdf" }
{"file": "classifier/car-maint/Commercial Motor Vehicle - Trey. pdf" }
Overwriting a model
1 Note
The v4.0 2024-11-30 (GA) custom classification model supports overwriting a model
in-place.
You can now update the custom classification in-place. Directly overwriting the model
would lose you the ability to compare model quality before deciding to replace the
existing model. Model overwriting is allowed when the allowOverwrite property is


<!---- Page 276 ---------------------------------------------------------------------------------------------------------------------------------->
explicitly specified in the request body. It's impossible to recover the overwritten,
original model once this action is performed.
JSON
{
"classifierId": "existingClassifierName",
"allowOverwrite": true, // Default=false
...
}
Copy a model
1 Note
The custom classification v4.0 2024-11-30 (GA) model supports copying a model to
and from any of the following regions:
· East US
· West US2
· West Europe
Use the REST API or Document Intelligence Studio [ to copy a model to another
region.
Generate Copy authorization request
The following HTTP request gets copy authorization from your target resource. You
need to enter the endpoint and key of your target resource as headers.
HTTP
POST
https://myendpoint.cognitiveservices.azure.com/documentintelligence/document
Classifiers : authorizeCopy?api-version=2024-11-30
Ocp-Apim-Subscription-Key: {<your-key>}
Request body
JSON


<!---- Page 277 ---------------------------------------------------------------------------------------------------------------------------------->
{
"classifierId": "targetClassifier",
"description": "Target classifier description"
}
You receive a 200 response code with response body that contains the JSON payload
required to initiate the copy.
JSON
{
"targetResourceId":
"/subscriptions/targetSub/resourceGroups/targetRG/providers/Microsoft.Cognit
iveServices/accounts/targetService",
"targetResourceRegion": "targetResourceRegion",
"targetClassifierId": "targetClassifier",
"targetClassifierLocation":
"https://targetEndpoint.cognitiveservices. azure. com/documentintelligence/doc
umentClassifiers/targetClassifier",
"accessToken": "accessToken",
"expirationDateTime": "timestamp"
}
Start Copy operation
The following HTTP request starts the copy operation on the source resource. You need
to enter the endpoint and key of your source resource as the url and header. Notice that
the request URL contains the classifier ID of the source classifier you want to copy.
HTTP
POST
{endpoint}/documentintelligence/documentClassifiers/{classifierId} : copyTo?
api-version=2024-11-30
Ocp-Apim-Subscription-Key: {<your-key>}
The body of your request is the response from the previous step.
JSON
{
"targetResourceId":
"/subscriptions/targetSub/resourceGroups/targetRG/providers/Microsoft. Cognit
iveServices/accounts/targetService",
"targetResourceRegion": "targetResourceRegion",
"targetClassifierId": "targetClassifier",
"targetClassifierLocation":


<!---- Page 278 ---------------------------------------------------------------------------------------------------------------------------------->
"https://targetEndpoint. cognitiveservices. azure. com/documentintelligence/doc
umentClassifiers/targetClassifier",
"accessToken": "accessToken",
"expirationDateTime": "timestamp"
}
Model response
Analyze an input file with the document classification model.
JSON
https://{endpoint}/documentintelligence/documentClassifiers/{classifier}:ana
lyze?api-version=2024-11-30
The v4.0 2024-11-30 (GA) API enables you to specify pages to analyze from the input
document using the pages query parameter in the request.
The response contains the identified documents with the associated page ranges in the
documents section of the response.
JSON
{
"documents": [
{
"docType": "formA",
"boundingRegions": [
{ "pageNumber": 1, "polygon": [ ... ] },
{ "pageNumber": 2, "polygon": [ ... ] }
1,
"confidence": 0.97,
"spans": []
},
{
"docType": "formB",
"boundingRegions": [
{ "pageNumber": 3, "polygon": [ ... ] }
1,
"confidence": 0.97,
"spans": []
}, ...
]
}


<!---- Page 279 ---------------------------------------------------------------------------------------------------------------------------------->
Next steps
Learn to create custom classification models:
Build a custom classification model
Custom models overview
Feedback
Was this page helpful?
Yes
P No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 280 ---------------------------------------------------------------------------------------------------------------------------------->
Best practices: generating labeled
datasets
Article · 12/11/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
Custom models (template and neural) require a labeled dataset of at least five documents
to train a model. The quality of the labeled dataset affects the accuracy of the trained
model. This guide helps you learn more about generating a model with high accuracy by
assembling a diverse dataset and provides best practices for labeling your documents.
Understand the components of a labeled
dataset
A labeled dataset consists of several files:
· You provide a set of sample documents (typically PDFs or images). A minimum of five
documents is needed to train a model.
. Additionally, the labeling process generates the following files:
o A fields. json file is created when the first field is added. There's one
fields. json file for the entire training dataset, the field list contains the field
name and associated sub fields and types.
o The Studio runs each of the documents through the Layout API. The layout
response for each of the sample files in the dataset is added as {file} . ocr. json .
The layout response is used to generate the field labels when a specific span of
text is labeled.
o A {file}. labels. json file is created or updated when a field is labeled in a
document. The label file contains the spans of text and associated polygons from
the layout output for each span of text the user adds as a value for a specific field.
Video: Custom label tips and pointers
· The following video is the first of two presentations intended to help you build
custom models with higher accuracy (The second presentation examines Best
practices for labeling documents).


<!---- Page 281 ---------------------------------------------------------------------------------------------------------------------------------->
. We explore how to create a balanced data set and select the right documents to
label. This process sets you on the path to higher quality models.
https://www.microsoft.com/en-us/videoplayer/embed/RWWHru?postJsllMsg=true
Create a balanced dataset
Before you start labeling, it's a good idea to look at a few different samples of the
document to identify which samples you want to use in your labeled dataset. A balanced
dataset represents all the typical variations you would expect to see for the document.
Creating a balanced dataset results in a model with the highest possible accuracy. A few
examples to consider are:
· Document formats: If you expect to analyze both digital and scanned documents,
add a few examples of each type to the training dataset.
· Variations (template model): Consider splitting the dataset into folders and train a
model for each of variation. Any variations that include either structure or layout
should be split into different models. You can then compose the individual models
into a single composed model.
· Variations (Neural models): When your dataset has a manageable set of variations,
about 15 or fewer, create a single dataset with a few samples of each of the different
variations to train a single model. If the number of template variations is larger than
15, you train multiple models and compose them together.
. Tables: For documents containing tables with a variable number of rows, ensure that
the training dataset also represents documents with different numbers of rows.
· Multi page tables: When tables span multiple pages, label a single table. Add
documents to the training dataset with the expected variations represented-
documents with the table on a single page only and documents with the table
spanning two or more pages with all the rows labeled.
. Optional fields: If your dataset contains documents with optional fields, validate that
the training dataset has a few documents with the options represented.
Start by identifying the fields
Take the time to identify each of the fields you plan to label in the dataset. Pay attention to
optional fields. Define the fields with the labels that best match the supported types.
Use the following guidelines to define the fields:


<!---- Page 282 ---------------------------------------------------------------------------------------------------------------------------------->
· For custom neural models, use semantically relevant names for fields. For example, if
the value being extracted is Effective Date, name it effective_date or
EffectiveDate not a generic name like date1.
. Ideally, name your fields with Pascal or camel case.
· If a value is part of a visually repeating structure and you only need a single value,
label it as a table and extract the required value during post-processing.
· For tabular fields spanning multiple pages, define and label the fields as a single
table.
1 Note
Custom neural models share the same labeling format and strategy as custom
template models. Currently custom neural models only support a subset of the field
types supported by custom template models.
Model capabilities
Custom neural models currently only support key-value pairs, structured fields (tables),
and selection marks.
0
Expand table
Model
type
Form fields
Selection
marks
Tabular
fields
Signature
Region
Overlapping
fields
Custom
neural
Supported
Supported
Supported
Unsupported
Supported
1
Supported2
Custom
template
Supported
Supported
Supported
Supported
Supported
Unsupported
1 Region labeling implementation differs between template and neural models. For
template models, the training process injects synthetic data at training time if no text is
found in the region labeled. With neural models, no synthetic text is injected and the
recognized text is used as is.
2 Overlapping fields are supported starting with the API version v4.0 2024-11-30 (GA) .
Overlapping fields have some limits. For more information, see overlapping fields.
Tabular fields

| Model type | Form fields | Selection marks | Tabular fields | Signature :unselected: | Region | Overlapping fields |
| --- | --- | --- | --- | --- | --- | --- |
| Custom neural | :selected: Supported | :selected: Supported | :selected: Supported | :unselected: Unsupported :unselected: | :selected: Supported 1 | :selected: Supported2 |
| Custom template | :selected: Supported | :selected: Supported | :selected: Supported | :selected: Supported | :selected: Supported | Unsupported |


<!---- Page 283 ---------------------------------------------------------------------------------------------------------------------------------->
Tabular fields (tables) are supported with custom neural models with API version v4.0
2024-11-30 (GA) . Models trained with API version 2022-06-30-preview or later will accept
tabular field labels and documents analyzed with the model with API version 2022-06-30-
preview or later will produce tabular fields in the output within the documents section of
the result in the analyzeResult object.
Tabular fields support cross page tables by default. To label a table that spans multiple
pages, label each row of the table across the different pages in the single table. As a best
practice, ensure that your dataset contains a few samples of the expected variations. For
example, include both samples where an entire table is on a single page and samples of a
table spanning two or more pages.
Tabular fields are also useful when extracting repeating information within a document
that isn't recognized as a table. For example, a repeating section of work experiences in a
resume can be labeled and extracted as a tabular field.
1 Note
Table field when labeled are extracted as part of the documents section of the
response. The response also contains a tables section which contains the tables
extracted from the document by the layout model. If you have labeled a field as a
table, look for the field in the documents section of the response.
Labeling guidelines
· Labeling values is required. Don't include the surrounding text. For example when
labeling a checkbox, name the field to indicate the check box selection for example
selectionYes and selectionNo rather than labeling the yes or no text in the
document.
. Don't provide interleaving field values. The value of words and/or regions of one
field must be a consecutive sequence in natural reading order.
· Consistent labeling. If a value appears in multiple contexts within the document,
consistently pick the same context across documents to label the value.
· Visually repeating data. Tables support visually repeating groups of information not
just explicit tables. Explicit tables are identified in tables section of the analyzed
documents as part of the layout output and don't need to be labeled as tables. Only
label a table field if the information is visually repeating and not identified as a table
as part of the layout response. An example would be the repeating work experience
section of a resume.


<!---- Page 284 ---------------------------------------------------------------------------------------------------------------------------------->
· Region labeling (custom template). Labeling specific regions allows you to define a
value when none exists. If the value is optional, ensure that you leave a few sample
documents with the region not labeled. When labeling regions, don't include the
surrounding text with the label.
· Overlapping fields (custom neural). Label the field overlaps using region labeling.
Ensure that you have at least on sample that describes how the fields can overlap in
your training dataset.
Next steps
· Train a custom model:
How to train a model
· View the REST APIs:
Document Intelligence API v4.0:2024-11-30 (GA)
Document Intelligence API v3.1:2023-07-31 (GA)
Feedback
Was this page helpful?
Yes
P No
Provide product feedback z | Get help at Microsoft Q&A


<!---- Page 285 ---------------------------------------------------------------------------------------------------------------------------------->
Tips for building labeled datasets
Article · 12/11/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
This article highlights the best methods for labeling custom model datasets in the
Document Intelligence Studio. Labeling documents can be time consuming when you
have a large number of labels, long documents, or documents with varying structure.
These tips should help you label documents more efficiently.
Video: Custom labels best practices
. The following video is the second of two presentations intended to help you build
custom models with higher accuracy (the first presentation explores How to create
a balanced data set).
. We examine best practices for labeling your selected documents. With semantically
relevant and consistent labeling, you should see an improvement in model
performance.
https://www.microsoft.com/en-us/videoplayer/embed/RE5fZKB?postJsllMsg=true
Search
The Studio now includes a search box for instances when you know you need to find
specific words to label, but just don't know where to locate them in the document.
Simply search for the word or phrase and navigate to the specific section in the
document to label the occurrence.
Auto label tables
Tables can be challenging to label, when they have many rows or dense text. If the
layout table extracts the result you need, you should just use that result and skip the
labeling process. In instances where the layout table isn't exactly what you need, you
can start with generating the table field from the values layout extracts. Start by
selecting the table icon on the page and select on the auto label button. You can then
edit the values as needed. Auto label currently only supports single page tables.
Shift select


<!---- Page 286 ---------------------------------------------------------------------------------------------------------------------------------->
When labeling a large span of text, rather than mark each word in the span, hold down
the shift key as you're selecting the words to speed up labeling and ensure you don't
miss any words in the span of text.
Region labeling
A second option for labeling larger spans of text is to use region labeling. When region
labeling is used, the OCR results are populated in the value at training time. The
difference between the shift select and region labeling is only in the visual feedback the
shift labeling approach provides.
Label overlapping fields
Overlapping fields are supported for fields and table cells. If you expect your analyze
results to contain overlapping fields, you should add at least one sample to the training
dataset with the specific field overlaps labeled. To label an overlapping field, use the
region labeling feature to select the regions for each field. Both complete and partial
overlaps are supported. Any single word in the document can only be labeled for two
fields.
Field subtypes
When creating a field, select the right subtype to minimize post processing, for instance
select the dmy option for dates to extract the values in a dd-mm-yyyy format.
Next steps
· Learn more about custom labeling:
Custom labels
· Learn more about custom template models:
Custom models
Feedback
Was this page helpful?
Yes
No


<!---- Page 287 ---------------------------------------------------------------------------------------------------------------------------------->
Provide product feedback
Get help at Microsoft Q&A


<!---- Page 288 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence custom model
lifecycle
Article · 12/11/2024
This content applies to:
v4.0 (GA)
v3.1 (GA)
With the v3.1 (GA) and later APIs, custom models introduce a expirationDateTime
property that is set for each model trained with the 3.1 API or later. Custom models are
dependent on the API version of the Layout API version and the API version of the
model build operation. For best results, continue to use the API version the model was
trained with for all analyze requests. The guidance applies to all Document Intelligence
custom models including extraction and classification models.
Models trained with GA API version
With the v3.1 API, custom models introduce a new model expiration property. The
model expiration is set to two years from the date the model is built for all requests that
use a GA API to build a model. To continue to use the model past the expiration date,
you need to train the model with a current GA API version. The API version can be the
one that the model was originally trained with or a later API version. The following figure
illustrates the options when you need to retrain an expiring or expired model.
Train modelA
Train modelB
modelB expires
Use the 2023-07-31 API version to re-train
GA
End of life
2022-08-31 (v3.0)
Announce
deprecation
modelA expires
Use either API version to re-train
GA
End of life
2023-07-31 (v3.1)
Announce
deprecation
Models trained with preview API version
For build requests, using a preview API version, the expiration date is set to two years
from the date the model is built. Models trained with a preview API shouldn't be used in
production and should be retrained once the corresponding GA API version is available.
Compatibility between preview API versions and GA API versions isn't always


<!---- Page 289 ---------------------------------------------------------------------------------------------------------------------------------->
maintained. Models trained with a preview API version are no longer usable the
corresponding GA API is available.
Viewing model expiration date
The GET model API returns the model details including the expirationDateTime
property.
rest
GET /documentModels/{customModelId}?api-version={apiVersion}
{
"modelId": "{customModelId}",
"description": "{customModelDescription}",
"createdDateTime": "2021-09-24T12: 54: 35Z",
"expirationDateTime": "2023-01-01T00:00:00Z",
"apiVersion": "2023-07-31",
"docTypes": { ... }
}
Retrain a model
To retrain a model with a more recent API version, ensure that the layout results for the
documents in your training dataset correspond to the API version of the build model
request. For instance, if you plan to build the model with the v3.1:2023-07-31 API
version, the corresponding *. ocr.json files in your training dataset should also be
generated with the v3.1:2023-07-31 API version. The ocr.json files are generated by
running layout on your training dataset. To validate the version of the layout results,
check the apiVersion property in the analyzeResult of the ocr.json documents.
Next steps
Learn to create and compose custom models:
Build a custom model
Compose custom models
Feedback
Was this page helpful?
Yes
No


<!---- Page 290 ---------------------------------------------------------------------------------------------------------------------------------->
Provide product feedback
Get help at Microsoft Q&A


<!---- Page 291 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence add-on capabilities
Article · 02/27/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA) ::: moniker-end
Capabilities
Document Intelligence supports more sophisticated and modular analysis capabilities. Use the
add-on features to extend the results to include more features extracted from your documents.
Some add-on features incur an extra cost. These optional features can be enabled and disabled
depending on the scenario of the document extraction. To enable a feature, add the associated
feature name to the features query string property. You can enable more than one add-on
feature on a request by providing a comma-separated list of features. The following add-on
capabilities are available for 2023-07-31 (GA) and later releases.
· ocrHighResolution
· formulas
· styleFont
· barcodes
· languages
· Searchable PDF support
· queryFields
· keyValuePairs
1 Note
Not all models or Microsoft Office file types support add-on capabilities. For more
information, see model data extraction.
Version availability
[] Expand table


<!---- Page 292 ---------------------------------------------------------------------------------------------------------------------------------->
Add-on Capability
Add-
On/Free
2024-11-30
(GA)
2023-07-31
(GA)
2022-08-31
(GA)
v2.1
(GA)
Barcode extraction
Free
n/a
n/a
Language detection
Free
n/a
n/a
Key value pairs
Free
n/a
n/a
n/a
Searchable PDF
Free
n/a
n/a
n/a
Font property
extraction
Add-On
n/a
n/a
Formula extraction
Add-On
n/a
n/a
High resolution
extraction
Add-On
n/a
n/a
Query fields
Add-On
n/a
n/a
n/a
* Add-On - Query fields are priced differently than the other add-on features. See pricing
for details.
** Add-On - Searchable pdf is available only with Read model as an add-on feature.
Supported file formats
· PDF
· Images: JPEG / JPG, PNG, BMP, TIFF, HEIF
* Microsoft Office files are currently not supported.
High resolution extraction
The task of recognizing small text from large-size documents, like engineering drawings, is a
challenge. Often the text is mixed with other graphical elements and has varying fonts, sizes,
and orientations. Moreover, the text can be broken into separate parts or connected with other
symbols. Document Intelligence now supports extracting content from these types of
documents with the ocr. highResolution capability. You get improved quality of content
extraction from A1/A2/A3 documents by enabling this add-on capability.
REST API
Bash

| Add-on Capability | Add- On/Free | 2024-11-30 (GA) | 2023-07-31 (GA) | 2022-08-31 (GA) | v2.1 (GA) |
| --- | --- | --- | --- | --- | --- |
| Barcode extraction | Free | :selected: | :selected: | n/a | n/a |
| Language detection | Free | :selected: | :selected: | :unselected: n/a | n/a |
| Key value pairs | :unselected: Free | :selected: | n/a | :unselected: n/a | :unselected: n/a |
| Searchable PDF | Free | :selected: | n/a | :unselected: n/a | :unselected: n/a |
| Font property extraction | Add-On | :selected: | :selected: | :unselected: n/a | :unselected: n/a |
| Formula extraction | Add-On | :selected: | :selected: | :unselected: n/a | n/a |
| High resolution extraction | Add-On | :selected: | :selected: | :unselected: n/a | n/a |
| Query fields | Add-On | :selected: | n/a | :unselected: n/a | :unselected: n/a |


<!---- Page 293 ---------------------------------------------------------------------------------------------------------------------------------->
{your-resource-
endpoint}.cognitiveservices. azure. com/documentintelligence/documentModels/preb
uilt-layout : analyze?api-version=2024-11-30&features=ocrHighResolution
Formula extraction
The ocr. formula capability extracts all identified formulas, such as mathematical equations, in
the formulas collection as a top level object under content . Inside content, detected formulas
are represented as : formula: . Each entry in this collection represents a formula that includes
the formula type as inline or display, and its LaTeX representation as value along with its
polygon coordinates. Initially, formulas appear at the end of each page.
1 Note
The confidence score is hard-coded.
REST API
Bash
{your-resource-
endpoint}.cognitiveservices.azure.com/documentintelligence/documentModels/preb
uilt-layout : analyze?api-version=2024-11-30&features=formulas
Font property extraction
The ocr. font capability extracts all font properties of text extracted in the styles collection as
a top-level object under content. Each style object specifies a single font property, the text
span it applies to, and its corresponding confidence score. The existing style property is
extended with more font properties such as similarFontFamily for the font of the text,
fontStyle for styles such as italic and normal, fontWeight for bold or normal, color for color
of the text, and backgroundColor for color of the text bounding box.
REST API
Bash


<!---- Page 294 ---------------------------------------------------------------------------------------------------------------------------------->
{your-resource-
endpoint}.cognitiveservices. azure.com/documentintelligence/documentModels/preb
uilt-layout : analyze?api-version=2024-11-30&features=styleFont
Barcode property extraction
The ocr. barcode capability extracts all identified barcodes in the barcodes collection as a top
level object under content . Inside the content, detected barcodes are represented as
: barcode: . Each entry in this collection represents a barcode and includes the barcode type as
kind and the embedded barcode content as value along with its polygon coordinates. Initially,
barcodes appear at the end of each page. The confidence is hard-coded for as 1.
Supported barcode types
0
Expand table
Barcode Type
Example
QR Code
Code 39
0123456789ABC
Code 93
ABC-1234-/+
Code 128
0123456789abcdefg

| Barcode Type | Example |
| --- | --- |
| QR Code | :selected: :selected: |
| Code 39 | 0123456789ABC |
| Code 93 | ABC-1234-/+ |
| Code 128 | 0123456789abcdefg |


<!---- Page 295 ---------------------------------------------------------------------------------------------------------------------------------->
Barcode Type
Example
UPC (UPC-A &
UPC-E)
0
12345 67890
5
PDF417
EAN-8
9031
1017
EAN-13
9
780201 379624
Codabar
0123456789
Databar
(01) 0 9501101 53000 3

| Barcode Type | Example |
| --- | --- |
| UPC (UPC-A & UPC-E) | 0 12345 67890 5 |
| PDF417 |  |
| EAN-8 | 9031 1017 |
| EAN-13 | 9 780201 379624 |
| Codabar | 0123456789 |
| Databar |  |


<!---- Page 296 ---------------------------------------------------------------------------------------------------------------------------------->
Barcode Type
Example
Databar
Expanded
(01)1234567890123-ABCabc
ITF
050 12345 67890 0
Data Matrix
REST API
Bash
{your-resource-
endpoint}.cognitiveservices.azure.com/documentintelligence/documentModels/preb
uilt-layout : analyze?api-version=2024-11-30&features=barcodes
Language detection
Adding the languages feature to the analyzeResult request predicts the detected primary
language for each text line along with the confidence in the languages collection under
analyzeResult.
REST API
Bash

| Barcode Type | Example |
| --- | --- |
| Databar Expanded | (01)1234567890123-ABCabc |
| ITF | 050 12345 67890 0 |
| Data Matrix |  |


<!---- Page 297 ---------------------------------------------------------------------------------------------------------------------------------->
{your-resource-
endpoint}.cognitiveservices. azure. com/documentintelligence/documentModels/preb
uilt-layout : analyze?api-version=2024-11-30&features=languages
Searchable PDF
The searchable PDF capability enables you to convert an analog PDF, such as scanned-image
PDF files, to a PDF with embedded text. The embedded text enables deep text search within
the PDF's extracted content by overlaying the detected text entities on top of the image files.
1 Important
. Currently, only the Read model prebuilt-read supports the searchable PDF
capability. When using this feature, specify the modelId as prebuilt-read.
· Searchable PDF is included with the 2024-11-30 (GA) prebuilt-read model with no
usage cost for general PDF consumption.
Use searchable PDF
To use searchable PDF, make a POST request using the Analyze operation and specify the
output format as pdf :
Bash
POST /documentModels/prebuilt-read: analyze?output=pdf
{ . . . }
202
Once the Analyze operation is complete, make a GET request to retrieve the Analyze operation
results.
Upon successful completion, the PDF can be retrieved and downloaded as application/pdf .
This operation allows direct downloading of the embedded text form of PDF instead of
Base64-encoded JSON.
Bash
// Monitor the operation until completion.


<!---- Page 298 ---------------------------------------------------------------------------------------------------------------------------------->
GET /documentModels/prebuilt-read/analyzeResults/{resultId}
200
{. .. }
// Upon successful completion, retrieve the PDF as application/pdf.
GET /documentModels/prebuilt-read/analyzeResults/{resultId}/pdf
200 OK
Content-Type: application/pdf
Key-value Pairs
In earlier API versions, the prebuilt-document model extracted key-value pairs from forms and
documents. With the addition of the keyValuePairs feature to prebuilt-layout, the layout
model now produces the same results.
Key-value pairs are specific spans within the document that identify a label or key and its
associated response or value. In a structured form, these pairs could be the label and the value
the user entered for that field. In an unstructured document, they could be the date a contract
was executed on based on the text in a paragraph. The AI model is trained to extract
identifiable keys and values based on a wide variety of document types, formats, and
structures.
Keys can also exist in isolation when the model detects that a key exists, with no associated
value or when processing optional fields. For example, a middle name field can be left blank on
a form in some instances. Key-value pairs are spans of text contained in the document. For
documents where the same value is described in different ways, for example, customer/user,
the associated key is either customer or user (based on context).
REST API
Bash
{your-resource-
endpoint}.cognitiveservices.azure.com/documentintelligence/documentModels/prebuilt
-layout : analyze?api-version=2024-11-30&features=keyValuePairs
Query Fields
Query fields are an add-on capability to extend the schema extracted from any prebuilt model
or define a specific key name when the key name is variable. To use query fields, set the
features to queryFields and provide a comma-separated list of field names in the queryFields
property.


<!---- Page 299 ---------------------------------------------------------------------------------------------------------------------------------->
· Document Intelligence now supports query field extractions. With query field extraction,
you can add fields to the extraction process using a query request without the need for
added training.
. Use query fields when you need to extend the schema of a prebuilt or custom model or
need to extract a few fields with the output of layout.
· Query fields are a premium add-on capability. For best results, define the fields you want
to extract using camel case or Pascal case field names for multi-word field names.
· Query fields support a maximum of 20 fields per request. If the document contains a
value for the field, the field and value are returned.
. This release has a new implementation of the query fields capability that is priced lower
than the earlier implementation and should be validated.
1 Note
Document Intelligence Studio query field extraction is currently available with the Layout
and Prebuilt models 2024-11-30 (GA) API except for US tax models W2, 1098, and 1099.
Query field extraction
For query field extraction, specify the fields you want to extract and Document Intelligence
analyzes the document accordingly. Here's an example:
. If you're processing a contract in the Document Intelligence Studio , use the 2024-11-30
(GA) version:
Layout
API version: 2023-10-31 (Preview)
Service resource: fr-preview-demo
0
In Run analysis
Query fields
Analyze options
Drag & drop file
here or
Browse for files
or
Fetch from URL
. You can pass a list of field labels like Party1, Party2, TermsOfUse, PaymentTerms,
PaymentDate, and TermEndDate as part of the analyze document request.


<!---- Page 300 ---------------------------------------------------------------------------------------------------------------------------------->
Azure Al | Document Intelligence Studio
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
Document Intelligence Studio > Layout
Layout
API version: 2023-10-31 (Preview)
Service resource: fr-preview-dem
7
Run analysis
Query fields
Query fields
X
V
Drag & drop file
here or
Browse for files
Add fields you want to extract to the list and select the ones to be used in Analyze
operations. Press Save to update all edits in the list. Learn more about query fields.
or
Fetch from URL
+
Y
Party1
0
0
NEWS TODAY
Sample
Party2
0
Ů
TermsOfUse
0
Ů
PaymentTerms
0
Ů
layout-news.png
PaymentDate
0
Ô
--
Sample
LALILLE
TermEndDate
0
Ů
111
Maximum of 6/20 fields
I acknowledge that using Query Fields will incur usage to my account. See pricing.
layout-report.png
Delete all
Save
Cancel
Sample
-
a column, and then click i
Save time in Word wide new tuttoes that
show up where you need them. To change
the way a picture fiti in your document, click
i and a button for layout opdomus appears
Tiem. You can collapse pa
· Document Intelligence is able to analyze and extract the field data and return the values
in a structured JSON output.
. In addition to the query fields, the response includes text, tables, selection marks, and
other relevant data.
REST API
Bash
{your-resource-
endpoint}.cognitiveservices.azure.com/documentintelligence/documentModels/preb
uilt-layout : analyze?api-version=2024-11-
30&features=queryFields&queryFields=TERMS
Next steps
Learn more:
Read model
Layout model
SDK samples:
python
Find more samples:
Add-on capabilities


<!---- Page 301 ---------------------------------------------------------------------------------------------------------------------------------->


<!---- Page 302 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence query field
extraction
Article · 02/13/2025
Document Intelligence now supports query field to extend the schema of any prebuilt model to
extract the specific fields you need. Query fields can also be added to layout to extract fields in
addition to structure from forms or documents.
1 Note
Document Intelligence Studio query field extraction is currently available with layout and
prebuilt models, excluding the UX.Tax prebuilt models.
Query fields or key value pairs
Query fields and key value pairs perform similar functions, there are a few distinctions to be
aware of when deciding which feature to choose.
· Key value pairs are only available with layout and invoice models. If you're looking to
extend the schema for a prebuilt model, use query fields.
· You don't know the specific fields to be extracted, or the number of fields is large (greater
than 20), key value pairs might be a better solution.
· Key-value pairs extract the keys and values as they exist in the form or document, you
need to plan for any key variations. For example, keys First Name or Given Name. With
query fields, you define the key and the model only extracts the corresponding value.
. Use query fields when the value you require can't be described as a key value pair in the
document. For example, the agreement date of a contract.
For query field extraction, specify the fields you want to extract and Document Intelligence
analyzes the document accordingly. Here's an example:
. If you're processing a contract in the Document Intelligence Studio , use the 2024-11-30
(GA), API version:


<!---- Page 303 ---------------------------------------------------------------------------------------------------------------------------------->
Layout
API version: 2023-10-31 (Preview)
Service resource: fr-preview-demo
0
mi Run analysis
Query fields
Analyze options
Drag & drop file
here or
Browse for files
or
Fetch from URL
. You can pass a list of field labels like Party1, Party2, TermsOfUse, PaymentTerms
PaymentDate, and TermEndDate" as part of the AnalyzeDocument request.
Azure Al | Document Intelligence Studio
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
Document Intelligence Studio > Layout
Layout
API version: 2023-10-31 (Preview)\
Service resource: fr-preview-dem
oui Run analysis
Query fields
Query fields
X
Drag & drop file
here or
Browse for files
or
Fetch from URL
Add fields you want to extract to the list and select the ones to be used in Analyze
operations. Press Save to update all edits in the list. Learn more about query fields.
+
Y
NEWS TODAY
Sample
Party1
Ů
Party2
Ů
TermsOfUse
Ů
PaymentTerms
Ů
layout-news.png
PaymentDate
0
Ů
Sample
TermEndDate
Ů
Maximum of 6/20 fields
I acknowledge that using Query Fields will incur usage to my account. See pricing.
layout-report.png
Delete all
Save
Cancel
Sample
a column, and then click
Save time in Word with new tuttoen that
de mer a picture fiti in your document, click Reading is easier, 100, in d
in and a button for layout options appears
. In addition to the query fields, the response includes the model output. For a list of
features or schema extracted by each model, see model analysis features.
Query fields REST API request
Use the query fields feature with the prebuilt layout model, and add fields to the extraction
process without having to train a custom model:
HTTP


<!---- Page 304 ---------------------------------------------------------------------------------------------------------------------------------->
POST https://{endpoint}/documentintelligence/documentModels/prebuilt-
layout : analyze?api-version=2024-11-
30&features=queryFields&queryFields=OurReference, BookingDate HTTP/1.1
Host: *. cognitiveservices.azure.com
Content-Type: application/json
Ocp-Apim-Subscription-Key:
{
"urlSource": "https://raw.githubusercontent. com/Azure-Samples/cognitive-
services-REST-api-samples/master/curl/form-recognizer/rest-api/layout.png"
}
Next steps
Try the Document Intelligence Studio quickstart
Learn about other add-on capabilities


<!---- Page 305 ---------------------------------------------------------------------------------------------------------------------------------->
Interpret and improve accuracy and
confidence scores
Article · 03/03/2025
A confidence score indicates probability by measuring the degree of statistical certainty
that the extracted result is detected correctly. The estimated accuracy is calculated by
running a few different combinations of the training data to predict the labeled values.
In this article, learn to interpret accuracy and confidence scores and best practices for
using those scores to improve accuracy and confidence results.
Confidence scores
1 Note
· Field level confidence includes word confidence scores with 2024-11-30 (GA)
API version for custom models.
· Confidence scores for tables, table rows, and table cells are available starting
with the 2024-11-30 (GA) API version for custom models.
Document Intelligence analysis results return an estimated confidence for predicted
words, key-value pairs, selection marks, regions, and signatures. Currently, not all
document fields return a confidence score.
Field confidence indicates an estimated probability between 0 and 1 that the prediction
is correct. For example, a confidence value of 0.95 (95%) indicates that the prediction is
likely correct 19 out of 20 times. For scenarios where accuracy is critical, confidence can
be used to determine whether to automatically accept the prediction or flag it for
human review.
Document Intelligence Studio
Analyzed invoice prebuilt-invoice model


<!---- Page 306 ---------------------------------------------------------------------------------------------------------------------------------->
Analyzing invoice
Analyzed document has doc type prebuilt:invoice with confidence : 1.00
Vendor Name: CONTOSO LTD., confidence: 0.96
Vendor address: 123 456th St New York, NY, 10001, confidence: 0.95
Customer Name: MICROSOFT CORPORATION, confidence: 0.95
Customer Address Recipient: Microsoft Corp, confidence: 0.96
Invoice ID: INV-100, confidence: 0.98
Invoice Date: 2019-11-15, confidence: 0.98
Invoice Total: 110.00, confidence: 0.97
Invoice Items:
Unit Price: 1.000000, confidence: 0.68
Description: Test for 23 fields, confidence: 0.90s
Quantity: 1.000000, confidence: 0.88
Improve confidence scores
After an analysis operation, review the JSON output. Examine the confidence values for
each key/value result under the pageResults node. You should also look at the
confidence score in the readResults node, which corresponds to the text-read
operation. The confidence of the read results doesn't affect the confidence of the
key/value extraction results, so you should check both. Here are some tips:
. If the confidence score for the readResults object is low, improve the quality of
your input documents.
. If the confidence score for the pageResults object is low, ensure that the
documents you're analyzing are of the same type.
· Consider incorporating human review into your workflows.
. Use forms that have different values in each field.
· For custom models, use a larger set of training documents. A larger training set
teaches your model to recognize fields with greater accuracy.
Accuracy scores for custom models


<!---- Page 307 ---------------------------------------------------------------------------------------------------------------------------------->
1 Note
· Custom neural and generative models don't provide accuracy scores during
training.
The output of a build (v3.0 and onward) or train (v2.1) custom model operation
includes the estimated accuracy score. This score represents the model's ability to
accurately predict the labeled value on a visually similar document. Accuracy is
measured within a percentage value range from 0% (low) to 100% (high). It's best to
target a score of 80% or higher. For more sensitive cases, like financial or medical
records, we recommend a score of close to 100%. You can also add a human review
stage to validate for more critical automation workflows.
Document Intelligence Studio
Trained custom model (invoice)
form-recognizer-studio
X
Field Name
Accuracy
Receipt No
95 %
Sold To
95 %
ID #
83.3 %
Live Delivery?
95 %
Online Delivery?
95 %
Video Delivery?
95 %
Interpret accuracy and confidence scores for
custom models
Custom template models generate an estimated accuracy score when trained.
Documents analyzed with a custom model produce a confidence score for extracted

| Field Name | Accuracy |  |
| --- | --- | --- |
| Receipt No | 95 % |  |
| Sold To | 95 % |  |
| ID # | 83.3 % |  |
| Live Delivery? | 95 % |  |
| Online Delivery? | 95 % |  |
| Video Delivery? | 95 % |  |
|  |  |  |


<!---- Page 308 ---------------------------------------------------------------------------------------------------------------------------------->
fields. When interpreting the confidence score from a custom model, you should
consider all the confidence scores returned from the model. Let's start with a list of all
the confidence scores.
. Document type confidence score: The document type confidence is an indicator
of closely the analyzed document resembles documents in the training dataset.
When the document type confidence is low, it's indicative of template or structural
variations in the analyzed document. To improve the document type confidence,
label a document with that specific variation and add it to your training dataset.
Once the model is retrained, it should be better equipped to handle that class of
variations.
· Field level confidence: Each labeled field extracted has an associated confidence
score. This score reflects the model's confidence on the position of the value
extracted. While evaluating confidence scores, you should also look at the
underlying extraction confidence to generate a comprehensive confidence for the
extracted result. Evaluate the OCR results for text extraction or selection marks
depending on the field type to generate a composite confidence score for the
field.
. Word confidence score Each word extracted within the document has an
associated confidence score. The score represents the confidence of the
transcription. The pages array contains an array of words and each word has an
associated span and confidence score. Spans from the custom field extracted
values match the spans of the extracted words.
. Selection mark confidence score: The pages array also contains an array of
selection marks. Each selection mark has a confidence score representing the
confidence of the selection mark and selection state detection. When a labeled
field has a selection mark, the custom field selection combined with the selection
mark confidence is an accurate representation of overall confidence accuracy.
The following table demonstrates how to interpret both the accuracy and confidence
scores to measure your custom model's performance.
" Expand table
Accuracy Confidence
Result
High
High
· The model is performing well with the labeled keys and document
formats. . You have a balanced training dataset.
High
Low
. The analyzed document appears different from the training dataset ..
The model would benefit from retraining with at least five more labeled

| Accuracy | Confidence | Result |
| --- | --- | --- |
| High | High | · The model is performing well with the labeled keys and document formats. . You have a balanced training dataset. |
| High | Low | . The analyzed document appears different from the training dataset .. The model would benefit from retraining with at least five more labeled |


<!---- Page 309 ---------------------------------------------------------------------------------------------------------------------------------->
Accuracy Confidence
Result
documents. . These results could also indicate a format variation
between the training dataset and the analyzed document.
Consider adding a new model.
Low
High
. This result is most unlikely .. For low accuracy scores, add more
labeled data or split visually distinct documents into multiple models.
Low
Low
· Add more labeled data .. Split visually distinct documents into
multiple models.
Ensure high model accuracy for custom models
Variances in the visual structure of your documents affect the accuracy of your model.
Reported accuracy scores can be inconsistent when the analyzed documents differ from
documents used in training. Keep in mind that a document set can look similar when
viewed by humans but appear dissimilar to an AI model. To follow, is a list of the best
practices for training models with the highest accuracy. Following these guidelines
should produce a model with higher accuracy and confidence scores during analysis and
reduce the number of documents flagged for human review.
. Ensure that all variations of a document are included in the training dataset.
Variations include different formats, for example, digital versus scanned PDFs.
. Add at least five samples of each type to the training dataset if you expect the
model to analyze both types of PDF documents.
· Separate visually distinct document types to train different models for custom
template and neural models.
· As a general rule, if you remove all user entered values and the documents look
similar, you need to add more training data to the existing model.
o If the documents are dissimilar, split your training data into different folders and
train a model for each variation. You can then compose the different variations
into a single model.
. Ensure that you don't have any extraneous labels.
· Ensure that signature and region labeling doesn't include the surrounding text.
Table, row, and cell confidence
Here are some common questions that should help with interpreting the table, row, and
cell scores:

| Accuracy Confidence | Result |
| --- | --- |
|  | documents. . These results could also indicate a format variation between the training dataset and the analyzed document. Consider adding a new model. |
| Low High | . This result is most unlikely .. For low accuracy scores, add more labeled data or split visually distinct documents into multiple models. |
| Low Low | · Add more labeled data .. Split visually distinct documents into multiple models. |


<!---- Page 310 ---------------------------------------------------------------------------------------------------------------------------------->
Can cells have high confidence scores while the row has a low
confidence score?
The different levels of table confidence (cell, row, and table) are meant to capture the
correctness of a prediction at that specific level. A correctly predicted cell that belongs
to a row with other possible misses would have high cell confidence, but the row's
confidence should be low. Similarly, a correct row in a table with challenges with other
rows would have high row confidence whereas the table's overall confidence would be
low.
How does merging cells affect confidence scores, given the
change in the number of identified columns?
Regardless of the type of table, the expectation for merged cells is that they should have
lower confidence values. Furthermore, the cell that is missing (because it was merged
with an adjacent cell) should have NULL value with lower confidence as well. How much
lower these values might be depends on the training dataset, the general trend of both
merged and missing cell having lower scores should hold.
What is the confidence score for optional values? Should you
expect a cell with a "NULL" value to have a high confidence score
since the value is absent?
If your training dataset is representative of the optionality of cells, it helps the model
know how often a value tends to appear in the training set, and thus what to expect
during inference. This feature is used when computing the confidence of either a
prediction or of making no prediction at all (NULL). You should expect an empty field
with high confidence for missing values that are mostly empty in the training set too.
Can confidence scores alter if an optional field is absent? Do the
confidence scores reflect this change?
When a value is missing from a row, the cell has a NULL value and confidence assigned.
A high confidence score here should mean that the model prediction (of there not being
a value) is more likely to be correct. In contrast, a low score should signal more
uncertainty from the model (and thus the possibility of an error, like the value being
missed).
What are the expectations for cell and row confidence when
extracting a multi-page table with a row split across pages?


<!---- Page 311 ---------------------------------------------------------------------------------------------------------------------------------->
Expect the cell confidence to be high and row confidence to be potentially lower than
rows that aren't split. The proportion of split rows in the training data set can affect the
confidence score. In general, a split row looks different than the other rows in the table
(thus, the model is less certain that it's correct).
For tables spanning multiple pages, can we assume confidence
scores remain consistent if rows end and start cleanly at page
boundaries?
Since rows look similar in shape and contents, regardless of where they are in the
document (or in which page), their respective confidence scores should be consistent.
What is the best way to utilize the new confidence scores?
. Look at all levels of table confidence starting in a top-to-bottom approach: begin
by checking a table's confidence as a whole, then drill down to the row level and
look at individual rows, finally look at cell-level confidences. Depending on the
type of table, there are a couple of things of note:
· For fixed tables, cell-level confidence already captures quite a bit of information
on the correctness of things. This means that simply going over each cell and
looking at its confidence can be enough to help determine the quality of the
prediction. For dynamic tables, the levels are meant to build on top of each other,
so the top-to-bottom approach is more important.
Next step
Learn more about custom models
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 312 ---------------------------------------------------------------------------------------------------------------------------------->
Analyze document API response
Article · 04/23/2025
In this article, let's examine the different objects returned as part of the AnalyzeDocument
response and how to use the document analysis API response in your applications.
Analyze document request
The Document Intelligence APIs analyze images, PDFs, and other document files to extract and
detect various content, layout, style, and semantic elements. The Analyze operation is an async
API. Submitting a document returns an Operation-Location header that contains the URL to
poll for completion. When an analysis request completes successfully, the response contains
the elements described in the model data extraction.
Response elements
. Content elements are the basic text elements extracted from the document.
· Layout elements group content elements into structural units.
· Style elements describe the font and language of content elements.
· Semantic elements assign meaning to the specified content elements.
All content elements are grouped according to pages, specified by page number (1 -indexed).
They're also sorted by reading order that arranges semantically contiguous elements together,
even if they cross line or column boundaries. When the reading order among paragraphs and
other layout elements is ambiguous, the service generally returns the content in a left-to-right,
top-to-bottom order.
1 Note
Currently, Document Intelligence doesn't support reading order across page boundaries.
Selection marks aren't positioned within the surrounding words.
The top-level content property contains a concatenation of all content elements in reading
order. All elements specify their position in the reader order via spans within this content
string. The content of some elements isn't always contiguous.
Analyze response


<!---- Page 313 ---------------------------------------------------------------------------------------------------------------------------------->
The Analyze response for each API returns different objects. API responses contain elements
from component models where applicable.
0
Expand table
Response
content
Description
API
pages
Words, lines and spans recognized from each page of the
input document.
Read, Layout, General
Document, Prebuilt,
and Custom models
paragraphs
Content recognized as paragraphs.
Read, Layout, General
Document, Prebuilt,
and Custom models
styles
Identified text element properties.
Read, Layout, General
Document, Prebuilt,
and Custom models
languages
Identified language associated with each span of the text
extracted
Read
tables
Tabular content identified and extracted from the document.
Tables relate to tables identified by the pretrained layout
model. Content labeled as tables is extracted as structured
fields in the documents object.
Layout, General
Document, Invoice,
and Custom models
figures
Figures (charts, images) identified and extracted from the
document, providing visual representations that aid in the
understanding of complex information.
The Layout model
sections
Hierarchical document structure identified and extracted from
the document. Section or subsection with the corresponding
elements (paragraph, table, figure) attached to it.
The Layout model
keyValuePairs
Key-value pairs recognized by a pretrained model. The key is
a span of text from the document with the associated value.
General document and
Invoice models
documents
Fields recognized are returned in the fields dictionary within
the list of documents
Prebuilt models,
Custom models.
For more information on the objects returned by each API, see model data extraction.
Element properties
Spans

| Response content | Description | API |
| --- | --- | --- |
| pages | Words, lines and spans recognized from each page of the input document. | Read, Layout, General Document, Prebuilt, and Custom models |
| paragraphs | Content recognized as paragraphs. | Read, Layout, General Document, Prebuilt, and Custom models |
| styles | Identified text element properties. | Read, Layout, General Document, Prebuilt, and Custom models |
| languages | Identified language associated with each span of the text extracted | Read |
| tables | Tabular content identified and extracted from the document. Tables relate to tables identified by the pretrained layout model. Content labeled as tables is extracted as structured fields in the documents object. | Layout, General Document, Invoice, and Custom models |
| figures | Figures (charts, images) identified and extracted from the document, providing visual representations that aid in the understanding of complex information. | The Layout model |
| sections | Hierarchical document structure identified and extracted from the document. Section or subsection with the corresponding elements (paragraph, table, figure) attached to it. | The Layout model |
| keyValuePairs | Key-value pairs recognized by a pretrained model. The key is a span of text from the document with the associated value. | General document and Invoice models |
| documents | Fields recognized are returned in the fields dictionary within the list of documents | Prebuilt models, Custom models. |


<!---- Page 314 ---------------------------------------------------------------------------------------------------------------------------------->
Spans specify the logical position of each element in the overall reading order, with each span
specifying a character offset and length into the top-level content string property. By default,
character offsets and lengths are returned in units of user-perceived characters (also known as
grapheme clusters or text elements). To accommodate different development environments
that use different character units, user can specify the stringIndexIndex query parameter to
return span offsets and lengths in Unicode code points (Python 3) or UTF16 code units (Java,
JavaScript, .NET) as well. For more information, see multilingual/emoji support.
12/6/22
Date (Optional)
Bounding Region
Bounding regions describe the visual position of each element in the file. When elements aren't
visually contiguous or cross pages (tables), the positions of most elements are described via an
array of bounding regions. Each region specifies the page number (1 -indexed) and bounding
polygon. The bounding polygon is described as a sequence of points, clockwise from the left
relative to the natural orientation of the element. For quadrilaterals, plot points are top-left,
top-right, bottom-right, and bottom-left corners. Each point represents its x, y coordinate in
the page unit specified by the unit property. In general, unit of measure for images is pixels
while PDFs use inches.
4
1
1
2
World
1
2
Hello
表格
4
3
4
3
3
2
1 Note
Currently, Document Intelligence only returns 4-vertex quadrilaterals as bounding
polygons. Future versions may return different number of points to describe more
complex shapes, such as curved lines or nonrectangular images. Bounding regions are


<!---- Page 315 ---------------------------------------------------------------------------------------------------------------------------------->
applied only to rendered files, if the file isn't rendered, bounding regions aren't returned.
Currently files of docx/xlsx/pptx/html format aren't rendered.
Content elements
Word
A word is a content element composed of a sequence of characters. With Document
Intelligence, a word is defined as a sequence of adjacent characters, with whitespace separating
words from one another. For languages that don't use space separators between words each
character is returned as a separate word, even if it doesn't represent a semantic word unit.
Microsoft (R) Corp.
Selection marks
A selection mark is a content element that represents a visual glyph indicating the state of a
selection. Checkbox is a common form of selection marks. However, they're also represented
via radio buttons or a boxed cell in a visual form. The state of a selection mark can be selected
or unselected, with different visual representation to indicate the state.
Yes
No
Yes
No
Selected
Unselected
Layout elements
Line
A line is an ordered sequence of consecutive content elements separated by a visual space, or
ones that are immediately adjacent for languages without space delimiters between words.
Content elements in the same horizontal plane (row) but separated by more than a single
visual space are most often split into multiple lines. While this feature sometimes splits
semantically contiguous content into separate lines, it enables the representation of textual


<!---- Page 316 ---------------------------------------------------------------------------------------------------------------------------------->
content split into multiple columns or cells. Lines in vertical writing are detected in the vertical
direction.
To make your document look professionally
produced, Word provides header, footer,
cover page, and text box designs that
complement each other. For example, you
can add a matching cover page, header, and
sidebar.
Paragraph
A paragraph is an ordered sequence of lines that form a logical unit. Typically, the lines share
common alignment and spacing between lines. Paragraphs are often delimited via indentation,
added spacing, or bullets/numbering. Content can only be assigned to a single paragraph.
Select paragraphs can also be associated with a functional role in the document. Currently
supported roles include page header, page footer, page number, title, section heading, and
footnote.
Video provides a powerful way to help you
prove your point. When you click Online
Video, you can paste in the embed code for
the video you want to add. You can also type
a keyword to search online for the video that
best fits your document.
Themes and styles also help keep your
document coordinated. When you click
Design and choose a new Theme, the
pictures, charts, and SmartArt graphics
change to match your new theme. When
you apply styles, your headings change to
match the new theme.
To make your document look professionally
produced, Word provides header, footer,
cover page, and text box designs that
Save time in Word with new buttons that
show up where you need them. To change
4
1
Page
A page is a grouping of content that typically corresponds to one side of a sheet of paper. A
rendered page is characterized via width and height in the specified unit. In general, images
use pixel while PDFs use inch. The angle property describes the overall text angle in degrees for
pages that can be rotated.
1 Note
For spreadsheets like Excel, each sheet is mapped to a page. For presentations, like
PowerPoint, each slide is mapped to a page. For file formats like HTML or Word
documents, which lack a native page concept without rendering, the entire main content
is treated as a single page.
Table


<!---- Page 317 ---------------------------------------------------------------------------------------------------------------------------------->
A table organizes content into a group of cells in a grid layout. The rows and columns can be
visually separated by grid lines, color banding, or greater spacing. The position of a table cell is
specified via its row and column indices. A cell can span across multiple rows and columns.
Based on its position and styling, a cell can be classified as general content, row header,
column header, stub head, or description:
. A row header cell is typically the first cell in a row that describes the other cells in the row.
. A column header cell is typically the first cell in a column that describes the other cells in
a column.
. A row or column can contain multiple header cells to describe hierarchical content.
. A stub head cell is typically the cell in the first row and first column position. It can be
empty or describe the values in the header cells in the same row/column.
· A description cell generally appears at the topmost or bottom area of a table, describing
the overall table content. However, it can sometimes appear in the middle of a table to
break the table into sections. Typically, description cells span across multiple cells in a
single row.
. A table caption specifies content that explains the table. A table can further have an
associated caption and a set of footnotes. Unlike a description cell, a caption typically lies
outside the grid layout. A table footnote annotates content inside the table, often marked
with a footnote symbol often found below the table grid.
Layout tables differ from document fields extracted from tabular data. Layout tables are
extracted from tabular visual content in the document without considering the semantics of
the content. In fact, some layout tables are designed purely for visual layout and don't always
contain structured data. The method to extract structured data from documents with diverse
visual layout, like itemized details of a receipt, generally requires significant post processing.
It's essential to map the row or column headers to structured fields with normalized field
names. Depending on the document type, use prebuilt models or train a custom model to
extract such structured content. The resulting information is exposed as document fields. Such
trained models can also handle tabular data without headers and structured data in nontabular
forms, for example the work experience section of a resume.
1 Note
The bounding regions for figures and tables cover only the core content and exclude
associated caption and footnotes.


<!---- Page 318 ---------------------------------------------------------------------------------------------------------------------------------->
(In millions, except camings per share)
Year Ended June 30,
2001
2009
2019
Net income available for common shareholders (A)
$ 61,271
$ 44,281
$ 39,240
Weighted average outstanding shares of common stock (B)
7,547
7.610
7,573
Dilutive effect of stock-based awards
61
73
80
Common stock and common stock equivalents (C)
7,600
7.683
7,753
Earnings Per Share
Basic (A/B)
S
8.12
$
5.82
S
5.11
Diluted (A/C)
S
8.05
$
5.76
$
5.06
Figures
Figures (charts, images) in documents play a crucial role in complementing and enhancing the
textual content, providing visual representations that aid in the understanding of complex
information. The figures object detected by the Layout model has key properties like
boundingRegions (the spatial locations of the figure on the document pages, including the page
number and the polygon coordinates that outline the figure's boundary), spans (details the
text spans related to the figure, specifying their offsets and lengths within the document's text.
This connection helps in associating the figure with its relevant textual context), elements (the
identifiers for text elements or paragraphs within the document that are related to or describe
the figure) and caption, if any.
When output=figures is specified during the initial Analyze operation, the service generates
cropped images for all detected figures that can be accessed via
/analyzeResults/{resultId}/figures/{figureId}. FigureId is included in each figure object,
following an undocumented convention of {pageNumber} . {figureIndex} where figureIndex
resets to one per page.
JSON
{
"figures": [
{
"id": "{figureId}",
"boundingRegions": [],
"spans": [],
"elements": [
"/paragraphs/15",
. ..
1,
"caption": {
"content": "Here is a figure with some text",
"boundingRegions": [],
"spans": [],
"elements": [
"/paragraphs/15"
]
}
}

| (In millions, except camings per share) |  |  |  |
| --- | --- | --- | --- |
| Year Ended June 30, | 2001 | 2009 | 2019 |
| Net income available for common shareholders (A) | $ 61,271 | $ 44,281 | $ 39,240 |
| Weighted average outstanding shares of common stock (B) | 7,547 | 7.610 | 7,573 |
| Dilutive effect of stock-based awards | 61 | 73 | 80 |
| Common stock and common stock equivalents (C) | 7,600 | 7.683 | 7,753 |
| Earnings Per Share |  |  |  |
| Basic (A/B) | S 8.12 | $ 5.82 | S 5.11 |
| Diluted (A/C) | S 8.05 | $ 5.76 | $ 5.06 |


<!---- Page 319 ---------------------------------------------------------------------------------------------------------------------------------->
]
}
Sections
Hierarchical document structure analysis is pivotal in organizing, comprehending, and
processing extensive documents. This approach is vital for semantically segmenting long
documents to boost comprehension, facilitate navigation, and improve information retrieval.
The advent of retrieval-augmented generation (RAG) in document generative AI underscores
the significance of hierarchical document structure analysis. The Layout model supports
sections and subsections in the output, which identifies the relationship of sections and object
within each section. The hierarchical structure is maintained in elements of each section.
JSON
{
"sections": [
{
"spans": [],
"elements": [
"/paragraphs/0",
"/sections/1",
"/sections/2",
"/sections/5"
]
},
...
}
Form field (key value pair)
A form field consists of a field label (key) and value. The field label is generally a descriptive
text string describing the meaning of the field. It often appears to the left of the value, though
it can also appear over or under the value. The field value contains the content value of a
specific field instance. The value can consist of words, selection marks, and other content
elements. It can also be empty for unfilled form fields. A special type of form field has a
selection mark value with the field label to its right. Document field is a similar but distinct
concept from general form fields. The field label (key) in a general form field must appear in
the document. Thus, it can't generally capture information like the merchant name in a receipt.
Document fields are labeled and don't extract a key. Document fields only map an extracted
value to a labeled key. For more information, see document fields.


<!---- Page 320 ---------------------------------------------------------------------------------------------------------------------------------->
7. PROPOSED START DATE
2/1/2022
Style elements
Style
A style element describes the font style to apply to text content. The content is specified via
spans into the global content property. Currently, the only detected font style is whether the
text is handwritten. As other styles are added, text can be described via multiple nonconflicting
style objects. For compactness, all text sharing the particular font style (with the same
confidence) are described via a single style object.
12/6/22
Date (Optional)
JSON
{
"confidence": 1,
"spans": [
{
"offset": 2402,
"length": 7
}
1,
"isHandwritten": true
}
Language
A language element describes the detected language for content specified via spans into the
global content property. The detected language is specified via a BCP-47 language tag to
indicate the primary language and optional script and region information. For example, English
and traditional Chinese are recognized as "en" and zh-Hant, respectively. Regional spelling
differences for UK English can lead to text being detected as en-GB. Language elements don't
cover text without a dominant language (ex. numbers).
Semantic elements


<!---- Page 321 ---------------------------------------------------------------------------------------------------------------------------------->
1 Note
The mentioned semantic elements apply to Document Intelligence prebuilt models. Your
custom models may return different data representations. For example, date and time
returned by a custom model may be represented in a pattern that differs from standard
ISO 8601 formatting.
Document
A document is a semantically complete unit. A file can contain multiple documents, such as
multiple tax forms within a PDF file, or multiple receipts within a single page. However, the
ordering of documents within the file doesn't fundamentally affect the information it conveys.
1 Note
Currently, Document Intelligence doesn't support multiple documents on a single page.
The document type describes documents sharing a common set of semantic fields, represented
by a structured schema, independent of its visual template or layout. For example, all
documents of type "receipt" can contain the merchant name, transaction date, and transaction
total, although restaurant and hotel receipts often differ in appearance.
A document element includes the list of recognized fields from among the fields specified by
the semantic schema of the detected document type:
. A document field can be extracted or inferred. Extracted fields are represented via the
extracted content and optionally its normalized value, if interpretable.
. An inferred field doesn't have content property and is represented only via its value.
. An array field doesn't include a content property. The content can be concatenated from
the content of the array elements.
. An object field does contain a content property that specifies the full content
representing the object that can be a superset of the extracted subfields.
The semantic schema of a document type is described via the fields it contains. Each field
schema is specified via its canonical name and value type. Field value types include basic (ex.
string), compound (ex. address), and structured (ex. array, object) types. The field value type
also specifies the semantic normalization performed to convert detected content into a
normalization representation. Normalization can be locale dependent.


<!---- Page 322 ---------------------------------------------------------------------------------------------------------------------------------->
Basic types
[ Expand table
Field value
type
Description
Normalized representation
Example (Field content ->
Value)
string
Plain text
Same as content
MerchantName: "Contoso" -
"Contoso"
date
Date
ISO 8601 - YYYY-MM-DD
InvoiceDate: "5/7/2022" ->
"2022-05-07"
time
Time
ISO 8601 - hh:mm:ss
TransactionTime: "9:45 PM" -
"21:45:00"
phoneNumber
Phone number
E.164 - +{CountryCode}
{SubscriberNumber}
WorkPhone: "(800) 555-7676" ->
"+18005557676"
countryRegion
Country/Region
ISO 3166-1 alpha-3
CountryRegion: "United States"
- "USA"
selectionMark
Is selected
"signed" or "unsigned"
AcceptEula:
- "selected"
signature
Is signed
Same as content
LendeeSignature: {signature} -
"signed"
number
Floating point
number
Floating point number
Quantity: "1.20" - 1.2
integer
Integer number
64-bit signed number
Count: "123" - 123
boolean
Boolean value
true/false
IsStatutoryEmployee:
- true
Compound types
· Currency: Currency amount with optional currency unit. A value, for example:
InvoiceTotal: $123.45
JSON
{
"amount": 123.45,
"currencySymbol": "$"
}
. Address: Parsed address. For example: ShipToAddress: 123 Main St., Redmond, WA 98052

| Field value type | Description | Normalized representation | Example (Field content -> Value) |
| --- | --- | --- | --- |
| string | Plain text | Same as content | MerchantName: "Contoso" - "Contoso" |
| date | Date | ISO 8601 - YYYY-MM-DD | InvoiceDate: "5/7/2022" -> "2022-05-07" |
| time | Time | ISO 8601 - hh:mm:ss | TransactionTime: "9:45 PM" - "21:45:00" |
| phoneNumber | Phone number | E.164 - +{CountryCode} {SubscriberNumber} | WorkPhone: "(800) 555-7676" -> "+18005557676" |
| countryRegion | Country/Region | ISO 3166-1 alpha-3 | CountryRegion: "United States" - "USA" |
| selectionMark | Is selected | "signed" or "unsigned" | AcceptEula: :selected: - "selected" |
| signature | Is signed | Same as content | LendeeSignature: {signature} - "signed" |
| number | Floating point number | Floating point number | Quantity: "1.20" - 1.2 |
| integer | Integer number | 64-bit signed number | Count: "123" - 123 |
| boolean | Boolean value | true/false | IsStatutoryEmployee: :selected: - true |


<!---- Page 323 ---------------------------------------------------------------------------------------------------------------------------------->
JSON
{
"poBox": "PO Box 12",
"houseNumber": "123",
"streetName": "Main St.",
"city": "Redmond",
"state": "WA",
"postalCode": "98052",
"countryRegion": "USA",
"streetAddress": "123 Main St."
}
Structured types
· Array: List of fields of the same type
JSON
"Items": {
"type": "array",
"valueArray": [
]
}
· Object: Named list of subfields of potentially different types
JSON
"InvoiceTotal": {
"type": "currency",
"valueCurrency": {
"currencySymbol": "$",
"amount": 110
},
"content": "$110.00",
"boundingRegions": [
{
"pageNumber": 1,
"polygon": [
7.3842,
7.465,
7.9181,
7.465,
7.9181,
7.6089,
7.3842,
7.6089
]


<!---- Page 324 ---------------------------------------------------------------------------------------------------------------------------------->
}
]
],
"confidence": 0.945,
"spans": [
{
"offset": 806,
"length": 7
}
}
Next steps
. Try processing your own forms and documents with Document Intelligence Studio [7.
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.


<!---- Page 325 ---------------------------------------------------------------------------------------------------------------------------------->
Understanding Document Intelligence
Layout API Markdown Output Format
Article · 05/05/2025
Azure AI Document Intelligence Layout API can transform your documents into rich Markdown,
preserving their original structure and formatting. Just specify outputContentFormat=markdown in
your request to receive semantically structured content that maintains paragraphs, headings,
tables, and other document elements in their proper hierarchy.
This Markdown output elegantly captures the document's original organization while providing
standardized, easily consumable content for downstream applications. The preserved semantic
structure enables more sophisticated document processing workflows without losing the
context and relationships between document elements.
Markdown elements supported in Layout Analysis
The following Markdown elements are included in Layout API responses:
· Paragraph
· Heading
· Table
· Figure
· Selection Mark
· Formula
· Barcode
· PageNumber/PageHeader/PageFooter
· PageBreak
· KeyValuePairs/Language/Style
· Spans and Content
Paragraph
Paragraphs represent cohesive blocks of text that belong together semantically. The Layout API
maintains paragraph integrity by:
· Preserving paragraph boundaries with empty lines between separate paragraphs
. Using line breaks within paragraphs to maintain the visual structure of the original
document
· Maintaining proper text flow that respects the original document's reading order
Here's an example:


<!---- Page 326 ---------------------------------------------------------------------------------------------------------------------------------->
Markdown
This is paragraph 1.
This is still paragraph 1, even if in another Markdown line.
This is paragraph 2. There is a blank line between paragraph 1 and paragraph 2.
Heading
Headings organize document content into a hierarchical structure to make navigation and
understanding easier. The Layout API has the following capabilities:
· Uses standard Markdown heading syntax with 1-6 hash symbols (#) corresponding to
heading levels.
· Maintains proper spacing with two blank lines before each heading for improved
readability.
Here's an example:
Markdown
# This is a title
## This is heading 1
### This is heading 2
#### This is heading 3
Table
Tables preserve complex structured data in a visually organized format. The Layout API uses
HTML table syntax for maximum fidelity and compatibility:
· Implements full HTML table markup (<table>, <tr>, <th>, <td>) rather than standard
Markdown tables
· Preserves merged cell with HTML rowspan and colspan attributes.
· Preserves table captions with the <caption> tag to maintain document context
· Handles complex table structures including headers, cells, and footers
· Maintains proper spacing with two blank lines before each table for improved readability
· Preserves table footnotes as separate paragraph following the table


<!---- Page 327 ---------------------------------------------------------------------------------------------------------------------------------->
Here's an example:
Markdown
<table>
<caption>Table 1. This is a demo table</caption>
<tr><th>Header</th><th>Header</th></tr>
<tr><td>Cell</td><td>Cell</td></tr>
<tr><td>Cell</td><td>Cell</td></tr>
<tr><td>Cell</td><td>Cell</td></tr>
<tr><td>Footer</td><td>Footer</td></tr>
</table>
This is the footnote of the table.
Figure
The Layout API preserves figure elements:
· Encapsulates figure content in <figure> tags to maintain semantic distinction from
surrounding text
· Preserves figure captions with the <figcaption> tag to provide important context
· Preserves figure footnotes as separate paragraphs following the figure container
Here's an example:
Markdown
‹figure>
<figcaption>Figure 2 This is a figure</figcaption>
Values
300
200
100
0
Jan Feb Mar Apr May Jun Months
</figure>
This is footnote if the figure have.
Selection Mark


<!---- Page 328 ---------------------------------------------------------------------------------------------------------------------------------->
Selection marks represent checkbox-like elements in forms and documents. The Layout API:
. Uses Unicode characters for visual clarity: X] (checked) and [ (unchecked)
· Filters out low-confidence checkbox detections (below 0.1 confidence) to improve
reliability
· Maintains the semantic relationship between selection marks and their associated text
Formula
Mathematical formulas are preserved with LaTeX-compatible syntax that allows for rendering
of complex mathematical expressions:
. Inline formulas are enclosed in single dollar signs ( $ ... $) to maintain text flow
. Block formulas use double dollar signs ( $ .. $) for standalone display
· Multi-line formulas are represented as consecutive block formulas, preserving
mathematical relationships
· Original spacing and formatting are maintained to ensure accurate representation
Here's an example of inline formula, single-line formula block and multiple-lines formula block:
Markdown
The mass-energy equivalence formula $E = m c ^ { 2 }$ is an example of an inline
formula
$$\frac {n ! } {k ! \left( n - k \right) ! } = \binom { n } { k }$$
$ \ frac { p _ { j } } { p _ { 1 } } = \prod _ { k = 1 } ^ { j - 1 } e ^ { - \beta
_ {k , k + 1 } \Delta E _ {k, k +1 } }$$
$= \exp \ left [ - \sum _ { k = 1 } ^ { j - 1 } \beta _ { k , k + 1 } \Delta E _ {
k , k + 1 } \right] .$$
Barcode
Barcodes and QR codes are represented using Markdown image syntax with added semantic
information:
· Uses standard image Markdown syntax with descriptive attributes
· Captures both the barcode type (QR code, barcode, etc.) and its encoded value
· Preserves the semantic relationship between barcodes and surrounding content
Here's an example:


<!---- Page 329 ---------------------------------------------------------------------------------------------------------------------------------->
! [QRCode] (barcodes/1.1 "https://www.microsoft.com")
! [UPCA] (barcodes/1.2 "012345678905")
! [barcode type] (barcodes/pagenumber. barcodenumber "barcode value/content")
PageNumber/PageHeader/PageFooter
Page metadata elements provide context about document pagination but aren't meant to be
displayed inline with the main content:
. Enclosed in HTML comments to preserve the information while keeping it hidden from
standard Markdown rendering
· Maintains original page structure information that might be valuable for document
reconstruction
· Enables applications to understand document pagination without disrupting the content
flow
Here's an example:
Markdown
<! -- PageHeader="This is page header" -- >
<! -- PageFooter="This is page footer" -- >
<! -- PageNumber="1" -- >
PageBreak
To easily figure out which parts belong to which page base on the pure Markdown content, we
introduced PageBreak as the delimiter of the pages
Here's an example:
Markdown
<! -- PageBreak -- >


<!---- Page 330 ---------------------------------------------------------------------------------------------------------------------------------->
KeyValuePairs/Language/Style
For KeyValuePairs/Language/Style, we map them to Analytics JSON body and not in the
Markdown content.
1 Note
For more information on Markdown that is currently supported for user content on
GitHub.com, see GitHub Flavored Markdown Spec.
Conclusion
Document Intelligence's Markdown elements provide a powerful way to represent the structure
and content of analyzed documents. By understanding and properly utilizing these Markdown
elements, you can enhance your document processing workflows and build more sophisticated
content extraction applications.
Next steps
. Try processing your documents with Document Intelligence Studio [.
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.


<!---- Page 331 ---------------------------------------------------------------------------------------------------------------------------------->
Retrieval-Augmented Generation with
Azure AI Document Intelligence
Article · 02/27/2025
This content applies to:
v4.0 (GA)
Introduction
Retrieval-Augmented Generation (RAG) is a design pattern that combines a pretrained
Large Language Model (LLM) like ChatGPT with an external data retrieval system to
generate an enhanced response incorporating new data outside of the original training
data. Adding an information retrieval system to your applications enables you to chat
with your documents, generate captivating content, and access the power of Azure
OpenAI models for your data. You also have more control over the data used by the LLM
as it formulates a response.
The Document Intelligence Layout model is an advanced machine-learning based
document analysis API. The Layout model offers a comprehensive solution for advanced
content extraction and document structure analysis capabilities. With the Layout model,
you can easily extract text and structural elements to divide large bodies of text into
smaller, meaningful chunks based on semantic content rather than arbitrary splits. The
extracted information can be conveniently outputted to Markdown format, enabling you
to define your semantic chunking strategy based on provided building blocks.
Data Sources
Azure Al Search
P
Semantic
chunk
Extract
Index
PDF
Query ->Knowledge
Azure Al Document
Intelligence
DOCX
PPTX
App UX
App Server,
Orchestrator
Prompt + Knowledge ->
Response
XLSX
HTML
</>
MD
TXT
Azure OpenAI
(GPT/ChatGPT)
Semantic chunking


<!---- Page 332 ---------------------------------------------------------------------------------------------------------------------------------->
Long sentences are challenging for natural language processing (NLP) applications.
Especially when they're composed of multiple clauses, complex noun or verb phrases,
relative clauses, and parenthetical groupings. Just like the human beholder, an NLP
system also needs to successfully keep track of all the presented dependencies. The goal
of semantic chunking is to find semantically coherent fragments of a sentence
representation. These fragments can then be processed independently and recombined
as semantic representations without loss of information, interpretation, or semantic
relevance. The inherent meaning of the text is used as a guide for the chunking process.
Text data chunking strategies play a key role in optimizing the RAG response and
performance. Fixed-sized and semantic are two distinct chunking methods:
· Fixed-sized chunking. Most chunking strategies used in RAG today are based on
fix-sized text segments known as chunks. Fixed-sized chunking is quick, easy, and
effective with text that doesn't have a strong semantic structure such as logs and
data. However it isn't recommended for text that requires semantic understanding
and precise context. The fixed-size nature of the window can result in severing
words, sentences, or paragraphs impeding comprehension and disrupting the flow
of information and understanding.
· Semantic chunking. This method divides the text into chunks based on semantic
understanding. Division boundaries are focused on sentence subject and use
significant computational algorithmically complex resources. However, it has the
distinct advantage of maintaining semantic consistency within each chunk. It's
useful for text summarization, sentiment analysis, and document classification
tasks.
Semantic chunking with Document Intelligence
Layout model
Markdown is a structured and formatted markup language and a popular input for
enabling semantic chunking in RAG (Retrieval-Augmented Generation). You can use the
Markdown content from the Layout model to split documents based on paragraph
boundaries, create specific chunks for tables, and fine-tune your chunking strategy to
improve the quality of the generated responses.
Benefits of using the Layout model
· Simplified processing. You can parse different document types, such as digital and
scanned PDFs, images, office files (docx, xlsx, pptx), and HTML, with just a single
API call.


<!---- Page 333 ---------------------------------------------------------------------------------------------------------------------------------->
· Scalability and Al quality. The Layout model is highly scalable in Optical Character
Recognition (OCR), table extraction, and document structure analysis. It supports
309 printed and 12 handwritten languages, further ensuring high-quality results
driven by AI capabilities.
· Large language model (LLM) compatibility. The Layout model Markdown
formatted output is LLM friendly and facilitates seamless integration into your
workflows. You can turn any table in a document into Markdown format and avoid
extensive effort parsing the documents for greater LLM understanding.
Text image processed with Document Intelligence Studio and output to MarkDown
using Layout model
Title
NEWS TODAY
Content
Result
Code
Page header
Latest news and bulletin updates
Markdown
Text
Selection marks
Tables
₡
Section heading
The scoop of the day
The latest updates
<!-- PageHeader="Tuesday, Sep 20, YYYY" -- >
Paragraph
Video provides a pommiful way to help you
prove your point. When you check Omine
Vidro, you can paule in the embed code for
the Thing you want to add. You can also
mpe a keyword to search online for the
Laideo that best fits your document.
NEWS TODAY
To make your document look
profesionaly produced, Word provide
<!- PageHeader="Latest news and bulletin updates" -- > <!-
PageHeader="Issue \#10" -- >
Mirjam Nilsson
magie, you can add a matching cover
page, header, and sidebar.
Click Insert and then choose the element
[you want from the defreest gadlerini.
The scoop of the day The
latest updates
Themes and styles also help keep your
Mirjam Nilsson
you apply sayles, your lunadings change
mah the new theese
The scoop of the day
The latest updates to get you through the day
Same time in Word with new buttons that
click it and a butos for layout options
appears mest to it
[Video provides a powerful way to help you
prove your point. When you click Online
15den tott can paste in the embed code for
the tides you want to add. You can sho type
a keyword to search online for the video du
best fns sour document
Themes and myles also help keep your
Video provides a powerful way to help you prove your point.
When you click Online Video, you can paste in the embed code
for the video you want to add. You can also type a keyword to
search online for the video that best fits your document.
Design and choose a nei Theme, the
V Sen you work on a title, click where you
was to add a row or a column, and then
you apply inim, your headings charen to
match the new thương
To make your document look prodeusionsa
produced, Word provides header, footer,
cover page, and text box designs that
மொழிக்வேளையர் லெஸ் எல்லா, or எலவாடிப்பே, yoவை
can add a matching cover page, header, and
Save time in Word with new buttons that
show up where you need them. To charer
Save time in Word with new buttons the
show up where you need them. To charge
the way a picture fits in your document, các
it and a button for layout options appears
Best to i: When you work on a table, click
where you want to add a row or a column,
and then click the plus sign.
appears nest to it. When you work on a
utile, dick where you want to add a rom
[a column, and then click the plus sign
To make your document look professionally produced, Word
provides header, footer, cover page, and text box designs that
complement each other. For example, you can add a matching
cover page, header, and sidebar.
Reading is easier, 100, in the new Reading
Picture caption
IF you need to stop pracy before you
reach the end, Word semembers when
you left off - munn on another device.
Click Insert and then choose the elements you want from the
different galleries.
The scoop of the day
The latest updates
The scoop of the day
The latest updates
The scoop of the day
The latest updates
Themes and styles also help keep your document coordinated.
When you click Design and choose a new Theme, the pictures,
charts, and SmartArt graphics change to match your new
theme. When you apply styles, your headings change to match
the new theme.
To make your document look
Reading is easier, too, in the new
To make your document lock
the document and focus on the test you
-
desgus that complement each oder
Page number
besser, footer, cover page, and test ben
designs that complement each odier.
Per XX
Table image processed with Document Intelligence Studio using Layout model


<!---- Page 334 ---------------------------------------------------------------------------------------------------------------------------------->
NOTE 2- EARNINGS PER SHARE
Basic earnings per share ("EPS') is computed based on the weighted average number of shares of common stock
outstanding during the period. Diluted EPS is computed based on the weighted average number of shares of common stock
plus the effect of dilutive potential common shares outstanding during the period using the treasury stock method. Dilutive
potential common shares include outstanding stock options and stock awards
The components of basic and diluted EPS were as follows
(In millions, except earnings per share)
. Year Ended June 30,
2021
2020
2019
¿Net income available for common shareholders (A)
$ 61,271
$ 44,281
$ 39,240
: Weighted average outstanding shares of common stock (B)
7,547
7,610
7,673
· Dilutive effect of stock-based awards
61
73
80
. Common stock and common stock equivalents (C)
7,608
7,683
7,753
" Earnings Per Share
· Basic (A/B)
"Diluted (A/C)
$
8.12
$
5.82
$
5.11
$
8.05
$
5.76
$
5.06.
Table
X
(In millions, except earnings per share)
Year Ended June 30,
2021
2020
2019
Net income available for common shareholders (A)
$ 61,271
$ 44,281
$ 39,240
Weighted average outstanding shares of common stock (B)
7,547
7,610
7,673
Dilutive effect of stock-based awards
61
73
80
Common stock and common stock equivalents (C)
7,608
7,683
7,753
Earnings Per Share
Basic (A/B)
$ 8.12
$ 5.82
$ 5.11
Diluted (A/C)
$ 8.05
$ 5.76
$ 5.06
Get started
The Document Intelligence Layout model 2024-11-30 (GA) supports the following
development options:
. Document Intelligence Studio 2 .
· REST API.
· . NET . Java . JavaScript . Python programming language client libraries (SDKs).
Ready to begin?
Document Intelligence Studio
You can follow the Document Intelligence Studio quickstart to get started. Next, you can
integrate Document Intelligence features with your own application using the sample

| (In millions, except earnings per share) |  |  |  |
| --- | --- | --- | --- |
| . Year Ended June 30, | 2021 | 2020 | 2019 |
| ¿Net income available for common shareholders (A) | $ 61,271 | $ 44,281 | $ 39,240 |
| : Weighted average outstanding shares of common stock (B) | 7,547 | 7,610 | 7,673 |
| · Dilutive effect of stock-based awards | 61 | 73 | 80 |
| . Common stock and common stock equivalents (C) | 7,608 | 7,683 | 7,753 |
| " Earnings Per Share |  |  |  |
| · Basic (A/B) "Diluted (A/C) | $ 8.12 | $ 5.82 | $ 5.11 |
|  | $ 8.05 | $ 5.76 | $ 5.06. |

| (In millions, except earnings per share) |  |  |  |
| --- | --- | --- | --- |
| Year Ended June 30, | 2021 | 2020 | 2019 |
| Net income available for common shareholders (A) | $ 61,271 | $ 44,281 | $ 39,240 |
| Weighted average outstanding shares of common stock (B) | 7,547 | 7,610 | 7,673 |
| Dilutive effect of stock-based awards | 61 | 73 | 80 |
| Common stock and common stock equivalents (C) :unselected: | 7,608 | 7,683 | 7,753 |
| Earnings Per Share |  |  |  |
| Basic (A/B) | $ 8.12 | $ 5.82 | $ 5.11 |
| Diluted (A/C) | $ 8.05 | $ 5.76 | $ 5.06 |


<!---- Page 335 ---------------------------------------------------------------------------------------------------------------------------------->
code provided.
· Start with the Layout model . You need to select the following Analyze options to
use RAG in the studio:
** Required **
o Run analysis range - Current document.
o Page range - All pages.
o Output format style - Markdown.
** Optional **
o You can also select relevant optional detection parameters.
· Select Save.
Analyze options
X
Configure basic settings and additional options for analyzing documents. Settings and
options will be saved within this session, but can be changed at any time.
Run analysis range
Current document
All documents
Page range
All pages
Range
Output format style
Text
Markdown
Optional detection
Barcodes
Language
Key-value pairs
Premium detection (Charged: See pricing)
High resolution
Style font
Formulas
Save
Cancel
. Select the Run analysis button to view the output.
out Run analysis
Query fields
Analyze options
SDK or REST API


<!---- Page 336 ---------------------------------------------------------------------------------------------------------------------------------->
. You can follow the Document Intelligence quickstart for your preferred
programming language SDK or REST API. Use the Layout model to extract content
and structure from your documents.
. You can also check out GitHub repos for code samples and tips for analyzing a
document in markdown output format.
o Python
o JavaScript
o Java zz
o .NET 2
Build document chat with semantic chunking
· Azure OpenAI on your data enables you to run supported chat on your documents.
Azure OpenAI on your data applies the Document Intelligence Layout model to
extract and parse document data by chunking long text based on tables and
paragraphs. You can also customize your chunking strategy using Azure OpenAI
sample scripts located in our GitHub repo.
. Azure Al Document Intelligence is now integrated with LangChain as one of its
document loaders. You can use it to easily load the data and output to Markdown
format. For more information, see our sample code [ that shows a simple demo
for RAG pattern with Azure AI Document Intelligence as document loader and
Azure Search as retriever in LangChain.
. The chat with your data solution accelerator code sample ‹ demonstrates an end-
to-end baseline RAG pattern sample. It uses Azure AI Search as a retriever and
Azure AI Document Intelligence for document loading and semantic chunking.
Use case
If you're looking for a specific section in a document, you can use semantic chunking to
divide the document into smaller chunks based on the section headers helping you to
find the section you're looking for quickly and easily:
Python
# pip install azure-ai-documentintelligence == 1.0.0b1
# pip install langchain langchain-community azure-ai-documentintelligence


<!---- Page 337 ---------------------------------------------------------------------------------------------------------------------------------->
from azure.ai.documentintelligence import DocumentIntelligenceClient
endpoint = "https://<my-custom-subdomain>.cognitiveservices.azure.com/"
key = "<api_key>"
from langchain_community . document_loaders import
AzureAIDocumentIntelligenceLoader
from langchain. text_splitter import MarkdownHeaderTextSplitter
# Initiate Azure AI Document Intelligence to load the document. You can
either specify file_path or url_path to load the document.
loader = AzureAIDocumentIntelligenceLoader(file_path="<path to your file>",
api_key = key, api_endpoint = endpoint, api_model="prebuilt-layout")
docs = loader.load()
# Split the document into chunks base on markdown headers.
headers_to_split_on = [
("#", "Header 1"),
("##", "Header 2"),
("###", "Header 3"),
]
text_splitter =
MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
docs_string = docs [0].page_content
splits = text_splitter. split_text(docs_string)
splits
View samples on GitHub.
Next steps
· Learn more about Azure Al Document Intelligence.
. Learn how to process your own forms and documents with the Document
Intelligence Studio 7 .
· Complete a Document Intelligence quickstart and get started creating a document
processing app in the development language of your choice.
Feedback
Was this page helpful?
Yes
P No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 338 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence batch analysis
Article · 02/26/2025
The batch analysis API allows you to bulk process up to 10,000 documents using one request.
Instead of analyzing documents one by one and keeping track of their respective request IDs,
you can simultaneously analyze a collection of documents like invoices, loan papers, or custom
documents. The input documents must be stored in an Azure blob storage container. Once the
documents are processed, the API writes the results to a specified storage container.
Batch analysis limits
· The maximum number of document files that can be in a single batch request is 10,000.
· Batch operation results are retained for 24 hours after completion. The batch operation
status is no longer available 24 hours after batch processing is completed. The input
documents and respective result files remain in the storage containers provided.
Prerequisites
· An active Azure subscription. If you don't have an Azure subscription, you can create one
for free z .
. A Document Intelligence Azure Resource : once you have your Azure subscription,
create a Document Intelligence resource in the Azure portal. You can use the free pricing
tier (F0) to try the service. After your resource is deployed, select "Go to resource" to
retrieve your key and endpoint. You need the resource key and endpoint to connect your
application to the Document Intelligence service. You can also find these values on the
Keys and Endpoint page in Azure portal.
. An Azure Blob Storage account . Create two containers in your Azure Blob Storage
account for your source and result files:
o Source container: This container is where you upload document files for analysis.
o Result container: This container is where results from the batch analysis API are stored.
Storage container authorization
To allow the API to process documents and write results in your Azure storage containers, you
must authorize using one of the following two options:
V Managed Identity. A managed identity is a service principal that creates a Microsoft Entra
identity and specific permissions for an Azure managed resource. Managed identities enable
you to run your Document Intelligence application without having to embed credentials in


<!---- Page 339 ---------------------------------------------------------------------------------------------------------------------------------->
your code, a safer way to grant access to storage data without including access signature
tokens (SAS) in your code.
Review Managed identities for Document Intelligence to learn how to enable a managed
identity for your resource and grant it access to your storage container.
1 Important
When using managed identities, don't include a SAS token URL with your HTTP requests.
Using managed identities replaces the requirement for you to include shared access
signature tokens (SAS).
V Shared Access Signature (SAS). A shared access signature is a URL that grants restricted
access to your storage container. To use this method, create Shared Access Signature (SAS)
tokens for your source and result containers. Go to the storage container in Azure portal and
select "Shared access tokens" to generate SAS token and URL.
· Your source container or blob must designate read, write, list, and delete permissions.
· Your result container or blob must designate write, list, delete permissions.


<!---- Page 340 ---------------------------------------------------------------------------------------------------------------------------------->
Generate SAS
X
A shared access signature (SAS) is a URI that grants restricted access to an Azure Storage
container. Use it when you want to grant access to storage account resources for a specific
time range without sharing your storage account key. Learn more
Signing method
Account key
User delegation key
Permissions *
4 selected
V
Read
0
Add
Create
ind
2:19:14 PM
US & Canada)
V
Write
Delete
List
And
10:19:14 PM
(UTC-08:00) Pacific Time (US & Canada)
V
Allowed IP addresses
for example, 168.1.5.65 or 168.1.5.65-168.1 ...
Allowed protocols
HTTPS only
HTTPS and HTTP
Generate SAS token and URL
Review Create SAS tokens to learn more about generating SAS tokens and how they work.
Calling the batch analysis API
1. Specify the input files
The batch API supports two options for specifying the files to be processed.
. If you want to process all the files in a container or a folder, and the number of files is less
than the 10000 limit, use the azureBlobSource object in your request.
Bash
POST /documentModels/{modelId} : analyzeBatch
{
"azureBlobSource": {
"containerUrl":
"https: //myStorageAccount . blob. core.windows . net/myContainer?mySasToken",


<!---- Page 341 ---------------------------------------------------------------------------------------------------------------------------------->
},
. . .
}
. If you don't want to process all the files in a container or folder, but rather specific files in
that container or folder, use the azureBlobFileListSource object. This operation requires
a File List JSONL file which lists the files to be processed. Store the JSONL file in the root
folder of the container. Here's an example JSONL file with two files listed:
JSON
{"file": "Adatum Corporation. pdf"}
{"file": "Best For You Organics Company. pdf" }
Use a file list JSONL file with the following conditions:
· When you need to process specific files instead of all files in a container;
· When the total number of files in the input container or folder exceeds the 10,000 file
batch processing limit;
· When you want more control over which files get processed in each batch request;
Bash
POST /documentModels/{modelId}: analyzeBatch
{
"azureBlobFileListSource": {
"containerUrl": "https://myStorageAccount. blob.core.windows. net/myContainer?
mySasToken",
"fileList": "myFileList. jsonl"
...
},
...
}
A container URL or a container SAS URL is required in both options. Use container URL if using
managed Identity to access your storage container. If you're using Shared Access Signature
(SAS), use a SAS URL.
2. Specify the results location


<!---- Page 342 ---------------------------------------------------------------------------------------------------------------------------------->
· Specify the Azure Blob Storage container URL (or container SAS URL) for where you want
your results to be stored using resultContainerURL parameter. We recommend using
separate containers for source and results to prevent accidental overwriting.
· Set the overwriteExisting Boolean property to False and prevent overwriting any
existing results for the same document. If you'd like to overwrite any existing results, set
the Boolean to True. You're still billed for processing the document even if any existing
results aren't overwritten.
· Use resultPrefix to group and store results in a specific container folder.
3. Build and run the POST request
Remember to replace the following sample container URL values with real values from your
Azure Blob storage containers.
This example shows a POST request with azureBlobSource input
Bash
POST /documentModels/{modelId}:analyzeBatch
{
"azureBlobSource": {
"containerUrl": "https://myStorageAccount. blob.core.windows. net/myContainer?
mySasToken",
"prefix": "inputDocs/"
},
"resultContainerUrl":
"https://myStorageAccount. blob. core.windows. net/myOutputContainer?mySasToken",
"resultPrefix": "batchResults/",
"overwriteExisting": true
}
This example shows a POST request with azureBlobFileListSource and a file list input
Bash
POST /documentModels/{modelId} :analyzeBatch
{
"azureBlobFileListSource": {
"containerUrl": "https://myStorageAccount. blob. core. windows. net/myContainer?
mySasToken",
"fileList": "myFileList. jsonl"
},
"resultContainerUrl":


<!---- Page 343 ---------------------------------------------------------------------------------------------------------------------------------->
"https: //myStorageAccount . blob. core. windows . net/myOutputContainer?mySasToken",
"resultPrefix": "batchResults/",
"overwriteExisting": true
}
Here's an example successful response
Bash
202 Accepted
Operation-Location: /documentModels/{modelId}/analyzeBatchResults/{resultId}
4. Retrieve API results
Use the GET operation to retrieve batch analysis results after the POST operation is executed.
The GET operation fetches status information, batch completion percentage, and operation
creation and update date/time. This information is only retained for 24 hours after the batch
analysis is completed.
Bash
GET /documentModels/{modelId}/analyzeBatchResults/{resultId}
200 OK
{
"status": "running",
// notStarted, running, completed, failed
"percentCompleted": 67,
// Estimated based on the number of processed
documents
"createdDateTime": "2021-09-24T13:00:46Z",
"lastUpdatedDateTime": "2021-09-24T13:00:49Z"
. ..
}
5. Interpret status messages
For each document processed, a status is assigned, either succeeded, failed, running,
notStarted, or skipped. A source URL, which is the source blob storage container for the input
document, is provided.
· Status notStarted or running. The batch analysis operation isn't initiated or isn't
completed. Wait until the operation is completed for all documents.
. Status completed. The batch analysis operation is finished.


<!---- Page 344 ---------------------------------------------------------------------------------------------------------------------------------->
· Status succeeded. The batch operation was successful, and input document was
processed. The results are available at resultUrl, which is created by combining
resultContainerUrl, resultPrefix, input filename, and .ocr. json extension. Only files
that have succeeded have the property resultUrl.
Example of a succeeded status response:
Bash
{
"resultId": "myresultId-",
"status": "succeeded",
"percentCompleted": 100,
"createdDateTime": "2025-01-01T00:00:000",
"lastUpdatedDateTime": "2025-01-01T00:00:000",
"result": {
"succeededCount": 10,000,
"failedCount": 0,
"skippedCount": 0,
"details": [
{
"sourceUrl": "https://{your-source-
container}/inputFolder/document1.pdf",
"resultUrl": "https://{your-result-
container}/resultsFolder/document1.pdf. ocr. json",
"status": "succeeded"
},
. ..
{
"sourceUrl": "https://{your-source-
container}/inputFolder/document10000.pdf",
"resultUrl": "https://{your-result-
container}/resultsFolder/document10000.pdf. ocr.json",
"status": "succeeded"
}
]
}
}
. Status failed. This error is only returned if there are errors in the overall batch request.
Once the batch analysis operation starts, the individual document operation status
doesn't affect the status of the overall batch job, even if all the files have the status
failed .
Example of a failed status response:
Bash


<!---- Page 345 ---------------------------------------------------------------------------------------------------------------------------------->
[
"result": {
"succeededCount": 0,
"failedCount": 2,
"skippedCount": 0,
"details": [
"sourceUrl": "https://{your-source-
container}/inputFolder/document1. jpg",
"status": "failed",
"error": {
"code": "InvalidArgument",
"message": "Invalid argument.",
"innererror": {
"code": "InvalidSasToken",
"message": "The shared access signature (SAS) is invalid:
{details}"
}
]
}
}
]
. ..
· Status skipped: Typically, this status happens when output for the document is already
present in the specified output folder and the overwriteExisting Boolean property is set
to false.
Example of skipped status response:
Bash
[
"result": {
"succeededCount": 3,
"failedCount": 0,
"skippedCount": 2,
"details": [
...
"sourceUrl": "https://{your-source-
container}/inputFolder/document1.pdf",
"status": "skipped",
"error": {
"code": "OutputExists",
"message": "Analysis skipped because result file https://{your-
result-container}/resultsFolder/document1.pdf.ocr.json already exists."
}
]
}
]
. . .


<!---- Page 346 ---------------------------------------------------------------------------------------------------------------------------------->
1 Note
Analysis results aren't returned for individual files until analysis for the entire batch is
completed. To track detailed progress beyond percentCompleted, you can monitor
*. ocr. json files as they're written into the resultContainerUrl.
Next steps
View code samples on GitHub.[


<!---- Page 347 ---------------------------------------------------------------------------------------------------------------------------------->
Troubleshooting latency issues in Azure
AI Document Intelligence
Article · 02/06/2025
This article presents troubleshooting tips, remedial solutions, and best practices to
address Document Intelligence latency issues. Latency refers to the duration an API
server takes to handle and process an incoming request before delivering the response
to the client. The time required to analyze a document varies based on its size (such as
the number of pages) and the content on each page.
Document Intelligence operates as a multitenant service, ensuring that latency for
similar documents is comparable, though not always identical. Variability in latency and
performance is an inherent characteristic of any microservice-based, stateless,
asynchronous service, especially when processing images and large documents on a
large scale. Despite continuous efforts to increase hardware capacity and enhance
scalability, some latency issues can still arise during runtime.
1 Note
· Azure Al services don't provide a Service Level Agreement (SLA) for latency.
· The Document Intelligence API offers asynchronous functionality, allowing
you to access results up to 24 hours after sending your request to our
backend.
· Use the request ID provided by the POST operation to retrieve these results. If
you encounter issues during your standard polling sequence, save the request
ID and try again later before considering a retry. For further assistance, refer
to our service_page.
Set your latency baseline
To evaluate latency, you should first establish baseline metrics for your specific scenario.
These metrics give you the expected end-to-end and server latency within the context
of your application environment. Once you have these baseline metrics, it becomes
easier to distinguish between expected and unexpected conditions.
Check Azure region status


<!---- Page 348 ---------------------------------------------------------------------------------------------------------------------------------->
When you're experiencing latency issues, the first step is to check Azure status for any
current outages or issues that might impact your services.
. All active events are listed under the Current Impact tab.
. You can also check your resource in the host region. Go to Geography - Products
And Services -> Al + Machine Learning -> Azure Al Document Intelligence and
check the status for your region:
Current Impact
Americas
Europe
Asia Pacific
Middle East and Africa
Azure Government
Azure China
Jio®
Products And Services
*Non-
Regional O
East US
East US 2
Central US
North Central
US
South Central
US
West Central
US
West US
West US 2
"West US 3
Canada East
Canada
Central
Brazil South
Brazil
Southeast
Mexico
Central
+
AI + MACHINE LEARNING
Azure Al Document
Intelligence
-
-
-
Check file size
Monitor the size of files you send via the request API. Processing larger files in parallel
can result in increased processing times. Normalize your metric by measuring latency
per page. If you observe sustained periods (exceeding one hour) where latency per page
consistently surpasses 15 seconds, consider addressing the issue.
Check Azure Blob storage latency
The size of a request affects latency in Azure Storage operations. Larger operations take
more time to complete due to the increased volume of data transferred over the
network and processed by Azure Storage.
Azure Storage provides two latency metrics for block blobs in the Azure portal:
· End-to-end (E2E) latency measures the interval from when Azure Storage receives
the first packet of the request until Azure Storage receives a client
acknowledgment on the last packet of the response.
. Server latency measures the interval from when Azure Storage receives the last
packet of the request until the first packet of the response is returned from Azure
Storage.
To view latency metrics, navigate to your storage resource in the Azure portal:
. On the left navigation window, select Insights from the Monitoring drop-down
menu.

|  | Current Impact |  | Americas | Europe | Asia Pacific | Middle East and Africa |  | Azure Government |  | Azure China | Jio® |  |  |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Products And Services | *Non- Regional O | East US | East US 2 | Central US | North Central US | South Central US | West Central US | West US | West US 2 | "West US 3 | Canada East | Canada Central | Brazil South | Brazil Southeast | Mexico Central + |
| AI + MACHINE LEARNING |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| Azure Al Document Intelligence | - |  |  |  |  |  |  |  |  |  | - |  |  | - |  |


<!---- Page 349 ---------------------------------------------------------------------------------------------------------------------------------->
· The insights tab opens a window that includes a chart showing both E2E and
Server latency metrics:
Partner solutions
Success latency - End-to-end & Server
> Data storage
> Security + networking
300ms
Data management
250ms
> Settings
200ms
Monitoring
Insights
150ms
Alerts
100ms
Metrics
50ms
Workbooks
Diagnostic settings
0ms
12 PM
1 PM
2 PM
UTC-08:00
Logs
Monitoring (classic)
Success E2E Latency (Avg)
51.96ms
Success Server Latency (Avg)
14.71ms
For more information, see Latency in Blob storage.
Check monitoring metrics for your resource
Azure portal monitors offer insights into your applications to enhance their performance
and availability. There are several tools that you can use to monitor your app's
performance in the Azure portal:
1. On the Overview page, select Monitoring, select the time period, and review the
Request latency metrics on page.
Get Started
Monitoring
Show data for the last
15 minutes
1 hour
1 day
1 week
30 days
Requests
Errors
Request latency
140ms
2
2
120ms
1.5
1,5
100ms
80ms
1
1
60ms
0.5
0.5
40ms
20ms
0
0
Oms
3:30 PM
3:45 PM
4 PM
4:15 PM
UTC-06:00
3:30 PM
3:45 PM
4 PM
4:15 PM
UTC-08:00
3:30 PM
3:45 PM
4 PM
4:15 PM
UTC-08:00
Blocked Calls (Sum), Response code: 429 | 0
Total Calls (Sum), | 15
Total Errors (Sum), | 15
Client Errors (Sum), Response code: 4xx |15
Latency (Avg),
40.4ms
2. On the left navigation window, select Metrics from the Monitoring drop-down
menu.
. In the main window, select +Add metric.
. Keep the Scope and Metric Namespace fields unchanged. Add the Latency
parameter to the Metric field and adjust the Aggregation field as needed.


<!---- Page 350 ---------------------------------------------------------------------------------------------------------------------------------->
Avg Latency for
your-resource-name
0
+ Add metric
V
Add filter
Apply splitting
Scope
Metric Namespace
Cognitive Service sta ...
Metric
Aggregation
~
your-resource-name
V
Latency
V
Avg
V
COGNITIVE SERVICES - HTTP REQUESTS
4
90.00ms
# Blocked Calls
Client Errors
80.00ms
Data In
70.00ms
Data Out
# Latency
60.00ms
Ratelimit
50.00ms
Server Errors
¥
<
40.00ms
30.00ms
20.00ms
10.00ms
Set a latency alert in the Azure portal
Alerts assist you in identifying and resolving issues by providing proactive notifications
when Azure Monitor data suggests a potential issue. An alert rule keeps an eye on your
data and notifies you when set criteria are met on your specified resource. You can set
up an alert in the Azure portal as follows:
1. On the left navigation window, select Alerts from the Monitoring drop-down
menu.
2. Select the Create alert rule button.
3. In the new window that opens, select Latency from the Select a signal drop-down
menu.


<!---- Page 351 ---------------------------------------------------------------------------------------------------------------------------------->
Create an alert rule
...
Scope
Condition
Actions
Details
Tags
Review + create
Configure when the alert rule should trigger by selecting a signal and defining its logic.
Signal name'
®
Latency
V
Popular
Total Errors
Alert logic
Total Calls
i
We have set the condition configuration al
needed.
Latency
Threshold type
Server Errors
Client Errors
Aggregation type
Custom log search
Value is @
See all signals
Threshold Sensitivity
O
Iviealum
V
4. Configure the alert by completing the fields on the page.
5. After you complete the configuration, select Review + create
Contact us
If you're unable to resolve long latency issue, email us with the following information:
· Model Name
· Version
· Subscription ID
· Resource ID
· Timestamp and issue description
· Request IDs of the concerning operations (if possible)
· Logs
· Sample files
· JSON file (output/analyze results)
· Training set (if it's a training issue related to custom neural models)
For more assistance, you can also or use the feedback widget at the bottom of any
Microsoft Learn page.


<!---- Page 352 ---------------------------------------------------------------------------------------------------------------------------------->
Feedback
Was this page helpful?
& Yes
P
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 353 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence Studio custom
projects
Article · 03/14/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
Document Intelligence Studio [ is an online tool for visually exploring, understanding,
and integrating features from the Document Intelligence service in your applications.
This quickstart aims to give you a guide of setting up a custom project in Document
Intelligence Studio.
Prerequisites for new users
For details on subscription, resource, and authentication setup, see Get started with
Document Intelligence Studio.
Additional prerequisites for custom projects
In addition to the Azure account and a Document Intelligence or Azure AI services
resource, you need:
Azure Blob Storage container
A standard performance Azure Blob Storage account . You create containers to store
and organize your training documents within your storage account. If you don't know
how to create an Azure storage account with a container, following these quickstarts:
. Create a storage account. When creating your storage account, make sure to
select Standard performance in the Instance details - Performance field.
. Create a container. When creating your container, set the Public access level field
to Container (anonymous read access for containers and blobs) in the New
Container window.
Azure role assignments
For custom projects, the following role assignments are required for different scenarios.
· Basic


<!---- Page 354 ---------------------------------------------------------------------------------------------------------------------------------->
o Cognitive Services User: You need this role for Document Intelligence or Azure
AI services resource to train the custom model or do analysis with trained
models.
o Storage Blob Data Contributor: You need this role for the Storage Account to
create a project and label data.
· Advanced
o Storage Account Contributor: You need this role for the Storage Account to set
up CORS settings (this action is a one-time effort if the same storage account is
reused).
o Contributor: You need this role to create a resource group and resources.
0
Note
If local (key-based) authentication is disabled for your Document Intelligence
service resource and storage account, be sure to obtain Cognitive Services
User and Storage Blob Data Contributor roles respectively, so you have
enough permissions to use Document Intelligence Studio. The Storage
Account Contributor and Contributor roles only allow you to list keys but
don't give you permission to use the resources when key-access is disabled.
Configure CORS
CORS (Cross Origin Resource Sharing) needs to be configured on your Azure storage
account for it to be accessible from the Document Intelligence Studio. To configure
CORS in the Azure portal, you need access to the CORS tab of your storage account.
1. Select the CORS tab for the storage account.
Settings
Configuration
Data Lake Gen2 upgrade
Resource sharing (CORS)
Advisor recommendations
Endpoints
Locks
2. Start by creating a new CORS entry in the Blob service.
3. Set the Allowed origins to https: //documentintelligence.ai.azure.com.


<!---- Page 355 ---------------------------------------------------------------------------------------------------------------------------------->
Allowed origins
Allowed methods
Allowed headers
Exposed headers
Max age
https://documentintelligence.ai.azur ...
8 selected
V
*
*
120
Ô
Blob service
File service
Queue service
Table service
? Tip
You can use the wildcard character '*' rather than a specified domain to allow
all origin domains to make requests via CORS.
4. Select all the available 8 options for Allowed methods.
5. Approve all Allowed headers and Exposed headers by entering an * in each field.
6. Set the Max Age to 120 seconds or any acceptable value.
7. To save the changes, select the save button at the top of the page.
CORS should now be configured to use the storage account from Document Intelligence
Studio.
Sample documents set
1. Sign in to the Azure portal and navigate to Your storage account > Data
storage > Containers.
Data storage
Containers
File shares
Queues
Tables
2. Select a container from the list.
3. Select Upload from the menu at the top of the page.
₸ Upload
Change access level
Refresh
Delete
Change tier
Acquire lease
Break lease
View snapshots
Create snapshot
4. The Upload blob window appears.
5. Select your files to upload.

| Allowed origins | Allowed methods | Allowed headers | Exposed headers | Max age |
| --- | --- | --- | --- | --- |
| https://documentintelligence.ai.azur ... | 8 selected V | * | * | 120 Ô |


<!---- Page 356 ---------------------------------------------------------------------------------------------------------------------------------->
Upload blob
form-rec-invoice-with-labels/
X
Files
"invoice_1.pdf" "invoice_2.pdf" "inv ...
Overwrite if files already exi
invoice_1.pdf
invoice_2.pdf
invoice_3.pdf
Advanced
Upload
1 Note
By default, the Studio uses documents that are located at the root of your
container. However, you can use data organized in folders by specifying the folder
path in the Custom form project creation steps. See Organize your data in
subfolders
Use Document Intelligence Studio features
Auto label documents with prebuilt models or one of
your own models
· In custom extraction model labeling page, you can now auto label your documents
using one of Document Intelligent Service prebuilt models or your trained models.


<!---- Page 357 ---------------------------------------------------------------------------------------------------------------------------------->
Azure Al | Document Intelligence Studio
-
@
®
?
1
Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
Form Recognizer Studio > Custom extraction model > Hello-Custom > Label data
%
Custom extraction model
Label data
Train
Hello-Custom
212 Run layout
V
Auto label
V
0
Draw region
O
0
+ Add a field
G
Label data
Drag & drop file
here or
Browse for files
Current document
Unlabeled documents
All documents
Models
LTD.
INVOICE
& Test
P8TH
invoice_2.pdf
Contoso Headquarters
123 456th St
New York, NY, 10001
INVOICE: INV-100
€
Settings
INVOICE DATE: 11/15/2019
DUE DATE: 12/15/2019
sample.pdf
CUSTOMER NAME: MICROSOFT CORPORATION
SERVICE PERIOD: 10/14/2019 - 11/14/2019
read-ba. de pal
CUSTOMER ID: CID-12345
invoice.pdf
Microsoft Corp
123 Other St,
Redmond WA, 98052
layout.png
BILL TO:
Microsoft Finance
123 Bill St,
Redmond WA, 98052
SHIP TO:
Microsoft Delivery
SERVICE ADDRESS:
Microsoft Services
123 Service St,
Redmond WA, 98052
123 Ship St,
Redmond WA, 98052
THANK YOU FOR YOUR BUSINESS!
REMIT TO:
Contoso Billing
123 Remit St
New York, NY, 10001
< 1 of1 >
Q Q . Q
Privacy & Cookies @ Microsoft 202
ELE
SALESPERSON
P.O. NUMBER
REQUISITIONER
SHIPPED VIA
F.O.B. POINT
TERMS
PO-3333
HE
DATE
ITEM CODE
DESCRIPTION
QTY
UM
PRICE
TAX
AMOUNT
3/4/2021
A123
Consulting Services
2
hours
$30.00
$6.00
$60.00
3/5/2021
B456
Document Fee
3
$10.00
$3.00
$30.00
3/6/2021
C789
Printing Fee
10
pages
$1.00
$1.00
$10.00
SUBTOTAL
$100.00
SALES TAX
$10.00
TOTAL
$110.00
PREVIOUS UNPAID BALANCE
$500.00
AMOUNT DUE
$610.00
· For some documents, duplicate labels after running autolabel are possible. Make
sure to modify the labels so that there are no duplicate labels in the labeling page
afterwards.
SOLD TO:
SHIP TO:
Duplicated labels
X
We found some labels present in multiple fields, which may lead to training failure.
Please review the labels listed below and remove any duplicates.
Close
Labels
Related fields
New Belgium Brewery
CustomerAddressRecipient
CustomerName
Auto label tables
. In custom extraction model labeling page, you can now auto label the tables in the
document without having to label the tables manually.

| ELE | SALESPERSON | P.O. NUMBER | REQUISITIONER | SHIPPED VIA | F.O.B. POINT | TERMS |
| --- | --- | --- | --- | --- | --- | --- |
|  |  | PO-3333 |  |  |  |  |

| HE DATE | ITEM CODE | DESCRIPTION | QTY | UM | PRICE | TAX | AMOUNT |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 3/4/2021 | A123 | Consulting Services | 2 | hours | $30.00 | $6.00 | $60.00 |
| 3/5/2021 | B456 | Document Fee | 3 |  | $10.00 | $3.00 | $30.00 |
| 3/6/2021 | C789 | Printing Fee | 10 | pages | $1.00 | $1.00 | $10.00 |

|  | SUBTOTAL | $100.00 |
| --- | --- | --- |
|  | SALES TAX | $10.00 |
|  | TOTAL | $110.00 |
|  | PREVIOUS UNPAID BALANCE | $500.00 |
|  | AMOUNT DUE | $610.00 |

| Labels | Related fields |
| --- | --- |
| New Belgium Brewery | :selected: CustomerAddressRecipient :selected: CustomerName |


<!---- Page 358 ---------------------------------------------------------------------------------------------------------------------------------->
Azure Al | Document Intelligence Studio
1
@
?
៛
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
×
Form Recognizer Studio > Custom extraction model > Hello-Custom > Label data
Custom extraction model
Label data
Train
Hello-Custom
215 Run layout
V
Auto label
V
7
Draw region
O
pov
+
Add a field
0
Label data
Drag & drop file
here or
Browse for files
Liberty's Delightful Sinful Bakery & Café
765 Halifax St. Clearwater, FL 33756
Models
5
Test
PYTL
Our reference: 3456623
Your reference: 2334566
e
Settings
invoice_2.pdf
Received from:
8556 Indian Summer Ave.
New Haven, CT 06511
Liberty booking contact
40 River Street
East Northport, NY 11731
sample.pdf
read-ba ... de.pdf
Name Summer River
Tel. 34456632
invoice.pdf
E-mail email6.@libertydelightfulsinful.com
layout.png
Booking Confirmation - ORIGINAL
Our reference:
3456623
Your reference:
2334566
05-Dec-2018
Booking date:
Contract No:
334566
BU/NO:
EURH234
Summary:
45x72
Opt. A
Opt. B
Export:
Common
Export empty pick up depot(s)
Rows 3, Columns: 5 $ Lane
Heights, MI 48310
18 Queen Street
52 West Trenton St.
com
TO
By
ETD
cause science slow
09-Dec-2018
ETA
Hoboken, NJ 07030
09-Dec-2020
Harleysville, PA
19438
19:00
11:00
9 Ketch Harbour
75 Fawn Street
Peabody, MA 01960
tone late spoken
Ave
12-Dec-2018
10:00
19-Dec-2020
Vincentown, NJ
<
1
of 1 >
Q Q + Q
Privacy & Cookies @ Microsoft 2022
@
oadline
Location
Date/Time (local)
Required action
able
Harleysville
(PA)
08-Dec-2019
Nobody loves a pig
wearing lipstick.
Flight
Harleysville
(PA)
08-Dec-2019
13:00
Two more days and all his
problems would be solved
Round
Harleysville
(PA)
09-Dec-2019
Accent
Harleysville
(PA)
10-Dec-2019
Monkey
Harleysville
11-Dec-2019
Route
Harleysville
(PA)
11-Dec-2019
Peanuts don't grow on
trees
Add test files directly to your training dataset
. Once you train a custom extraction model, make use of the test page to improve
your model quality by uploading test documents to training dataset if needed.
. If a low confidence score is returned for some labels, make sure to correctly label
your content. If not, add them to the training dataset and relabel to improve the
model quality.
Azure Al | Document Intelligence Studio
=
0
@
?
?
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
×
Form Recognizer Studio > Custom extraction model > Hello-Custom > Test
Custom extraction model
Test model test-model
Upload files
Hello-Custom
+
Run analysis
Analyze options
O
Fields
Result
Code
G
Label data
Drag & drop file
here or
Browse for files
or
receipt.png
Q 1
D
Models
4
Test
Fetch from URL
Contoso
DocType: test-model
ğ
Settings
P811
BH
address
123 Main Street Redmond, WA 98052
24.80%
receipt.png
123 Main Street
Redmond, WA 98052
name
Contoso
58.10%
987-654-3210
6/10/2019 13:59
Sales Associate: Paul
2 #Surface Pro 6
$1,998.00
3 Surface Pen
$299.97
Sub-Total
$2,297.97
Tax
$218.31
Total
$2,516.28
<
1
of 1 >
QQ ? ?
Privacy & Cookies @ Microsoft 2022

| @ | oadline |  | Location | Date/Time (local) | Required action |
| --- | --- | --- | --- | --- | --- |
|  | able |  | Harleysville (PA) | 08-Dec-2019 | Nobody loves a pig wearing lipstick. |
|  | Flight |  | Harleysville (PA) | 08-Dec-2019 13:00 | Two more days and all his problems would be solved |
|  | Round |  | Harleysville (PA) | 09-Dec-2019 |  |
|  | Accent |  | Harleysville (PA) | 10-Dec-2019 |  |
|  | Monkey |  | Harleysville | 11-Dec-2019 |  |
|  | Route |  | Harleysville | 11-Dec-2019 | Peanuts don't grow on |
|  |  |  | (PA) |  | trees |


<!---- Page 359 ---------------------------------------------------------------------------------------------------------------------------------->
Make use of the document list options and filters in
custom projects
· Use the custom extraction model labeling page to navigate through your training
documents with ease by making use of the search, filter, and sort by feature.
. Utilize the grid view to preview documents or use the list view to scroll through the
documents more easily.
Azure Al | Document Intelligence Studio
-
®
$
€
?
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
×
0℃
Form Recognizer Studio > Custom extraction model > Hello-Custom > Label data
= Custom extraction model
Label data
Train
Hello-Custom
23 Run layout
V
Auto label
V
Draw region
0
0
v
+
Add a field
Label data
Drag & drop file
here or
Browse for files
= 1
address
E
CONTOSO LTD.
INVOICE
Contoso Headquarters 123 456th St
New York, NY, 10001
Models
4
Test
0811 #
=
2
name
CONTOSO LTD.
:
Contost Headquarters
INVOICE: INV-100
×
₿
Settings
12: 456 5
New York NY. 10001
INVOICE DATE: 11/15/2019
DUE DATE: 12/15/2019
CUSTOMER NAME: MICROSOFT CORPORATION
SERVICE PERIOD: 10/14/2019 - 11/14/2019
Microsoft Corp
123 Other St,
Redmond WA, 98052
CUSTOMER ID: CID-12345
receipt.png
BILL TO:
Microsoft Finance
123 Bill St,
Redmond WA, 98052
SHIP TO:
Microsoft Delivery
SERVICE ADDRESS:
Microsoft Services
123 Service St,
Redmond WA, 98052
123 Ship St,
Redmond WA, 98052
Invoice_2.pdf
CIT
sample pdf
WVOICE
M
read-barcode.pdf
THANK YOU FOR YOUR BUSINESS!
REMIT TO:
Contoso Billing
123 Remit St
New York, NY, 10001
3
invoice pdf
< 1 of1 >
Q Q q Q
Privacy & Cookies
@ Microsoft 2022
H
SALESPERSON
P.O. NUMBER
REQUISITIONER
SHIPPED VIA
F.O.B. POINT
TERMS
PO-3333
EFF
DATE
ITEM CODE
DESCRIPTION
QTY
UM
PRICE
TAX
AMOUNT
3/4/2021
A123
Consulting Services
2
hours
$30.00
$5.00
$50.00
3/5/2021
B456
Document Fee
3
$10.00
$3.00
$30.00
3/6/2021
C789
Printing Fee
10
pages
$1.00
$1.00
$10.00
SUBTOTAL
$100.00
SALES TAX
$10.00
TOTAL
$110.00
PREVIOUS UNPAID BALANCE
$500.00
AMOUNT DUE
$610.00
Project sharing
Share custom extraction projects with ease. For more information, see Project sharing
with custom models.
Next steps
. Follow our Document Intelligence v3.1 migration guide to learn the differences
from the previous version of the REST API.
· Explore our v4.0 SDK quickstarts to try the v3.0 features in your applications using
the new client libraries.
. Refer to our v4.0 REST API quickstarts to try the v3.0 features using the new REST
API.
Get started with the Document Intelligence Studio.

| H | SALESPERSON | P.O. NUMBER | REQUISITIONER | SHIPPED VIA | F.O.B. POINT | TERMS |
| --- | --- | --- | --- | --- | --- | --- |
|  |  | PO-3333 |  |  |  |  |
|  |  |  |  |  |  |  |

| EFF DATE | ITEM CODE | DESCRIPTION | QTY | UM | PRICE | TAX | AMOUNT |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 3/4/2021 | A123 | Consulting Services | 2 | hours | $30.00 | $5.00 | $50.00 |
| 3/5/2021 | B456 | Document Fee | 3 |  | $10.00 | $3.00 | $30.00 |
| 3/6/2021 | C789 | Printing Fee | 10 | pages | $1.00 | $1.00 | $10.00 |

|  | SUBTOTAL | $100.00 |
| --- | --- | --- |
|  | SALES TAX | $10.00 |
|  | TOTAL | $110.00 |
|  | PREVIOUS UNPAID BALANCE | $500.00 |
| AMOUNT DUE |  | $610.00 |


<!---- Page 360 ---------------------------------------------------------------------------------------------------------------------------------->
Feedback
Was this page helpful?
Yes
P No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 361 ---------------------------------------------------------------------------------------------------------------------------------->
Get started: Document Intelligence
Studio
Article · 03/19/2025
Document Intelligence Studio « is an online tool for visually exploring, understanding,
and integrating features from the Document Intelligence service in your applications.
You can get started by exploring the pretrained models with sample or your own
documents. You can also create projects to build custom template models and reference
the models in your applications.
https://www.microsoft.com/en-us/videoplayer/embed/RE56n49?postJsllMsg=true
Prerequisites for new users
To use Document Intelligence Studio, you need to acquire the following assets from the
Azure portal:
. Azure subscription - Create one for free .
· An Azure Al services or Document Intelligence resource. Once you have your Azure
subscription, create a single-service or Azure Al multi-service [ resource, in the
Azure portal, to get your key and endpoint.
. You can use the free pricing tier (F0) to try the service, and upgrade later to a paid
tier for production.
? Tip
Create an Azure AI services resource if you plan to access multiple Azure AI services
under a single endpoint/key. For Document Intelligence access only, create a
Document Intelligence resource. You need a single-service resource if you intend to
use Microsoft Entra authentication.
Document Intelligence now supports Azure Active Directory (Azure AD) token
authentication in addition to local (key-based) authentication when accessing the
Document Intelligence resources and storage accounts. Be sure to follow below
instructions to set up correct access roles, especially if your resources are applied
with DisableLocalAuth policy.
There are added prerequisites for using custom models in Document Intelligence Studio.
Refer to the documentation for step by step guidance.


<!---- Page 362 ---------------------------------------------------------------------------------------------------------------------------------->
Authorization policies
Your organization can opt to disable local authentication and enforce Microsoft Entra
(formerly Azure Active Directory) authentication for Azure AI Document Intelligence
resources and Azure blob storage.
· Microsoft Entra authentication requires that key based authorization is disabled.
After key access is disabled, Microsoft Entra ID is the only available authorization
method.
· Microsoft Entra allows granting minimum privileges and granular control for Azure
resources.
For more information, see the following guidance:
· Disable local authentication for Azure Al Services.
· Prevent Shared Key authorization for an Azure Storage account
1 Note
If local (key-based) authentication is disabled for your Document Intelligence
service resource, be sure to obtain Cognitive Services User role and your Azure AD
token to authenticate requests on Document Intelligence Studio. The Contributor
role only allows you to list keys but doesn't give you permission to use the resource
when key-access is disabled.
· Designating role assignments. Document Intelligence Studio basic access requires
the Cognitive Services User role. For more information, see Document Intelligence
role assignments.
1 Important
. Make sure you have the Cognitive Services User role, and not the Cognitive
Services Contributor role when setting up Microsoft Entra ID authentication.
· V Cognitive Services User: you need this role to Document Intelligence or
Azure AI services resource to enter the analyze page.
· V Contributor: you need this role to create resource group, Document
Intelligence service, or Azure AI services resource.
. In Azure context, Contributor role can only perform actions to control and
manage the resource itself, including listing the access keys.


<!---- Page 363 ---------------------------------------------------------------------------------------------------------------------------------->
. User accounts with a Contributor are only able to access the Document
Intelligence service by calling with access keys. However, when setting up
access with Microsoft Entra ID, key-access is disabled and Cognitive Services
User role is required for an account to use the resources.
Authentication in Studio
Navigate to the Document Intelligence Studio [. If it's your first time logging in, a
popup window appears prompting you to configure your service resource. In
accordance with your organization's policy, you have one or two options:
· Microsoft Entra authentication: access by Resource (recommended).
o Choose your existing subscription.
o Select an existing resource group within your subscription or create a new one.
o Select your existing Document Intelligence or Azure Al services resource.
Welcome to Document Intelligence Studio
×
Configure service resource
Configure service resource
To use Document Intelligence, you need an Azure subscription containing a service
resource for usage and billing. Resources are organized in resource groups. Learn
more
Review
Access By
Resource
API endpoint and key
Subscription "
v
Resource group "
V
Create new
Document Intelligence or Cognitive Service Resource *
V
Create new resource
+
Continue
Cancel
. Local authentication: access by API endpoint and key.
o Retrieve your endpoint and key from the Azure portal.
o Go to the overview page for your resource and select Keys and Endpoint from
the left navigation bar.
o Enter the values in the appropriate fields.


<!---- Page 364 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
x
Search
C Regenerate Key 1 C Regenerate Key2
Overview
Activity log
Access control (IAM)
These keys are used to access your Azure Al service APL Do not share your kryt. Store them securely= for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API cal. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
Diagnose and solve problems
V Resource Management
Show Keys
" Keys and Endpoint
KEY 1
00
Encryption
KEY 2
Pricing tier
O
<- > Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
> Monitoring
+
Automation
Helo
. After validating the scenario in the Document Intelligence Studio, use the C#, Java,
JavaScript, or Python client libraries or the REST API to get started incorporating
Document Intelligence models into your own applications.
Try a Document Intelligence model
To learn more about the available Document Intelligence models, see our model support
page.
. Once your resource is configured, you can try the different models offered by
Document Intelligence Studio. From the front page, select any Document
Intelligence model to try using with a no-code approach.
. To test any of the document analysis or prebuilt models, select the model and use
one of the sample documents or upload your own document to analyze. The
analysis result is displayed at the right in the content-result-code window.
. Custom models need to be trained on your documents. See custom models
overview for an overview of custom models.
. After validating the scenario in the Document Intelligence Studio, use the C#, Java,
JavaScript, or Python client libraries or the REST API to get started incorporating
Document Intelligence models into your own applications.
View resource details
To view resource details such as name and pricing tier, select the Settings icon in the
top-right corner of the Document Intelligence Studio home page and select the


<!---- Page 365 ---------------------------------------------------------------------------------------------------------------------------------->
Resource tab. If you have access to other resources, you can switch resources as well.
Form Recognizer Studio > Settings
Settings
Directory
Resource
+
Resources are your unique aliases for the service and allow usage and billing. Choose your default resource. Learn more about Azure resources.
With Document Intelligence, you can quickly automate your data processing in
applications and workflows, easily enhance data-driven strategies, and skillfully enrich
document search capabilities.
Manage third-party settings for Studio access
Edge:
· Go to Settings for Microsoft Edge
. Search for "third-party"
· Go to Manage and delete cookies and site data
. Turn off the setting of Block third-party cookies
Chrome:
· Go to Settings for Chrome
. Search for "Third-party"
. Under Default behavior, select Allow third-party cookies
Firefox:
· Go to Settings for Firefox
· Search for "cookies"
. Under Enhanced Tracking Protection, select Manage Exceptions
. Add exception for https://documentintelligence.ai.azure.com or the Document
Intelligence Studio URL of your environment
Safari:
. Choose Safari > Preferences
· Select Privacy
. Deselect Block all cookies
Troubleshooting


<!---- Page 366 ---------------------------------------------------------------------------------------------------------------------------------->
[ Expand table
Scenario
Cause
Resolution
You receive the error message
Your Document
Intelligence
resource, bound
to the custom
project was
deleted or
moved to
another resource
group.
There are two
Form Recognizer Not Found when opening a custom project.
ways to
resolve this
problem:
· Re-create
the
Document
Intelligence
resource
under the
same
subscription
and resource
group with
the same
name.
· Re-create a
custom
project with
the migrated
Document
Intelligence
resource and
specify the
same storage
account.
You receive the error message
The principal
Reference
PermissionDenied when using prebuilt apps or opening a
doesn't have
access to
API/Operation
when analyzing
against prebuilt
models or
opening a
custom project.
It's likely the local
(key-based)
authentication is
disabled for your
Document
Intelligence
resource don't
have enough
permission to
Azure role
custom project.
assignments
to configure
your access
roles.

| Scenario | Cause | Resolution |
| --- | --- | --- |
| You receive the error message | Your Document Intelligence resource, bound to the custom project was deleted or moved to another resource group. | There are two |
| Form Recognizer Not Found when opening a custom project. |  | ways to resolve this problem: · Re-create the Document Intelligence resource under the same subscription and resource group with the same name. |
|  |  | · Re-create a custom project with the migrated Document Intelligence resource and specify the same storage account. |
| You receive the error message | The principal | Reference |
| PermissionDenied when using prebuilt apps or opening a | doesn't have access to API/Operation when analyzing against prebuilt models or opening a custom project. It's likely the local (key-based) authentication is disabled for your Document Intelligence resource don't have enough permission to | Azure role |
| custom project. |  | assignments to configure your access roles. |
|  |  |  |


<!---- Page 367 ---------------------------------------------------------------------------------------------------------------------------------->
Scenario
Cause
Resolution
access the
resource.
You receive the error message
The request isn't
Reference
AuthorizationPermissionMismatch when opening a custom
authorized to
Azure role
project.
perform the
assignments
to configure
your access
roles.
operation using
the designated
permission. It's
likely the local
(key-based)
authentication is
disabled for your
storage account
and you don't
have the granted
permission to
access the blob
data.
You can't sign in to Document Intelligence Studio and receive
the error message
It's likely that
To resolve,
see Manage
your browser is
InteractionRequiredAuthError : login_required : AADSTS50058 : A
blocking third-
third-party
settings for
your browser.
silent sign-request was sent but no user is signed in
party cookies so
you can't
successfully sign
in.
Next steps
· Learn how to create custom projects in Document Intelligence Studio
. Get started with Document Intelligence client libraries
Feedback
Was this page helpful?
& Yes
No
Provide product feedback | Get help at Microsoft Q&A

| Scenario | Cause | Resolution |
| --- | --- | --- |
|  | access the resource. |  |
| You receive the error message | The request isn't | Reference |
| AuthorizationPermissionMismatch when opening a custom | authorized to | Azure role |
| project. | perform the | assignments to configure your access roles. |
|  | operation using the designated permission. It's likely the local (key-based) authentication is disabled for your storage account and you don't have the granted permission to access the blob data. |  |
| You can't sign in to Document Intelligence Studio and receive the error message | It's likely that | To resolve, see Manage |
|  | your browser is |  |
| InteractionRequiredAuthError : login_required : AADSTS50058 : A | blocking third- | third-party settings for your browser. |
| silent sign-request was sent but no user is signed in | party cookies so you can't successfully sign in. |  |


<!---- Page 368 ---------------------------------------------------------------------------------------------------------------------------------->
Get started with Document Intelligence
Article · 04/11/2025
1 Important
· Azure Cognitive Services Form Recognizer is now Azure Al Document Intelligence.
· Some platforms are still awaiting the renaming update.
· All mention of Form Recognizer or Document Intelligence in our documentation
refers to the same Azure service.
This content applies to:
v4.0 (GA) Earlier versions:
v3.1 (GA)
v3.0 (GA)
· Get started with Azure Al Document Intelligence latest stable version v4.0 2024-11-30
(GA).
· Azure Al Document Intelligence / Form Recognizer is a cloud-based Azure Al service that
uses machine learning to extract key-value pairs, text, tables, and key data from your
documents.
. You can easily integrate document processing models into your workflows and
applications by using a programming language SDK or calling the REST API.
. We recommend that you use the free service while you're learning the technology for this
quickstart. Remember that the number of free pages is limited to 500 per month.
To learn more about the API features and development options, visit our Overview page.
Client library | REST API reference | Package | Samples |Supported REST API version
In this quickstart, use the following features to analyze and extract data and values from forms
and documents:
· Layout model-Analyze and extract tables, lines, words, and selection marks like radio
buttons and check boxes in documents, without the need to train a model.
· Prebuilt model-Analyze and extract common fields from specific document types using
a prebuilt model.
Prerequisites
. Azure subscription - Create one for free.
. The current version of Visual Studio IDEEZ.


<!---- Page 369 ---------------------------------------------------------------------------------------------------------------------------------->
· An Azure Al services or Document Intelligence resource. Once you have your Azure
subscription, create a single-service or Azure Al multi-service [ resource, in the Azure
portal, to get your key and endpoint.
. You can use the free pricing tier (F0) to try the service, and upgrade later to a paid tier for
production.
? Tip
Create an Azure AI services resource if you plan to access multiple Azure AI services under
a single endpoint/key. For Document Intelligence access only, create a Document
Intelligence resource. You need a single-service resource if you intend to use Microsoft
Entra authentication.
. After your resource deploys, select Go to resource. You need the key and endpoint from
the resource you create to connect your application to the Document Intelligence API.
You paste your key and endpoint into the code later in the quickstart:
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
P Search
«
C Regenerate Key1 3 Regenerate Key2
Overview
Activity log
& Access control (IAM)
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
LA
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
> Monitoring
> Automation
Lo
> Help
Set up
1. Start Visual Studio.
2. On the start page, choose Create a new project.


<!---- Page 370 ---------------------------------------------------------------------------------------------------------------------------------->
Visual Studio 2022
Open recent
As you use Visual Studio, any projects, folders, or files that you open will show up here for quick
access.
You can pin anything that you open frequently so that it's always at the top of the list.
Get started
Clone a repository
Get code from an online repository like GitHub or
Azure DevOps
Open a project or solution
Open a local Visual Studio project or .sIn file
Open a local folder
Navigate and edit code within any folder
Create a new project
Choose a project template with code scaffolding
to get started
Continue without code ->
3. On the Create a new project page, enter console in the search box. Choose the Console
Application template, then choose Next.
-
X
Create a new project
console
×
¥
Clear all
Recent project templates
C#
All platforms
All project types
A list of your recently accessed templates will be
displayed here.
C#
Console Application
DA
A project for creating a command-line application that can run on .NET Core on
Windows, Linux and macOS
C#
Linux
macOS
Windows
Console
C#
Console App (.NET Framework)
DA
A project for creating a command-line application
C#
Windows
Console
Other results based on your search
F#
Console Application
DA
A project for creating a command-line application that can run on .NET Core on
Windows, Linux and macOS
F#
Linux
macOS
Windows
Console
VB
Console Application
CA
A project for creating a command-line application that can run on .NET Core on
Windows, Linux and macOS
Visual Basic
Linux
macOS
Windows
Console
Back
Next
4. In the Configure your new project dialog window, enter doc_intel_quickstart in the
Project name box. Then choose Next.
5. In the Additional information dialog window, select .NET 8.0 (Long-term support), and
then select Create.


<!---- Page 371 ---------------------------------------------------------------------------------------------------------------------------------->
Additional information
Console App
C#
Linux
macOS
Windows
Console
Framework
8
.NET 8.0 (Long Term Support)
¥
Do not use top-level statements
O
Enable native AOT publish
®
Install the client library with NuGet
1. Right-click on your doc_intel_quickstart project and select Manage NuGet Packages ... .
Build Solution
Ctrl+Shift+B
K
Solution 'doc-intel-quickstart' (1 of 1 project)
#
doc-intel-quickstart
Rebuild Solution
Dependencies
Clean Solution
C# Program.cs
Analyze and Code Cleanup
Batch Build ...
Configuration Manager ...
8
Manage NuGet Packages for Solution ...
1
Restore NuGet Packages
2. Select the Browse tab and type Azure.AI.DocumentIntelligence.
3. Select the Include prerelease checkbox.
Browse
Installed
Updates
Consolidate
azure.ai.documentintelligence
x
0
Include prerelease
4. Choose a version from the dropdown menu and install the package in your project.
Build your application
To interact with the Document Intelligence service, you need to create an instance of the
DocumentIntelligenceClient class. To do so, you create an AzureKeyCredential with your key
from the Azure portal and a DocumentIntelligenceClient instance with the AzureKeyCredential
and your Document Intelligence endpoint .
!
Note

| Build Solution | Ctrl+Shift+B | K | Solution 'doc-intel-quickstart' (1 of 1 project) |  |
| --- | --- | --- | --- | --- |
|  |  | # doc-intel-quickstart |  |  |
| Rebuild Solution |  |  | Dependencies |  |
| Clean Solution |  | C# Program.cs |  |  |
| Analyze and Code Cleanup |  |  |  |  |
| Batch Build ... |  |  |  |  |
| Configuration Manager ... |  |  |  |  |
| 8 Manage NuGet Packages for Solution ... |  |  |  |  |
| 1 Restore NuGet Packages |  |  |  |  |


<!---- Page 372 ---------------------------------------------------------------------------------------------------------------------------------->
· Starting with .NET 6, new projects using the console template generate a new
program style that differs from previous versions.
· The new output uses recent C# features that simplify the code you need to write.
· When you use the newer version, you only need to write the body of the Main
method. You don't need to include top-level statements, global using directives, or
implicit using directives.
. For more information, see New C# templates generate top-level statements.
1. Open the Program.cs file.
2. Delete the existing code, including the line Console.Writeline("Hello World!"), and
select one of the following code samples to copy and paste into your application's
Program.cs file:
· Layout model
· Prebuilt model
1 Important
We recommend Microsoft Entra ID authentication with managed identities for Azure
resources to avoid storing credentials with your applications that run in the cloud.
Use API keys with caution. Don't include the API key directly in your code, and never post
it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys
regularly, and restrict access to Azure Key Vault using role based access control and
network access restrictions. For more information about using API keys securely in your
apps, see API keys with Azure Key Vault.
For more information about AI services security, see Authenticate requests to Azure AI
services.
Layout model
Extract text, selection marks, text styles, table structures, and bounding region coordinates
from documents.
V For this example, you'll need a document file from a URI. You can use our sample
document for this quickstart.
V We've added the file URI value to the Uri fileUri variable at the top of the script.


<!---- Page 373 ---------------------------------------------------------------------------------------------------------------------------------->
V To extract the layout from a given file at a URI, use the StartAnalyzeDocumentFromUri
method and pass prebuilt-layout as the model ID. The returned value is an
AnalyzeResult object containing data from the submitted document.
Add the following code sample to the Program.cs file. Make sure you update the key and
endpoint variables with values from your Azure portal Document Intelligence instance:
C#
using Azure;
using Azure.AI. DocumentIntelligence;
//set `<your-endpoint> and ><your-key> variables with the values from the Azure
portal to create your AzureKeyCredential' and `DocumentIntelligenceClient'
instance
string endpoint = "<your-endpoint>";
string key = "<your-key>";
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient(new
Uri(endpoint), credential);
// sample document
Uri fileUri = new Uri ("https://raw. githubusercontent. com/Azure-Samples/cognitive-
services-REST-api-samples/master/curl/form-recognizer/sample-layout.pdf");
AnalyzeDocumentContent content = new AnalyzeDocumentContent()
{
UrlSource= fileUri
};
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-layout", content);
AnalyzeResult result = operation. Value;
foreach (DocumentPage page in result. Pages)
{
Console.WriteLine($"Document Page {page. PageNumber} has {page. Lines. Count }
line(s), {page.Words.Count} word(s)," +
$" and {page. SelectionMarks. Count} selection mark(s). ");
for (int i = 0; i < page. Lines.Count; i++)
{
DocumentLine line = page. Lines[i];
Console.WriteLine($" Line {i}:");
Console. WriteLine($"
Content: '{line. Content} '");
Console.Write("
Bounding polygon, with points ordered clockwise:");
for (int j = 0; j < line. Polygon.Count; j += 2)
{


<!---- Page 374 ---------------------------------------------------------------------------------------------------------------------------------->
Console.Write($" ({line. Polygon[j]}, {line. Polygon[j + 1]})");
}
Console.WriteLine();
}
for (int i = 0; i < page. SelectionMarks. Count; i++)
{
DocumentSelectionMark selectionMark = page.SelectionMarks[i];
Console.WriteLine($" Selection Mark {i} is {selectionMark. State} . ");
Console.WriteLine($"
State: {selectionMark. State}");
Console.Write("
Bounding polygon, with points ordered clockwise:");
for (int j = 0; j < selectionMark. Polygon. Count; j++)
+ 1]})");
{
Console.Write($" ({selectionMark. Polygon[j]}, {selectionMark. Polygon [j
}
Console.WriteLine();
}
}
for (int i = 0; i < result. Paragraphs. Count; i++)
{
DocumentParagraph paragraph = result. Paragraphs[i];
Console.WriteLine($"Paragraph {i} :");
Console. WriteLine($" Content: {paragraph. Content}");
if (paragraph. Role != null)
{
Console.WriteLine($" Role: {paragraph. Role}");
}
}
foreach (DocumentStyle style in result.Styles)
{
// Check the style and style confidence to see if text is handwritten.
// Note that value '0.8' is used as an example.
bool isHandwritten = style. IsHandwritten. HasValue && style. IsHandwritten ==
true;
if (isHandwritten && style.Confidence > 0.8)
{
Console. WriteLine($"Handwritten content found:");
foreach (DocumentSpan span in style. Spans)
{
var handwrittenContent = result. Content. Substring(span.Offset,
span. Length) ;
Console.WriteLine($" {handwrittenContent}");
}


<!---- Page 375 ---------------------------------------------------------------------------------------------------------------------------------->
}
}
for (int i = 0; i < result. Tables.Count; i++)
{
DocumentTable table = result. Tables[i];
Console.WriteLine($"Table {i} has {table. RowCount} rows and
{table. ColumnCount} columns.");
foreach (DocumentTableCell cell in table. Cells)
{
Console.WriteLine($" Cell ({cell. RowIndex}, {cell. ColumnIndex} ) is a
'{cell.Kind}' with content: {cell. Content}");
}
}
Run your application
Once you add a code sample to your application, choose the green Start button next to
formRecognizer_quickstart to build and run your program, or press F5.
File
Edit
View
Git
Project
Build
Debug
Test
Analyze
Tools
Extensions
Window
Help
+
Debug
Any CPU
Prebuilt model
Analyze and extract common fields from specific document types using a prebuilt model. In
this example, we analyze an invoice using the prebuilt-invoice model.
? Tip
You aren't limited to invoices-there are several prebuilt models to choose from, each of
which has its own set of supported fields. The model to use for the analyze operation
depends on the type of document to be analyzed. See model data extraction.
V Analyze an invoice using the prebuilt-invoice model. You can use our sample invoice
document [ for this quickstart.
V We've added the file URI value to the Uri invoiceUri variable at the top of the Program.cs
file.
V To analyze a given file at a URI, use the StartAnalyzeDocumentFromUri method and pass
prebuilt-invoice as the model ID. The returned value is an AnalyzeResult object
containing data from the submitted document.


<!---- Page 376 ---------------------------------------------------------------------------------------------------------------------------------->
V For simplicity, all the key-value pairs that the service returns are not shown here. To see
the list of all supported fields and corresponding types, see our Invoice concept page.
Add the following code sample to your Program.cs file. Make sure you update the key and
endpoint variables with values from your Azure portal Document Intelligence instance:
C#
using Azure;
using Azure.AI. DocumentIntelligence;
//set ><your-endpoint>> and ><your-key>> variables with the values from the Azure
portal to create your AzureKeyCredential' and 'DocumentIntelligenceClient
instance
string endpoint = "<your-endpoint>";
string key = "<your-key>";
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient (new
Uri(endpoint), credential);
//sample invoice document
Uri uriSource = new Uri("https://raw. githubusercontent. com/Azure-
Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-
invoice.pdf");
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil.Completed, "prebuilt-invoice", uriSource);
AnalyzeResult result = operation. Value;
for (int i = 0; i < result.Documents.Count; i++)
{
Console.WriteLine($"Document {i}:");
AnalyzedDocument document = result. Documents[i];
if (document.Fields. TryGetValue("VendorName", out DocumentField
vendorNameField)
&& vendorNameField. FieldType == DocumentFieldType.String)
{
string vendorName = vendorNameField. ValueString;
Console.WriteLine($"Vendor Name: '{vendorName}', with confidence
{vendorNameField.Confidence}");
}
if (document.Fields. TryGetValue("CustomerName", out DocumentField
customerNameField)
&& customerNameField. FieldType == DocumentFieldType.String)
{
string customerName = customerNameField. ValueString;
Console.WriteLine($"Customer Name: '{customerName} ', with confidence
{customerNameField.Confidence}");


<!---- Page 377 ---------------------------------------------------------------------------------------------------------------------------------->
}
if (document. Fields. TryGetValue("Items", out DocumentField itemsField)
&& itemsField. FieldType == DocumentFieldType. List)
{
foreach (DocumentField itemField in itemsField. ValueList)
{
Console.WriteLine("Item:");
if (itemField. FieldType == DocumentFieldType.Dictionary)
{
IReadOnlyDictionary<string, DocumentField> itemFields =
itemField. ValueDictionary;
if (itemFields. TryGetValue("Description", out DocumentField
itemDescriptionField)
&& itemDescriptionField. FieldType == DocumentFieldType.String)
}
{
string itemDescription = itemDescriptionField. ValueString;
Console.WriteLine($" Description: '{itemDescription}', with
confidence {itemDescriptionField. Confidence}");
}
if (itemFields. TryGetValue("Amount", out DocumentField
itemAmountField)
&& itemAmountField. FieldType == DocumentFieldType. Currency)
{
CurrencyValue itemAmount = itemAmountField. ValueCurrency;
Console.WriteLine($" Amount: '{itemAmount. CurrencySymbol}
{itemAmount. Amount}', with confidence {itemAmountField. Confidence}");
}
}
}
if (document. Fields. TryGetValue("SubTotal", out DocumentField subTotalField)
&& subTotalField. FieldType == DocumentFieldType. Currency)
{
CurrencyValue subTotal = subTotalField. ValueCurrency;
Console.WriteLine($"Sub Total: '{subTotal. CurrencySymbol}
{subTotal.Amount}', with confidence {subTotalField.Confidence}");
}
if (document. Fields. TryGetValue("TotalTax", out DocumentField totalTaxField)
&& totalTaxField. FieldType == DocumentFieldType. Currency)
{
CurrencyValue totalTax = totalTaxField. ValueCurrency;
Console.WriteLine($"Total Tax: '{totalTax. CurrencySymbol}
{totalTax. Amount} ', with confidence {totalTaxField. Confidence}");
}
if (document.Fields. TryGetValue("InvoiceTotal", out DocumentField
invoiceTotalField)
&& invoiceTotalField. FieldType == DocumentFieldType. Currency)
{


<!---- Page 378 ---------------------------------------------------------------------------------------------------------------------------------->
CurrencyValue invoiceTotal = invoiceTotalField. ValueCurrency;
Console.WriteLine($"Invoice Total: '{invoiceTotal. CurrencySymbol}
{invoiceTotal. Amount} ', with confidence {invoiceTotalField. Confidence}");
}
}
Run your application
Once you add a code sample to your application, choose the green Start button next to
formRecognizer_quickstart to build and run your program, or press F5.
File
Edit
View
Git
Project
Build
Debug
Test
Analyze
Tools
Extensions
Window
Help
€
Debug
Any CPU
That's it, congratulations!
In this quickstart, you used a document Intelligence model to analyze various forms and
documents. Next, explore the Document Intelligence Studio and reference documentation to
learn about Document Intelligence API in depth.
Next steps
. For an enhanced experience and advanced model quality, try Document Intelligence
Studio z
· For v3.1 to v4.0 migration, see Changelog Migration guides.
. Find more samples on GitHub .


<!---- Page 379 ---------------------------------------------------------------------------------------------------------------------------------->
Use Document Intelligence models
Article · 02/07/2025
v2.1 (GA)
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
In this guide, learn how to add Document Intelligence models to your applications and
workflows. Use a programming language SDK of your choice or the REST API.
Azure AI Document Intelligence is a cloud-based Azure AI service that uses machine
learning to extract key text and structure elements from documents. We recommend
that you use the free service while you learn the technology. Remember that the
number of free pages is limited to 500 per month.
Choose from the following Document Intelligence models and analyze and extract data
and values from forms and documents:
V The prebuilt-read model is at the core of all Document Intelligence models and can
detect lines, words, locations, and languages. Layout, general document, prebuilt,
and custom models all use the read model as a foundation for extracting texts
from documents.
V The prebuilt-layout model extracts text and text locations, tables, selection marks,
and structure information from documents and images. You can extract key/value
pairs using the layout model with the optional query string parameter
features=keyValuePairs enabled.
V The prebuilt-contract model extracts key information from contractual agreements.
V The prebuilt-healthInsuranceCard.us model extracts key information from US health
insurance cards.
V The prebuilt tax document models model extracts information reported on US tax
forms.
V The prebuilt-invoice model extracts key fields and line items from sales invoices in
various formats and quality. Fields include phone-captured images, scanned
documents, and digital PDFs.
V The prebuilt-receipt model extracts key information from printed and handwritten
sales receipts.
V The prebuilt-idDocument model extracts key information from US drivers licenses,
international passport biographical pages, US state IDs, social security cards, and


<!---- Page 380 ---------------------------------------------------------------------------------------------------------------------------------->
permanent resident cards.
Client library | SDK reference [ | REST API reference | Package z | Samples z|Supported
REST API version
Prerequisites
. An Azure subscription - Create one for free .
. The Visual Studio IDEE.
. An Azure Al services or Document Intelligence resource. Create a single-service
or multi-service . You can use the free pricing tier (F0) to try the service, and
upgrade later to a paid tier for production.
. The key and endpoint from the resource you create to connect your application to
the Azure Document Intelligence service.
1. After your resource deploys, select Go to resource.
2. In the left navigation menu, select Keys and Endpoint.
3. Copy one of the keys and the Endpoint for use later in this article.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
P Search
«
Regenerate Key1 & Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
Encryption
KEY 2
Pricing tier
S
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
0
Locks
Monitoring
Automation
Ls
Help
. A document file at a URL location. For this project, you can use the sample forms
provided in the following table for each feature:
Expand table


<!---- Page 381 ---------------------------------------------------------------------------------------------------------------------------------->
Feature
modelID
document-url
Read model
prebuilt-read
Sample brochure [
Layout model
prebuilt-layout
Sample booking confirmation 2
W-2 form model
prebuilt-tax.us.w2
Sample W-2 form 22
Invoice model
prebuilt-invoice
Sample invoice
Receipt model
prebuilt-receipt
Sample receiptlz7
ID document model
prebuilt-idDocument
Sample ID document
Set your environment variables
To interact with the Document Intelligence service, you need to create an instance of the
DocumentAnalysisClient class. To do so, instantiate the client with your key and
endpoint from the Azure portal. For this project, use environment variables to store and
access credentials.
1 Important
Use API keys with caution. Don't include the API key directly in your code, and
never post it publicly. If you use an API key, store it securely in Azure Key Vault. For
more information about using API keys securely in your apps, see API keys with
Azure Key Vault.
For more information about AI services security, see Authenticate requests to
Azure AI services.
To set the environment variable for your Document Intelligence resource key, open a
console window, and follow the instructions for your operating system and development
environment. Replace <yourKey> and <yourEndpoint> with the values from your
resource in the Azure portal.
Windows
Environment variables in Windows aren't case-sensitive. They're typically declared in
uppercase, with words joined by an underscore. At a command prompt, run the
following commands:
1. Set your key variable:

| Feature | modelID | document-url |
| --- | --- | --- |
| Read model | prebuilt-read | Sample brochure [ |
| Layout model | prebuilt-layout | Sample booking confirmation 2 |
| W-2 form model | prebuilt-tax.us.w2 | Sample W-2 form 22 |
| Invoice model | prebuilt-invoice | Sample invoice |
| Receipt model | prebuilt-receipt | Sample receiptlz7 |
| ID document model | prebuilt-idDocument | Sample ID document |


<!---- Page 382 ---------------------------------------------------------------------------------------------------------------------------------->
Console
setx DI_KEY <yourKey>
2. Set your endpoint variable
Console
setx DI_ENDPOINT <yourEndpoint>
3. Close the Command Prompt window after you set your environment variables.
The values remain until you change them again.
4. Restart any running programs that read the environment variable. For
example, if you're using Visual Studio or Visual Studio Code as your editor,
restart before running the sample code.
Here are a few more helpful commands to use with environment variables:
Expand table
Command
Action
Example
setx VARIABLE_NAME=
Delete the environment variable by setting the
value to an empty string.
setx DI_KEY=
setx
Set or change the value of an environment
setx DI_KEY=
VARIABLE_NAME=value
variable.
<yourKey>
set VARIABLE_NAME
Display the value of a specific environment
variable.
set DI_KEY
set
Display all environment variables.
set
Set up your programming environment
1. Start Visual Studio.
2. On the start page, choose Create a new project.

| Command | Action | Example |
| --- | --- | --- |
| setx VARIABLE_NAME= | Delete the environment variable by setting the value to an empty string. | setx DI_KEY= |
| setx | Set or change the value of an environment | setx DI_KEY= |
| VARIABLE_NAME=value | variable. | <yourKey> |
| set VARIABLE_NAME | Display the value of a specific environment variable. | set DI_KEY |
| set | Display all environment variables. | set |


<!---- Page 383 ---------------------------------------------------------------------------------------------------------------------------------->
Visual Studio 2022
Open recent
Get started
As you use Visual Studio, any projects, folders, or files that you open will show up here for quick
access.
Clone a repository
Get code from an online repository like GitHub or
Azure DevOps
You can pin anything that you open frequently so that it's always at the top of the list.
Open a project or solution
Open a local Visual Studio project or .sin file
Open a local folder
Navigate and edit code within any folder
Create a new project
Choose a project template with code scaffolding
to get started
Continue without code ->
3. On the Create a new project page, enter console in the search box. Select the
Console Application template, then choose Next.
×
Create a new project
console
×
¥
Clear all
Recent project templates
C#
All platforms
All project types
A list of your recently accessed templates will be
displayed here.
C#
Console Application
CAN
A project for creating a command-line application that can run on .NET Core on
Windows, Linux and macOS
C#
Linux
macOS
Windows
Console
C#
Console App (.NET Framework)
CA
A project for creating a command-line application
C#
Windows
Console
Other results based on your search
F#
Console Application
DA
A project for creating a command-line application that can run on .NET Core on
Windows, Linux and macOS
F#
Linux
macOS
Windows
Console
VB
Console Application
CA
A project for creating a command-line application that can run on .NET Core on
Windows, Linux and macOS
Visual Basic
Linux
macOS
Windows
Console
Back
Next
4. In the Configure your new project page, under Project name enter
docIntelligence_app. Then select Next.


<!---- Page 384 ---------------------------------------------------------------------------------------------------------------------------------->
Configure your new project
Console App
C#
Linux
macOS
Windows
Console
Project name
0
Location
C:\Users\ ...
¥
...
Solution name O
Place solution and project in the same directory
Back
Next
5. In the Additional information page, select .NET 8.0 (Long-term support), and
then select Create.
Additional information
Console App
C#
Linux
macOS
Windows
Console
Framework
O
.NET 8.0 (Long Term Support)
¥
Do not use top-level statements
®
Enable native AOT publish @
Install the client library with NuGet
1. Right-click on your docIntelligence_app project and select Manage NuGet
Packages ... .


<!---- Page 385 ---------------------------------------------------------------------------------------------------------------------------------->
0
Solution Explorer
4 X
0
200
¥
&
S
Search Solution Explorer (Ctrl+;)
p
¥
age source:
nuget.org
€
Solution
Build
Rebuild
Clean
Analyze and Code Cleanup
Pack
Publish ...
Upgrade
8
Collapse All Descendants
Scope to This
Ctrl+Left Arrow
8
New Solution Explorer View
File Nesting
Edit Project File
PPV
8
Manage NuGet Packages ...
Manage User Secrets
Remove Unused References ...
2. Select the Browse tab and type Azure.AI.DocumentIntelligence.
3. Choose a version from the dropdown menu and install the package in your project.
Build your application
D Note
Starting with .NET 6, new projects using the console template generate a new
program style that differs from previous versions. The new output uses recent C#
features that simplify the code you need to write.
When you use the newer version, you only need to write the body of the Main
method. You don't need to include top-level statements, global using directives, or
implicit using directives. For more information, see C# console app template
generates top-level statements.
1. Open the Program.cs file.
2. Delete the existing code, including the line Console. Writeline("Hello World!").
3. Select one of the following code samples and copy/paste into your application's
Program.cs file:
· prebuilt-read
· prebuilt-layout
· prebuilt-tax.us.w2
· prebuilt-invoice
· prebuilt-receipt
· prebuilt-idDocument


<!---- Page 386 ---------------------------------------------------------------------------------------------------------------------------------->
4. After you add a code sample to your application, choose the green Start button
next to the project name to build and run your program, or press F5.
File
Edit
View
Git
Project
Build
Debug
Test
Analyze
Tools
Extensions
Window
Help
Debug
Any CPU
Use the Read model
C#
using Azure;
using Azure.AI. DocumentIntelligence;
//use your ` key` and ` endpoint` environment variables to create your
`AzureKeyCredential` and `DocumentIntelligenceClient' instances
string key = Environment. GetEnvironmentVariable("DI_KEY");
string endpoint = Environment. GetEnvironmentVariable("DI_ENDPOINT");
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient (new
Uri(endpoint), credential);
//sample document
Uri fileUri = new Uri("https://raw. githubusercontent. com/Azure-
Samples/cognitive-services-REST-api-samples/master/curl/form-
recognizer/rest-api/read.png");
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-read", fileUri);
AnalyzeResult result = operation. Value;
foreach (DocumentPage page in result.Pages)
{
Console.WriteLine($"Document Page {page. PageNumber} has
{page. Lines.Count} line(s), {page.Words.Count} word(s),");
Console.WriteLine($"and {page. SelectionMarks. Count} selection
mark(s).");
for (int i = 0; i < page. Lines.Count; i++)
{
DocumentLine line = page. Lines[i];
Console.WriteLine($" Line {i} has content: '{line. Content} '.");
Console.WriteLine($"
Its bounding polygon (points ordered
clockwise) :");
for (int j = 0; j < line. Polygon. Count; j++)
{
Console.WriteLine($"
Point {j} => X: {line. Polygon[] . X},
Y: {line. Polygon[j]. Y}");
}
}


<!---- Page 387 ---------------------------------------------------------------------------------------------------------------------------------->
}
foreach (DocumentStyle style in result. Styles)
{
// Check the style and style confidence to see if text is handwritten.
// Note that value '0.8' is used as an example.
bool isHandwritten = style. IsHandwritten. HasValue && style. IsHandwritten
== true;
if (isHandwritten && style.Confidence > 0.8)
{
Console.WriteLine($"Handwritten content found:");
foreach (DocumentSpan span in style. Spans)
}
{
Console.WriteLine($" Content:
{result.Content.Substring(span. Index, span. Length)}");
}
}
Console.WriteLine("Detected languages :");
foreach (DocumentLanguage language in result. Languages)
{
Console.WriteLine($" Found language with locale' {language. Locale}' with
confidence {language. Confidence} . ");
}
Visit the Azure samples repository on GitHub and view the read model output .
Use the Layout model
C#
using Azure;
using Azure.AI. DocumentIntelligence;
//use your `key` and ` endpoint` environment variables to create your
`AzureKeyCredential` and `DocumentIntelligenceClient' instances
string key = Environment. GetEnvironmentVariable("DI_KEY");
string endpoint = Environment. GetEnvironmentVariable("DI_ENDPOINT");
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient (new
Uri(endpoint), credential);
// sample document document
Uri fileUri = new Uri ("https://raw.githubusercontent. com/Azure-
Samples/cognitive-services-REST-api-samples/master/curl/form-
recognizer/rest-api/layout.png");


<!---- Page 388 ---------------------------------------------------------------------------------------------------------------------------------->
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync (WaitUntil. Completed, "prebuilt-layout",
fileUri);
AnalyzeResult result = operation. Value;
foreach (DocumentPage page in result. Pages)
{
Console.WriteLine($"Document Page {page. PageNumber} has
{page. Lines.Count} line(s), {page.Words.Count} word(s),");
Console.WriteLine($"and {page. SelectionMarks. Count} selection
mark(s).");
for (int i = 0; i < page. Lines.Count; i++)
{
DocumentLine line = page. Lines[i];
Console.WriteLine($" Line {i} has content: '{line. Content} ' .");
Console.WriteLine($"
Its bounding polygon (points ordered
clockwise) :");
for (int j = 0; j < line. Polygon. Count; j++)
{
Console.WriteLine($"
Point {j} => X: {line. Polygon[j] . X},
Y: {line.Polygon[j]. Y}");
}
}
for (int i = 0; i < page.SelectionMarks. Count; i++)
{
DocumentSelectionMark selectionMark = page. SelectionMarks[i];
Console. WriteLine($" Selection Mark {i} is
{selectionMark. State} . ");
Console.WriteLine($"
Its bounding polygon (points ordered
clockwise):");
for (int j = 0; j < selectionMark.Polygon. Count; j++)
{
Console.WriteLine($"
Point {j} => X:
{selectionMark. Polygon[] .X}, Y: {selectionMark. Polygon[] . Y}");
}
}
}
Console.WriteLine("Paragraphs :");
foreach (DocumentParagraph paragraph in result. Paragraphs)
{
Console.WriteLine($" Paragraph content: {paragraph. Content}");
if (paragraph. Role != null)
{
Console. WriteLine ($"
Role: {paragraph. Role}");


<!---- Page 389 ---------------------------------------------------------------------------------------------------------------------------------->
}
}
foreach (DocumentStyle style in result. Styles)
{
// Check the style and style confidence to see if text is handwritten.
// Note that value '0.8' is used as an example.
bool isHandwritten = style. IsHandwritten. HasValue && style. IsHandwritten
== true;
if (isHandwritten && style. Confidence > 0.8)
{
Console.WriteLine($"Handwritten content found:");
foreach (DocumentSpan span in style. Spans)
{
Console.WriteLine($" Content:
{result.Content. Substring(span. Index, span. Length) }");
}
}
}
Console. WriteLine("The following tables were extracted:");
for (int i = 0; i < result. Tables.Count; i++)
{
DocumentTable table = result. Tables[i];
Console.WriteLine($" Table {i} has {table. RowCount} rows and
{table. ColumnCount} columns.");
foreach (DocumentTableCell cell in table.Cells)
{
Console.WriteLine($" Cell ({cell. RowIndex}, {cell. ColumnIndex} )
has kind '{cell. Kind} ' and content: '{cell. Content} '.");
}
}
Visit the Azure samples repository on GitHub and view the layout model output.
Use the General document model
C#
using Azure;
using Azure.AI.DocumentIntelligence;
//use your `key` and `endpoint` environment variables to create your
`AzureKeyCredential` and ` DocumentIntelligenceClient' instances
string key = Environment. GetEnvironmentVariable( "DI_KEY");
string endpoint = Environment. GetEnvironmentVariable( "DI_ENDPOINT");


<!---- Page 390 ---------------------------------------------------------------------------------------------------------------------------------->
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient (new
Uri(endpoint), credential);
// sample document document
Uri fileUri = new Uri("https://raw.githubusercontent. com/Azure-
Samples/cognitive-services-REST-api-samples/master/curl/form-
recognizer/sample-layout.pdf");
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-document",
fileUri);
AnalyzeResult result = operation. Value;
Console.WriteLine("Detected key-value pairs:");
foreach (DocumentKeyValuePair kvp in result. KeyValuePairs)
{
if (kvp. Value == null)
{
Console.WriteLine($" Found key with no value:
'{kvp.Key. Content} '");
}
else
{
Console.WriteLine($" Found key-value pair: '{kvp. Key. Content}' and
'{kvp. Value. Content} '");
}
}
foreach (DocumentPage page in result. Pages)
{
Console.WriteLine($"Document Page {page. PageNumber} has
{page. Lines. Count} line(s), {page.Words.Count} word(s),");
Console.WriteLine($"and {page. SelectionMarks. Count} selection
mark(s).");
for (int i = 0; i < page. Lines.Count; i++)
{
DocumentLine line = page. Lines[i];
Console. WriteLine($" Line {i} has content: '{line. Content}'.") ;
Console.WriteLine($"
Its bounding polygon (points ordered
clockwise):");
for (int j = 0; j < line. Polygon. Count; j++)
{
Console.WriteLine($"
Point {j} => X: {line. Polygon[j] . X},
Y: {line.Polygon[j]. Y}");
}
}
for (int i = 0; i < page.SelectionMarks. Count; i++)
{


<!---- Page 391 ---------------------------------------------------------------------------------------------------------------------------------->
DocumentSelectionMark selectionMark = page. SelectionMarks[i];
Console.WriteLine($" Selection Mark {i} is
{selectionMark. State} . ");
Console. WriteLine($"
Its bounding polygon (points ordered
clockwise):");
for (int j = 0; j < selectionMark. Polygon. Count; j++)
{
Console.WriteLine($"
Point {j} => X:
{selectionMark. Polygon[j].X}, Y: {selectionMark. Polygon[j]. Y}");
}
}
}
foreach (DocumentStyle style in result.Styles)
{
// Check the style and style confidence to see if text is handwritten.
// Note that value '0.8' is used as an example.
bool isHandwritten = style. IsHandwritten. HasValue && style. IsHandwritten
== true;
if (isHandwritten && style. Confidence > 0.8)
{
Console.WriteLine($"Handwritten content found:");
foreach (DocumentSpan span in style. Spans)
{
Console. WriteLine($" Content:
{result.Content.Substring(span. Index, span. Length) }");
}
}
}
Console. WriteLine("The following tables were extracted:");
for (int i = 0; i < result. Tables. Count; i++)
{
DocumentTable table = result. Tables[i];
Console.WriteLine($" Table {i} has {table. RowCount} rows and
{table. ColumnCount} columns.");
foreach (DocumentTableCell cell in table. Cells)
{
Console.WriteLine($" Cell ({cell. RowIndex}, {cell. ColumnIndex} )
has kind '{cell. Kind}' and content: '{cell. Content} '.");
}
}
Visit the Azure samples repository on GitHub and view the general document model
output .


<!---- Page 392 ---------------------------------------------------------------------------------------------------------------------------------->
Use the W-2 tax model
C#
using Azure;
using Azure.AI. DocumentIntelligence;
//use your `key` and `endpoint` environment variables to create your
`AzureKeyCredential' and ' DocumentIntelligenceClient' instances
string key = Environment. GetEnvironmentVariable("DI_KEY");
string endpoint = Environment. GetEnvironmentVariable("DI_ENDPOINT");
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient (new
Uri(endpoint), credential);
/ / sample document document
Uri w2Uri = new Uri("https://raw.githubusercontent. com/Azure-
Samples/cognitive-services-REST-api-samples/master/curl/form-
recognizer/rest-api/w2.png");
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil.Completed, "prebuilt-tax.us.w2",
w2Uri);
AnalyzeResult result = operation. Value;
for (int i = 0; i < result.Documents.Count; i++)
{
Console.WriteLine($"Document {i}:");
AnalyzedDocument document = result.Documents[i];
if (document. Fields. TryGetValue("AdditionalInfo", out DocumentField?
additionalInfoField))
{
if (additionalInfoField. FieldType == DocumentFieldType. List)
{
foreach (DocumentField infoField in
additionalInfoField. Value. AsList())
{
Console.WriteLine("AdditionalInfo:");
if (infoField. FieldType == DocumentFieldType. Dictionary)
{
IReadOnlyDictionary<string, DocumentField> infoFields =
infoField.Value.AsDictionary();
if (infoFields. TryGetValue( "Amount", out DocumentField?
amountField))
{
if (amountField.FieldType ==
DocumentFieldType.Double)


<!---- Page 393 ---------------------------------------------------------------------------------------------------------------------------------->
{
double amount = amountField. Value. AsDouble();
Console. WriteLine($" Amount: '{amount} ', with
confidence {amountField. Confidence}");
}
}
if (infoFields. TryGetValue( "LetterCode", out
DocumentField? letterCodeField))
{
if (letterCodeField. FieldType ==
DocumentFieldType.String)
{
string letterCode =
letterCodeField. Value.AsString();
Console.WriteLine($" LetterCode:
' {letterCode} ' , with confidence {letterCodeField. Confidence}");
}
}
}
}
}
}
if (document. Fields. TryGetValue( "AllocatedTips", out DocumentField?
allocatedTipsField))
{
if (allocatedTipsField. FieldType == DocumentFieldType. Double)
{
double allocatedTips = allocatedTipsField. Value. AsDouble();
Console.WriteLine($"Allocated Tips: '{allocatedTips}', with
confidence {allocatedTipsField. Confidence}");
}
}
if (document.Fields. TryGetValue("Employer", out DocumentField?
employerField))
{
if (employerField.FieldType == DocumentFieldType.Dictionary)
{
IReadOnlyDictionary<string, DocumentField> employerFields =
employerField.Value.AsDictionary();
if (employerFields. TryGetValue( "Name", out DocumentField?
employerNameField))
{
if (employerNameField.FieldType == DocumentFieldType.String)
{
string name = employerNameField. Value. AsString();
Console. WriteLine($"Employer Name: ' {name}', with
confidence {employerNameField.Confidence}");


<!---- Page 394 ---------------------------------------------------------------------------------------------------------------------------------->
}
}
if (employerFields. TryGetValue("IdNumber", out DocumentField?
idNumberField))
{
if (idNumberField.FieldType == DocumentFieldType.String)
{
string id = idNumberField. Value. AsString();
Console.WriteLine($"Employer ID Number: '{id}', with
confidence {idNumberField.Confidence}");
}
}
if (employerFields. TryGetValue("Address", out DocumentField?
addressField))
{
if (addressField. FieldType == DocumentFieldType.Address)
{
Console.WriteLine($"Employer Address:
'{addressField. Content}', with confidence {addressField. Confidence}");
}
}
}
}
Visit the Azure samples repository on GitHub and view the W-2 tax model output .
}
Use the Invoice model
C#
using Azure;
using Azure.AI. DocumentIntelligence;
//use your ` key` and `endpoint` environment variables to create your
`AzureKeyCredential' and `DocumentIntelligenceClient' instances
string key = Environment. GetEnvironmentVariable("DI_KEY");
string endpoint = Environment. GetEnvironmentVariable( "DI_ENDPOINT");
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient(new
Uri(endpoint), credential);
// sample document document
Uri invoiceUri = new Uri("https : //github. com/Azure-Samples/cognitive-
services-REST-api-samples/raw/master/curl/form-recognizer/rest-
api/invoice.pdf");
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-invoice",


<!---- Page 395 ---------------------------------------------------------------------------------------------------------------------------------->
invoiceUri);
AnalyzeResult result = operation. Value;
for (int i = 0; i < result.Documents. Count; i++)
{
Console.WriteLine($"Document {i}:");
AnalyzedDocument document = result. Documents[i];
if (document.Fields. TryGetValue("VendorName", out DocumentField
vendorNameField))
{
if (vendorNameField.FieldType == DocumentFieldType. String)
{
string vendorName = vendorNameField. Value.AsString();
Console.WriteLine($"Vendor Name: '{vendorName}', with confidence
{vendorNameField.Confidence}");
}
}
if (document.Fields. TryGetValue("CustomerName", out DocumentField
customerNameField))
{
if (customerNameField. FieldType == DocumentFieldType. String)
{
string customerName = customerNameField.Value.AsString();
Console.WriteLine($"Customer Name: '{customerName}', with
confidence {customerNameField. Confidence}");
}
}
if (document.Fields. TryGetValue("Items", out DocumentField itemsField))
{
if (itemsField. FieldType == DocumentFieldType. List)
{
foreach (DocumentField itemField in itemsField.Value.AsList())
{
Console.WriteLine("Item:");
if (itemField. FieldType == DocumentFieldType.Dictionary)
{
IReadOnlyDictionary<string, DocumentField> itemFields =
itemField.Value.AsDictionary();
if (itemFields. TryGetValue( "Description", out
DocumentField itemDescriptionField))
{
if (itemDescriptionField. FieldType ==
DocumentFieldType.String)
{
string itemDescription =
itemDescriptionField.Value.AsString();
Console. WriteLine($" Description:


<!---- Page 396 ---------------------------------------------------------------------------------------------------------------------------------->
'{itemDescription} ', with confidence {itemDescriptionField. Confidence}");
}
}
if (itemFields. TryGetValue( "Amount", out DocumentField
itemAmountField))
{
if (itemAmountField.FieldType ==
DocumentFieldType.Currency)
{
CurrencyValue itemAmount =
itemAmountField.Value.AsCurrency();
Console.WriteLine($" Amount:
'{itemAmount . Symbol} {itemAmount. Amount}', with confidence
{itemAmountField.Confidence}");
}
}
}
}
}
}
if (document.Fields. TryGetValue("SubTotal", out DocumentField
subTotalField))
{
if (subTotalField. FieldType == DocumentFieldType. Currency)
{
CurrencyValue subTotal = subTotalField. Value. AsCurrency();
Console.WriteLine($"Sub Total: '{subTotal. Symbol}
{subTotal.Amount} ', with confidence {subTotalField. Confidence}");
}
}
if (document. Fields. TryGetValue("TotalTax", out DocumentField
totalTaxField))
{
if (totalTaxField. FieldType == DocumentFieldType. Currency)
{
CurrencyValue totalTax = totalTaxField. Value. AsCurrency();
Console.WriteLine($"Total Tax: '{totalTax. Symbol}
{totalTax. Amount} ', with confidence {totalTaxField. Confidence}");
}
}
if (document.Fields. TryGetValue("InvoiceTotal", out DocumentField
invoiceTotalField))
{
if (invoiceTotalField. FieldType == DocumentFieldType. Currency)
{
CurrencyValue invoiceTotal =
invoiceTotalField. Value. AsCurrency();
Console. WriteLine($"Invoice Total: '{invoiceTotal. Symbol}
{invoiceTotal. Amount} ', with confidence {invoiceTotalField. Confidence}");
}


<!---- Page 397 ---------------------------------------------------------------------------------------------------------------------------------->
}
}
Visit the Azure samples repository on GitHub and view the invoice model output.
Use the Receipt model
C#
using Azure;
using Azure.AI. DocumentIntelligence;
//use your `key` and `endpoint` environment variables to create your
`AzureKeyCredential' and `DocumentIntelligenceClient' instances
string key = Environment. GetEnvironmentVariable("DI_KEY");
string endpoint = Environment. GetEnvironmentVariable("DI_ENDPOINT");
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient(new
Uri(endpoint), credential);
// sample document document
Uri receiptUri = new Uri("https://raw.githubusercontent. com/Azure-
Samples/cognitive-services-REST-api-samples/master/curl/form-
recognizer/rest-api/receipt.png");
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-receipt",
receiptUri);
AnalyzeResult receipts = operation. Value;
foreach (AnalyzedDocument receipt in receipts. Documents)
{
if (receipt. Fields. TryGetValue("MerchantName", out DocumentField
merchantNameField))
{
if (merchantNameField.FieldType == DocumentFieldType.String)
{
string merchantName = merchantNameField. Value.AsString();
Console.WriteLine($"Merchant Name: '{merchantName}', with
confidence {merchantNameField.Confidence}");
}
}
if (receipt. Fields. TryGetValue( "TransactionDate", out DocumentField
transactionDateField))
{
if (transactionDateField. FieldType == DocumentFieldType. Date)
{
DateTimeOffset transactionDate =


<!---- Page 398 ---------------------------------------------------------------------------------------------------------------------------------->
transactionDateField. Value.AsDate();
Console. WriteLine($"Transaction Date: ' {transactionDate}', with
confidence {transactionDateField. Confidence}");
}
}
if (receipt. Fields. TryGetValue("Items", out DocumentField itemsField))
{
if (itemsField. FieldType == DocumentFieldType. List)
{
{
foreach (DocumentField itemField in itemsField. Value.AsList())
Console.WriteLine("Item:");
if (itemField. FieldType == DocumentFieldType.Dictionary)
{
IReadOnlyDictionary<string, DocumentField> itemFields =
itemField. Value.AsDictionary();
if (itemFields. TryGetValue( "Description", out
DocumentField itemDescriptionField))
{
if (itemDescriptionField. FieldType ==
DocumentFieldType.String)
{
string itemDescription =
itemDescriptionField.Value.AsString();
}
Console.WriteLine($" Description:
'{itemDescription} ', with confidence {itemDescriptionField. Confidence}");
}
if (itemFields. TryGetValue("TotalPrice", out
DocumentField itemTotalPriceField))
{
if (itemTotalPriceField. FieldType ==
DocumentFieldType.Double)
{
double itemTotalPrice =
itemTotalPriceField. Value.AsDouble();
Console.WriteLine($" Total Price:
'{itemTotalPrice}', with confidence {itemTotalPriceField. Confidence}");
}
}
}
}
}
}
if (receipt. Fields. TryGetValue("Total", out DocumentField totalField))
{
if (totalField. FieldType == DocumentFieldType. Double)


<!---- Page 399 ---------------------------------------------------------------------------------------------------------------------------------->
{
double total = totalField.Value. AsDouble();
Console.WriteLine($"Total: ' {total}', with confidence
'{totalField. Confidence} '");
}
}
}
Visit the Azure samples repository on GitHub and view the receipt model output .
Use the ID document model
C#
using Azure;
using Azure.AI. DocumentIntelligence;
//use your `key` and ` endpoint` environment variables to create your
`AzureKeyCredential` and `DocumentIntelligenceClient' instances
string key = Environment. GetEnvironmentVariable("DI_KEY");
string endpoint = Environment. GetEnvironmentVariable( "DI_ENDPOINT");
AzureKeyCredential credential = new AzureKeyCredential(key);
DocumentIntelligenceClient client = new DocumentIntelligenceClient(new
Uri(endpoint), credential);
// sample document document
Uri idDocumentUri = new Uri("https://raw.githubusercontent.com/Azure-
Samples/cognitive-services-REST-api-samples/master/curl/form-
recognizer/rest-api/identity_documents.png");
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-idDocument",
idDocumentUri);
AnalyzeResult identityDocuments = operation. Value;
AnalyzedDocument identityDocument = identityDocuments.Documents.Single();
if (identityDocument. Fields. TryGetValue("Address", out DocumentField
addressField))
{
if (addressField. FieldType == DocumentFieldType.String)
{
string address = addressField. Value. AsString();
Console.WriteLine($"Address: ' {address} ', with confidence
{addressField. Confidence}");
}
}


<!---- Page 400 ---------------------------------------------------------------------------------------------------------------------------------->
if (identityDocument. Fields. TryGetValue("CountryRegion", out DocumentField
countryRegionField))
{
if (countryRegionField. FieldType == DocumentFieldType. CountryRegion)
{
string countryRegion = countryRegionField. Value. AsCountryRegion();
Console.WriteLine($"CountryRegion: '{countryRegion}', with
confidence {countryRegionField.Confidence}");
}
}
if (identityDocument. Fields. TryGetValue("DateOfBirth", out DocumentField
dateOfBirthField))
{
if (dateOfBirthField. FieldType == DocumentFieldType.Date)
{
DateTimeOffset dateOfBirth = dateOfBirthField. Value.AsDate();
Console.WriteLine($"Date Of Birth: '{dateOfBirth}', with confidence
{dateOfBirthField. Confidence}");
}
}
if (identityDocument. Fields. TryGetValue("DateOfExpiration", out
DocumentField dateOfExpirationField))
{
if (dateOfExpirationField.FieldType == DocumentFieldType.Date)
}
{
DateTimeOffset dateOfExpiration =
dateOfExpirationField.Value.AsDate();
Console.WriteLine($"Date Of Expiration: '{dateOfExpiration}', with
confidence {dateOfExpirationField.Confidence}");
}
if (identityDocument. Fields. TryGetValue( "DocumentNumber", out DocumentField
documentNumberField))
{
if (documentNumberField.FieldType == DocumentFieldType.String)
{
string documentNumber = documentNumberField.Value.AsString();
Console.WriteLine($"Document Number: '{documentNumber}', with
confidence {documentNumberField.Confidence}");
}
}
if (identityDocument. Fields. TryGetValue("FirstName", out DocumentField
firstNameField))
{
if (firstNameField. FieldType == DocumentFieldType. String)
{
string firstName = firstNameField.Value.AsString();
Console. WriteLine($"First Name: ' {firstName}', with confidence
{firstNameField.Confidence}");
}


<!---- Page 401 ---------------------------------------------------------------------------------------------------------------------------------->
}
if (identityDocument. Fields. TryGetValue("LastName", out DocumentField
lastNameField))
{
if (lastNameField. FieldType == DocumentFieldType.String)
{
string lastName = lastNameField. Value. AsString();
Console.WriteLine($"Last Name: ' {lastName}', with confidence
{lastNameField.Confidence}");
}
}
if (identityDocument. Fields. TryGetValue("Region", out DocumentField
regionfield))
{
if (regionfield. FieldType == DocumentFieldType.String)
{
string region = regionfield. Value. AsString();
Console.WriteLine($"Region: '{region}', with confidence
{regionfield.Confidence}");
}
}
if (identityDocument.Fields. TryGetValue("Sex", out DocumentField sexfield))
{
if (sexfield. FieldType == DocumentFieldType.String)
{
string sex = sexfield. Value. AsString();
Console. WriteLine($"Sex: '{sex}', with confidence
{sexfield.Confidence}");
}
}
Visit the Azure samples repository on GitHub and view the ID document model
output .
Next steps
Congratulations! You learned to use Document Intelligence models to analyze various
documents in different ways. Next, explore the Document Intelligence Studio and
reference documentation.
Try the Document Intelligence Studio
Explore the Document Intelligence REST API


<!---- Page 402 ---------------------------------------------------------------------------------------------------------------------------------->
Feedback
Was this page helpful?
No
Provide product feedback z | Get help at Microsoft Q&A
Yes


<!---- Page 403 ---------------------------------------------------------------------------------------------------------------------------------->
Check usage and estimate cost
Article · 04/07/2025
This content applies to:
v4.0 (GA)
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
In this guide, learn how to use the metrics dashboard in the Azure portal to view how many
pages are processed. You also learn how to estimate the cost of processing those pages using
the Azure pricing calculator.
Check how many pages were processed
We start by looking at the page processing data for a given time period:
1. Sign in to the Azure portal 2 .
2. Navigate to your Document Intelligence resource.
3. From the Overview page, select the Monitoring tab located near the middle of the page.
Search (Ctrl+/)
«
Delete
Overview
Activity log
Help us improve Form Recognizer. Take our survey!
Access control (IAM)
Tags
Essentials
Diagnose and solve problems
Get Started
Monitoring
Resource Management
Show data for the last
15 minutes
1 hour
1 day
1 week
30 days
4. Select a time range and you see the Processed Pages chart displayed.
Requests
Errors
Request latency
Processed Pages
100
90
12
180ms
9
8
80
160ms
7
70
10
140ms
6
60
1
120ms
5
50
100ms
40
6
80ms
4
30
4
60ms
3
20
2
40rms
2
10
20ms
1
+
0
0
0mms
0
Jun 19
Jun 26
Total Errors (Surr)
Jul 3
Jul 10
UTC-07:00
Jun 19
Jun 26
Jul 3
Jul 10
UTC-07:00
Jun 19
Jan 26
Jul 3
Jul 10
UTC-07:00
Jun 19
Jun 26
Jul 3
Jul 10
UTC-07:00
Total Cala (Sum)
Blocked Cals (Sur)
Rengiorne code: 429
Client Enors (Sur)
Reporte casde: 4xx:
Server Emora (Sur)
Response code Sex
Latency [lug)
Processed Pages (Sur)
164
12
12
12
0
113.2m
20
Examine analyzed pages
We can now take a deeper dive to see each model's analyzed pages:
1. Under the Monitoring section, select Metrics from the left pane.


<!---- Page 404 ---------------------------------------------------------------------------------------------------------------------------------->
Monitoring
!
Alerts
Metrics
Diagnostic settings
Logs
2. On the Metrics page, select Add metric.
3. Select the Metric dropdown menu and, under USAGE, choose Processed Pages.
₩ Add metric
Add filter : Apply splitting
Line chart
2
Scope
Metric Namespace
Metric
Aggregation
form-recognizer
Cognitive Service stand ...
Select metric
Select aggregation
N Data Out
100
Latency
90
Server Errors
Successful Calls
80
Total Calls
70
Total Errors
or learn more below:
60
USAGE
Processed Pages
50
4. From the upper right corner, configure the time range and select the Apply button.
New chart
Refresh
Share
Feedback
Local Time: Last 4 hours (Automatic - 1 minute)
Sum Processed Pages for form-recognizer
Time range
Time granularity
Automatic
Add metric
Add filter
Apply splitting
Line chart
Drill
Last 30 minutes
Last 48 hours
Le
Last hour
Last 3 days
Show time as
UTC/GMT
form-recognizer, Processed Pages, Sum
Last 4 hours
Last 7 days
100
Last 12 hours
Last 30 days
Local
90
Last 24 hours
Custom
80
+
70
Apply
Cancel
60
5. Select Apply splitting.
Sum Processed Pages for form-recognizer
Add metric
Add filter
Apply splitting
form-recognizer, Processed Pages, Sum
×


<!---- Page 405 ---------------------------------------------------------------------------------------------------------------------------------->
6. Choose FeatureName from the Values dropdown menu.
Add metric
Add filter
Apply splitting
Values
Limit
form-recognizer, Processed Pages, Sum
×
Select value(s)
V
10
ApiName
100
FeatureName
90
Region
80
UsageChannel
7. You see a breakdown of the pages analyzed by each model.
৳ Add metric
Add filter ; Apply splitting
Line chart
2
Drill into Logs
!
New alert rule
H
Save to dashboard
...
Values
Limit
Sort
~
Processed Pages, Sum
x
FeatureName
v
10
Descending
v
ApiName
FeatureName
2.20
Region
UsageChannel
2
1.80
1.60
1.40
1.20
1
0.80
0.60
0.40
0.20
0
Jun 12
Jun 19
Jun 26
Receipts
Jul 03
UTC+08:00
Invoices
Layout
Business cards
IDs
Documents
+
7
7
5
5
3
1
Estimate price
Now that we have the page processed data from the portal, we can use the Azure pricing
calculator to estimate the cost:
1. Sign in to Azure pricing calculator [ with the same credentials you use for the Azure
portal.


<!---- Page 406 ---------------------------------------------------------------------------------------------------------------------------------->
Press Ctrl + right-click to open in a new tab!
2. Search for Azure AI Document Intelligence in the Search products search box.
3. Select Azure AI Document Intelligence and you see it was added to the page.
4. Under Your Estimate, select the relevant Region, Payment Option, and Instance for your
Document Intelligence resource. For more information, see Azure AI Document
Intelligence pricing options [ .
5. Enter the number of pages processed from the Azure portal metrics dashboard. That data
can be found using the steps in sections Check how many pages are processed or
Examine analyzed pages.
6. The estimated price is on the right page section, after the equal (=) sign.
Form Recognizer
REGION:
PAYMENT OPTION:
INSTANCE:
West US
v
Pay as you go
-
SO
V
Custom
<number of pages>
x
$ <price>
Per 1,000 pages
=
$ <subtotal>
Pages
Pre-built (S1)
O
The prebuilt SKU includes the following Form Recognizer APIs: Layout, Receipt, Business Card, ID, invoice, Document, and W-2.
<number of pages>
x
$ <price>
Per 1,000 pages
= $ <subtotal>
Pages
Upfront cost
$ <price>
Monthly cost
$ <total>
Azure Form Recognizer
®
Azure Form Recognizer, Pay as you go, Free: Up to ...
G
O
Upfront: $0.00
Monthly: $
That's it. You now know where to find how many pages you process using Document
Intelligence and how to estimate the cost.
Next steps
Learn more about Document Intelligence service quotas and limits


<!---- Page 407 ---------------------------------------------------------------------------------------------------------------------------------->
Create SAS tokens for storage
containers
Article · 12/11/2024
This content applies to:
v4.0 (GA)
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
In this article, learn how to create user delegation, shared access signature (SAS) tokens,
using either the Azure portal or Azure Storage Explorer. User delegation SAS tokens are
secured with Microsoft Entra credentials. SAS tokens provide secure, delegated access to
resources in your Azure storage account.
https://storagesample.blob.core.windows.net/sample-container/sampleBlob.pdf ? sv=2023-11-31&sr=b&sig=39Up9jzHkxhUIhFEJEh9594DJxe6cIRCgOv6ICGSØ%3A37&sp=rcw
Storage resource URI
Delimiter character
SAS token
At a high level, here's how SAS tokens work:
. First, your application submits the SAS token to Azure Storage as part of a REST
API request.
. Next, if the storage service verifies that the SAS is valid, the request is authorized.
If, the SAS token is deemed invalid, the request is declined and the error code 403
(Forbidden) is returned.
Azure Blob Storage offers three resource types:
· Storage accounts provide a unique namespace in Azure for your data.
· Data storage containers are located in storage accounts and organize sets of
blobs.
· Blobs are located in containers and store text and binary data such as files, text,
and images.
When to use a SAS token
· Training custom models. Your assembled set of training documents must be
uploaded to an Azure Blob Storage container. You can opt to use a SAS token to
grant access to your training documents.
. Using storage containers with public access. You can opt to use a SAS token to
grant limited access to your storage resources that have public read access.
1 Important

| https://storagesample.blob.core.windows.net/sample-container/sampleBlob.pdf ? | sv=2023-11-31&sr=b&sig=39Up9jzHkxhUIhFEJEh9594DJxe6cIRCgOv6ICGSØ%3A37&sp=rcw |  |
| --- | --- | --- |
| Storage resource URI Delimiter | character | SAS token |


<!---- Page 408 ---------------------------------------------------------------------------------------------------------------------------------->
o If your Azure storage account is protected by a virtual network or firewall,
you can't grant access with a SAS token. You'll have to use a managed
identity to grant access to your storage resource.
· Managed identity supports both privately and publicly accessible Azure
Blob Storage accounts.
· SAS tokens grant permissions to storage resources, and should be
protected in the same manner as an account key.
o Operations that use SAS tokens should be performed only over an HTTPS
connection, and SAS URIs should only be distributed on a secure
connection such as HTTPS.
Prerequisites
To get started, you need:
. An active Azure account . If you don't have one, you can create a free account .
. A Document Intelligence or multi-service resource.
. A standard performance Azure Blob Storage account . You need to create
containers to store and organize your blob data within your storage account. If you
don't know how to create an Azure storage account with a storage container,
follow these quickstarts:
o Create a storage account. When you create your storage account, select
Standard performance in the Instance details > Performance field.
o Create a container. When you create your container, set Public access level to
Container (anonymous read access for containers and blobs) in the New
Container window.
Upload your documents
1. Sign in to the Azure portal 27 .
. Select Your storage account - Data storage - Containers.


<!---- Page 409 ---------------------------------------------------------------------------------------------------------------------------------->
Data storage
Containers
File shares
Queues
Tables
2. Select a container from the list.
3. Select Upload from the menu at the top of the page.
₸ Upload
Change access level
Refresh
Delete
₹
Change tier
o
Acquire lease
Break lease
View snapshots
Create snapshot
4. The Upload blob window appears. Select your files to upload.
Upload blob
form-rec-invoice-with-labels/
Files
"invoice_1.pdf" "invoice_2.pdf" "inv ...
Overwrite if files already exi
invoice_1.pdf
invoice_2.pdf
invoice_3.pdf
V Advanced
Upload
Note
By default, the REST API uses documents located at the root of your container.
You can also use data organized in subfolders if specified in the API call. For
more information, see Organize your data in subfolders.
Generating SAS tokens
Once the prerequisites are met and you upload your documents, you can now generate
SAS tokens. There are two paths you can take from here; one using the Azure portal and
the other using the Azure storage explorer. Select between the two following tabs for
more information.
Azure Portal


<!---- Page 410 ---------------------------------------------------------------------------------------------------------------------------------->
The Azure portal is a web-based console that enables you to manage your Azure
subscription and resources using a graphical user interface (GUI).
1. Sign in to the Azure portal 2 .
2. Navigate to Your storage account > containers > your container.
3. Select Generate SAS from the menu near the top of the page.
4. Select Signing method -> User delegation key.
5. Define Permissions by selecting or clearing the appropriate checkbox.
. Make sure the Read, Write, Delete, and List permissions are selected.
Generate SAS
X
A shared access signature (SAS) is a URI that grants restricted access to an Azure Storage
container. Use it when you want to grant access to storage account resources for a specific
time range without sharing your storage account key. Learn more
Signing method
Account key
User delegation key
Permissions *
4 selected
V
Read
0
Add
Create
2:19:14 PM
Write
US & Canada)
V
Delete
List
10:19:14 PM
(UTC-08:00) Pacific Time (US & Canada)
V
Allowed IP addresses
for example, 168.1.5.65 or 168.1.5.65-168.1 ...
Allowed protocols
HTTPS only
HTTPS and HTTP
Generate SAS token and URL
1 Important
· If you receive a message similar to the following one, you'll also
need to assign access to the blob data in your storage account:


<!---- Page 411 ---------------------------------------------------------------------------------------------------------------------------------->
!
You don't have permissions to grant read, write, delete access. You can still create a shared
access signature, but you'll need an RBAC role with additional permissions before you can
grant that level of access to your signature recipient.
Learn more about Azure roles for access to blob data ['
· Azure role-based access control (Azure RBAC) is the authorization
system used to manage access to Azure resources. Azure RBAC
helps you manage access and permissions for your Azure resources.
. Assign an Azure role for access to blob data to assign a role that
allows for read, write, and delete permissions for your Azure storage
container. See Storage Blob Data Contributor.
6. Specify the signed key Start and Expiry times.
· When you create a SAS token, the default duration is 48 hours. After 48
hours, you'll need to create a new token.
· Consider setting a longer duration period for the time you're using your
storage account for Document Intelligence Service operations.
· The value of the expiry time is determined by whether you're using an
Account key or User delegation key Signing method:
o Account key: No imposed maximum time limit; however, best
practices recommended that you configure an expiration policy to
limit the interval and minimize compromise. Configure an expiration
policy for shared access signatures.
o User delegation key: The value for the expiry time is a maximum of
seven days from the creation of the SAS token. The SAS is invalid after
the user delegation key expires, so a SAS with an expiry time of
greater than seven days will still only be valid for seven days. For more
information, see Use Microsoft Entra credentials to secure a SAS.
7. The Allowed IP addresses field is optional and specifies an IP address or a
range of IP addresses from which to accept requests. If the request IP address
doesn't match the IP address or address range specified on the SAS token,
authorization fails. The IP address or a range of IP addresses must be public
IPs, not private. For more information, see, Specify an IP address or IP range.
8. The Allowed protocols field is optional and specifies the protocol permitted
for a request made with the SAS token. The default value is HTTPS.
9. Select Generate SAS token and URL.


<!---- Page 412 ---------------------------------------------------------------------------------------------------------------------------------->
10. The Blob SAS token query string and Blob SAS URL appear in the lower area
of the window. To use the Blob SAS token, append it to a storage service URI.
11. Copy and paste the Blob SAS token and Blob SAS URL values in a secure
location. The values are displayed only once and can't be retrieved after the
window is closed.
12. To construct a SAS URL, append the SAS token (URI) to the URL for a storage
service.
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 413 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence v4.0 migration
Article · 02/09/2025
1 Important
Document Intelligence REST API v4.0 introduces breaking changes in the REST API request and analyze response JSON.
Migrating from v3.1 to v4.0
Preview APIs are periodically deprecated. If you're using a preview API version, update your application to target the GA API version. To migrate
from a preview API version to the 2024-11-30 (GA) API version using the SDK, update to the current version of the language specific SDK.
Analysis features
Expand table
Model ID
Text
Extraction
Paragraphs
Paragraph
Roles
Selection
Marks
Tables
Key-
Value
Pairs
Languages
Barcodes
Document
Analysis
Formulas*
StyleFont*
OCR
Resol
prebuilt-read
✓
✓
O
O
O
O
prebuilt-layout
✓
✓
✓
✓
✓
O
O
O
O
prebuilt-document
✓
✓
✓
✓
✓
✓
O
O
O
O
prebuilt-businessCard
✓
✓
prebuilt-idDocument
✓
O
O
✓
O
O
prebuilt-invoice
✓
✓
✓
O
O
O
✓
O
O
prebuilt-receipt
✓
O
O
✓
O
O
prebuilt-
healthInsuranceCard.us
✓
O
O
✓
O
O
prebuilt-tax.us.w2
✓
✓
O
O
✓
O
O
prebuilt-tax.us.1098
✓
✓
O
O
✓
O
O
prebuilt-tax.us.1098E
✓
✓
O
O
✓
O
O
prebuilt-tax.us.1098T
✓
✓
O
O
✓
O
O
prebuilt-contract
✓
✓
✓
✓
O
O
✓
O
O
{ customModelName }
✓
✓
✓
✓
✓
O
O
✓
O
O
V - Enabled O - Optional Formulas/StyleFont/OCR High Resolution* - Premium features incur added costs
Migrating from v3.0
Compared with v3.0, Document Intelligence v3.1 introduces several new features and capabilities:
· Barcode extraction.
· Add-on capabilities including high resolution, formula, and font properties extraction.
· Custom classification model for document splitting and classification.
. Language expansion and new fields support in Invoice and Receipt model.
. New document type support in ID document model.
· New prebuilt Health insurance card model.
· Office/HTML files are supported in prebuilt-read model, extracting words and paragraphs without bounding boxes. Embedded images are
no longer supported. If add-on features are requested for Office/HTML files, an empty array is returned without errors.
. Model expiration for custom extraction and classification models - Our new custom models build upon on a large base model that we
update periodically for quality improvement. An expiration date is introduced to all custom models to enable the retirement of the
corresponding base models. Once a custom model expires, you need to retrain the model using the latest API version (base model).

| Model ID | Text Extraction | Paragraphs | Paragraph Roles | Selection Marks | Tables | Key- Value Pairs | Languages | Barcodes | Document Analysis | Formulas* | StyleFont* | OCR Resol |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| prebuilt-read | :selected: ✓ | :selected: ✓ | :unselected: |  |  |  | O :unselected: | :unselected: O |  | O :unselected: | O :unselected: |  |
| prebuilt-layout | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ |  | :unselected: O | :unselected: O |  | :unselected: O | O :unselected: |  |
| prebuilt-document | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ | ✓ :selected: | :unselected: O | :unselected: O |  | :unselected: O | O :unselected: |  |
| prebuilt-businessCard | :selected: ✓ | :unselected: |  |  |  |  |  |  | :selected: ✓ |  |  |  |
| prebuilt-idDocument | :selected: ✓ | :unselected: |  |  |  |  | O :unselected: | O :unselected: | :selected: ✓ | O :unselected: | O :unselected: |  |
| prebuilt-invoice | :selected: ✓ | :unselected: | :unselected: | :selected: ✓ | :selected: ✓ | O :unselected: | O :unselected: | :unselected: O | :selected: ✓ | :unselected: O | O :unselected: |  |
| prebuilt-receipt | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O | O :unselected: |  |
| prebuilt- healthInsuranceCard.us | :selected: ✓ |  |  |  |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O | O :unselected: |  |
| prebuilt-tax.us.w2 | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O | O :unselected: |  |
| prebuilt-tax.us.1098 | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O | O :unselected: |  |
| prebuilt-tax.us.1098E | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O | :unselected: O |  |
| prebuilt-tax.us.1098T | :selected: ✓ |  |  | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O | :unselected: O |  |
| prebuilt-contract | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ |  |  | :unselected: O | :unselected: O | :selected: ✓ | :unselected: O | :unselected: O |  |
| { customModelName } | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ | :selected: ✓ |  | :unselected: O | O :unselected: | :selected: ✓ | :unselected: O | O :unselected: |  |


<!---- Page 414 ---------------------------------------------------------------------------------------------------------------------------------->
HTTP
GET /documentModels/{customModelId}?api-version={apiVersion}
{
"modelId": "{customModelId}",
"description": "{customModelDescription}",
"createdDateTime": "2023-09-24T12: 54:35Z",
"expirationDateTime": "2025-01-01T00:00:00Z",
"apiVersion": "2023-07-31",
"docTypes": { ... }
}
· Custom neural model build quota - The number of neural models each subscription can build per region every month is limited. We expand
the result JSON to include the quota and used information to help you understand the current usage as part of the resource information
returned by GET /info.
HTTP
{
"customDocumentModels": { ... },
"customNeuralDocumentModelBuilds": {
"used": 1,
"quota": 10,
"quotaResetDateTime": "2023-03-01T00:00:00Z"
}
}
· An optional features query parameter to Analyze operations can optionally enable specific features. Some premium features can incur
added billing. Refer to Analyze feature list for details.
· Extend extracted currency field objects to output a normalized currency code field when possible. Currently, current fields can return amount
(ex. 123.45) and currencySymbol (ex. $). This feature maps the currency symbol to a canonical ISO 4217 code (ex. USD). The model can
optionally utilize the global document content to disambiguate or infer the currency code.
HTTP
{
"fields": {
"Total": {
"type": "currency",
"content": "$123.45",
"valueCurrency": {
"amount": 123.45,
"currencySymbol": "$",
"currencyCode": "USD"
},
}
}
}
Besides model quality improvement, you're highly recommended to update your application to use v3.1 to benefit from these new capabilities.
Migrating from v2.1 or v2.0
Document Intelligence v3.1 is the latest GA version with the richest features, most languages and document types coverage, and improved model
quality. Refer to model overview for the features and capabilities available in v3.1.
Starting from v3.0, Document Intelligence REST API is redesigned for better usability. In this section, learn the differences between Document
Intelligence v2.0, v2.1 and v3.1 and how to move to the newer version of the API.
& Caution
· REST API 2023-07-31 release includes a breaking change in the REST API analyze response JSON.
. The boundingBox property is renamed to polygon in each instance.
Changes to the REST API endpoints


<!---- Page 415 ---------------------------------------------------------------------------------------------------------------------------------->
The v3.1 REST API combines the analysis operations for layout analysis, prebuilt models, and custom models into a single pair of operations by
assigning documentModels and modelId to the layout analysis and prebuilt models.
POST request
HTTP
https://{your-form-recognizer-endpoint}/formrecognizer/documentModels/{modelId}?api-version=2023-07-31
GET request
HTTP
https://{your-form-recognizer-endpoint}/formrecognizer/documentModels/{modelId}/AnalyzeResult/{resultId}?api-version=2023-07-31
Analyze operation
· The request payload and call pattern remain unchanged.
· The Analyze operation specifies the input document and content-specific configurations, it returns the analyzed result URL via the
Operation-Location header in the response.
· Poll the Analyze Result URL, via a GET request to check the status of the Analyze operation (minimum recommended interval between
requests is 1 second).
. Upon success, status is set to succeeded and analyzeResult is returned in the response body. If errors are encountered, status sets to failed,
and an error is returned.
" Expand table
Model
v2.0
v2.1
v3.1
Request URL
prefix
https://{your-form-recognizer-
endpoint}/formrecognizer/v2.0
https://{your-form-recognizer-
endpoint}/formrecognizer/v2.1
https://{your-form-recognizer-
endpoint}/formrecognizer
General
document
N/A
N/A
/documentModels/prebuilt-document : analyze
Layout
/layout/analyze
/layout/analyze
/documentModels/prebuilt-layout : analyze
Custom
/custom/models/{modelId}/analyze
/custom/{modelId}/analyze
/documentModels/ {modelId}: analyze
Invoice
N/A
/prebuilt/invoice/analyze
/documentModels/prebuilt-invoice:analyze
Receipt
/prebuilt/receipt/analyze
/prebuilt/receipt/analyze
/documentModels/prebuilt-receipt:analyze
ID document
N/A
/prebuilt/idDocument/analyze
/documentModels/prebuilt-idDocument : analyze
Business card
N/A
/prebuilt/businessCard/analyze
/documentModels/prebuilt-
businessCard: analyze
W-2
N/A
N/A
/documentModels/prebuilt-tax.us.w2:analyze
Health
insurance card
N/A
N/A
/documentModels/prebuilt-
healthInsuranceCard. us :analyze
Contract
N/A
N/A
/documentModels/prebuilt-contract: analyze
Analyze request body
The content to be analyzed is provided via the request body. Either the URL or base64 encoded data can be user to construct the request.
To specify a publicly accessible web URL, set Content-Type to application/json and send the following JSON body:
JSON
{
"urlSource": "{urlPath}"

| Model | v2.0 | v2.1 | v3.1 |
| --- | --- | --- | --- |
| Request URL prefix | https://{your-form-recognizer- endpoint}/formrecognizer/v2.0 | https://{your-form-recognizer- endpoint}/formrecognizer/v2.1 | https://{your-form-recognizer- endpoint}/formrecognizer |
| General document | N/A | N/A | /documentModels/prebuilt-document : analyze |
| Layout | /layout/analyze | /layout/analyze | /documentModels/prebuilt-layout : analyze |
| Custom | /custom/models/{modelId}/analyze | /custom/{modelId}/analyze | /documentModels/ {modelId}: analyze |
| Invoice | N/A | /prebuilt/invoice/analyze | /documentModels/prebuilt-invoice:analyze |
| Receipt | /prebuilt/receipt/analyze | /prebuilt/receipt/analyze | /documentModels/prebuilt-receipt:analyze |
| ID document | N/A | /prebuilt/idDocument/analyze | /documentModels/prebuilt-idDocument : analyze |
| Business card | N/A | /prebuilt/businessCard/analyze | /documentModels/prebuilt- businessCard: analyze |
| W-2 | N/A | N/A | /documentModels/prebuilt-tax.us.w2:analyze |
| Health insurance card | N/A | N/A | /documentModels/prebuilt- healthInsuranceCard. us :analyze |
| Contract | N/A | N/A | /documentModels/prebuilt-contract: analyze |


<!---- Page 416 ---------------------------------------------------------------------------------------------------------------------------------->
}
Base 64 encoding is also supported in Document Intelligence v3.0:
{
"base64Source": "{base64EncodedContent}"
}
JSON
Additionally supported parameters
Parameters that continue to be supported:
· pages : Analyze only a specific subset of pages in the document. List of page numbers indexed from the number 1 to analyze. Ex. "1-3,5,7-9"
. locale : Locale hint for text recognition and document analysis. Value can contain only the language code (ex. en, fr ) or BCP 47 language
tag (ex. "en-US").
Parameters no longer supported:
· include TextDetails
The new response format is more compact and the full output is always returned.
Changes to analyze result
Analyze response is refactored to the following top-level results and supports multi-page elements.
· pages
· tables
· keyValuePairs
· entities
· styles
· documents
D Note
O
The analyzeResult response changes include changes such as moving up from a property of pages to a top lever property within
analyzeResult.
JSON
{
// Basic analyze result metadata
"apiVersion": "2022-08-31", // REST API version used
"modelId": "prebuilt-invoice", // ModelId used
"stringIndexType": "textElements", // Character unit used for string offsets and lengths:
// textElements, unicodeCodePoint, utf16CodeUnit // Concatenated content in global reading order across pages.
// Words are generally delimited by space, except CJK (Chinese, Japanese, Korean) characters.
// Lines and selection marks are generally delimited by newline character.
// Selection marks are represented in Markdown emoji syntax (:selected:, :unselected:).
"content": "CONTOSO LTD. \nINVOICE\nContoso Headquarters ... ", "pages": [ // List of pages analyzed
{
// Basic page metadata
"pageNumber": 1, // 1-indexed page number
"angle": 0, // Orientation of content in clockwise direction (degree)
"width": 0, // Page width
"height": 0, // Page height
"unit": "pixel", // Unit for width, height, and polygon coordinates
"spans": [ // Parts of top-level content covered by page
{
"offset": 0, // Offset in content
"length": 7 // Length in content
}
], // List of words in page
"words": [
{
"text": "CONTOSO", // Equivalent to $.content. Substring(span.offset, span. length)


<!---- Page 417 ---------------------------------------------------------------------------------------------------------------------------------->
"boundingBox": [ ... ], // Position in page
"confidence": 0.99, // Extraction confidence
"span": { ... } // Part of top-level content covered by word
}, ...
], // List of selectionMarks in page
"selectionMarks": [
{
"state": "selected", // Selection state: selected, unselected
"boundingBox": [ ... ], // Position in page
"confidence": 0.95, // Extraction confidence
"span": { ... } // Part of top-level content covered by selection mark
}, ...
], // List of lines in page
"lines": [
{
"content": "CONTOSO LTD.", // Concatenated content of line (may contain both words and selectionMarks)
"boundingBox": [ ... ], // Position in page
"spans": [ ... ], // Parts of top-level content covered by line
}, ...
]
}, ...
], // List of extracted tables
"tables": [
{
"rowCount": 1, // Number of rows in table
"columnCount": 1, // Number of columns in table
"boundingRegions": [ // Polygons or Bounding boxes potentially across pages covered by table
{
"pageNumber": 1, // 1-indexed page number
"polygon": [ ... ], // Previously Bounding box, renamed to polygon in the 2022-08-31 API
}
1,
"spans": [ ... ], // Parts of top-level content covered by table // List of cells in table
"cells": [
{
"kind": "stub", // Cell kind: content (default), rowHeader, columnHeader, stub, description
"rowIndex": 0, // 0-indexed row position of cell
"columnIndex": 0, // 0-indexed column position of cell
"rowSpan": 1, // Number of rows spanned by cell (default=1)
"columnSpan": 1, // Number of columns spanned by cell (default=1)
"content": "SALESPERSON", // Concatenated content of cell
"boundingRegions": [ ... ], // Bounding regions covered by cell
"spans": [ ... ] // Parts of top-level content covered by cell
}, ...
]
}, ...
], // List of extracted key-value pairs
"keyValuePairs": [
{
"key": { // Extracted key
"content": "INVOICE:", // Key content
"boundingRegions": [ ... ], // Key bounding regions
"spans": [ ... ] // Key spans
},
"value": { // Extracted value corresponding to key, if any
"content": "INV-100", // Value content
"boundingRegions": [ ... ], // Value bounding regions
"spans": [ ... ] // Value spans
},
"confidence": 0.95 // Extraction confidence
}, ...
],
"styles": [
{
"isHandwritten": true, // Is content in this style handwritten?
"spans": [ ... ], // Spans covered by this style
"confidence": 0.95 // Detection confidence
}, ...
], // List of extracted documents
"documents": [
{
"docType": "prebuilt-invoice", // Classified document type (model dependent)
"boundingRegions": [ ... ], // Document bounding regions
"spans": [ ... ], // Document spans
"confidence": 0.99, // Document splitting/classification confidence // List of extracted fields
"fields": {
"VendorName": { // Field name (docType dependent)
"type": "string", // Field value type: string, number, array, object, ...
"valueString": "CONTOSO LTD.",// Normalized field value
"content": "CONTOSO LTD.", // Raw extracted field content
"boundingRegions": [ ... ], // Field bounding regions
"spans": [ ... ], // Field spans


<!---- Page 418 ---------------------------------------------------------------------------------------------------------------------------------->
"confidence": 0.99 // Extraction confidence
}, ...
}
}, ...
]
}
Build or train model
The model object has three updates in the new API
. modelId is now a property that can be set on a model for a human readable name.
. modelName is renamed to description
· buildMode is a new property with values of template for custom form models or neural for custom neural models.
The build operation is invoked to train a model. The request payload and call pattern remain unchanged. The build operation specifies the model
and training dataset, it returns the result via the Operation-Location header in the response. Poll this model operation URL, via a GET request to
check the status of the build operation (minimum recommended interval between requests is 1 second). Unlike v2.1, this URL isn't the resource
location of the model. Instead, the model URL can be constructed from the given modelId, also retrieved from the resourceLocation property in
the response. Upon success, status is set to succeeded and result contains the custom model info. If errors are encountered, status is set to
failed, and the error is returned.
The following code is a sample build request using a SAS token. Note the trailing slash when setting the prefix or folder path.
JSON
POST https://{your-form-recognizer-endpoint}/formrecognizer/documentModels:build?api-version=2022-08-31
{
"modelId": {modelId},
"description": "Sample model",
"buildMode": "template",
"azureBlobSource": {
"containerUrl": "https://{storageAccount} . blob. core.windows. net/{containerName} ? {sasToken}",
"prefix": "{folderName/}"
}
}
Changes to compose model
Model compose is now limited to single level of nesting. Composed models are now consistent with custom models with the addition of modelId
and description properties.
JSON
POST https://{your-form-recognizer-endpoint}/formrecognizer/documentModels:compose?api-version=2022-08-31
{
"modelId": "{composedModelId}",
"description": "{composedModelDescription}",
"componentModels": [
{ "modelId": "{modelId1}" },
{ "modelId": " {modelId2}" },
]
}
Changes to copy model
The call pattern for copy model remains unchanged:
· Authorize the copy operation with the target resource calling authorizeCopy. Now a POST request.
. Submit the authorization to the source resource and copy the model calling copyTo
· Poll the returned operation to validate the operation completed successfully
The only changes to the copy model function are:


<!---- Page 419 ---------------------------------------------------------------------------------------------------------------------------------->
. HTTP action on the authorizeCopy is now a POST request.
. The authorization payload contains all the information needed to submit the copy request.
Authorize the copy
JSON
POST https://{targetHost}/formrecognizer/documentModels:authorizeCopy?api-version=2022-08-31
{
"modelId": "{targetModelId}",
"description": "{targetModelDescription}",
}
Use the response body from the authorize action to construct the request for the copy.
JSON
POST https://{sourceHost}/formrecognizer/documentModels/{sourceModelId}:copyTo?api-version=2022-08-31
{
"targetResourceId": "{targetResourceId}",
"targetResourceRegion": "{targetResourceRegion}",
"targetModelId": "{targetModelId}",
"targetModelLocation": "https://{targetHost}/formrecognizer/documentModels/{targetModelId}",
"accessToken": "{accessToken}",
"expirationDateTime": "2021-08-02T03:56:11Z"
}
Changes to list models
List models are extended to now return prebuilt and custom models. All prebuilt model names start with prebuilt -. Only models with a status of
succeeded are returned. To list models that either failed or are in progress, see List Operations.
Sample list models request
JSON
GET https://{your-form-recognizer-endpoint}/formrecognizer/documentModels?api-version=2022-08-31
Change to get model operation
As Get Model now includes prebuilt models, the Get operation returns a docTypes dictionary. Each document type description includes name,
optional description, field schema, and optional field confidence. The field schema describes the list of fields potentially returned with the
document type.
JSON
GET https://{your-form-recognizer-endpoint}/formrecognizer/documentModels/{modelId}?api-version=2022-08-31
New get info operation
The info operation on the service returns the custom model count and custom model limit.
JSON
GET https://{your-form-recognizer-endpoint}/formrecognizer/info? api-version=2022-08-31
Sample response
JSON
{
"customDocumentModels": {
"count": 5,
"limit": 100


<!---- Page 420 ---------------------------------------------------------------------------------------------------------------------------------->
}
}
Next steps
. Review the new REST API
· What is Document Intelligence?
· Document Intelligence quickstart 0


<!---- Page 421 ---------------------------------------------------------------------------------------------------------------------------------->
Build and train a custom extraction
model
Article · 12/11/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
v2.1
Document Intelligence custom models require a handful of training documents to get
started. If you have at least five documents, you can get started training a custom
model. You can train either a custom template model (custom form) or a custom neural
model (custom document). This document walks you through the process of training the
custom models.
Custom model input requirements
First, make sure your training data set follows the input requirements for Document
Intelligence.
· Supported file formats:
[ Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX ),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX ), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| Layout | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | :selected: ✔ | :selected: ✔ |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |
| Custom classification | :selected: ✔ | :selected: ✔ | :selected: ✔ |
|  |  |  |  |


<!---- Page 422 ---------------------------------------------------------------------------------------------------------------------------------->
· The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
. The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Training data tips
Follow these tips to further optimize your data set for training:
. Use text-based PDF documents instead of image-based documents. Scanned PDFs
are handled as images.
. Use examples that have all of the fields completed for forms with input fields.
· Use forms with different values in each field.
· Use a larger data set (10-15 images) if your form images are of lower quality.
Upload your training data
Once you gather a set of forms or documents for training, you need to upload it to an
Azure blob storage container. If you don't know how to create an Azure storage account
with a container, following the Azure Storage quickstart for Azure portal. You can use
the free pricing tier (F0) to try the service, and upgrade later to a paid tier for
production.
Video: Train your custom model


<!---- Page 423 ---------------------------------------------------------------------------------------------------------------------------------->
. Once you gather and upload your training dataset, you're ready to train your
custom model. In the following video, we create a project and explore some of the
fundamentals for successfully labeling and training a model.
https://www.microsoft.com/en-us/videoplayer/embed/RE5fX1c?postJslIMsg=true _
Create a project in the Document Intelligence
Studio
The Document Intelligence Studio provides and orchestrates all the API calls required to
complete your dataset and train your model.
1. Start by navigating to the Document Intelligence Studio [ . The first time you use
the Studio, you need to initialize your subscription, resource group, and resource.
Then, follow the prerequisites for custom projects to configure the Studio to access
your training dataset.
2. In the Studio, select the Custom extraction model tile and select the Create a
project button.
Document Intelligence Studio > Custom extraction model
Custom extraction models
Extract information from forms and documents with custom extraction models. Train a model by labeling as few as 5 example documents. (The same labeled dataset can train all types of custom extraction models.) Learn more about custom extraction models.
Watch these videos
Discover the secrets to successfully training a Document Intelligence custom extraction model by
watching our instructional videos. From creating balanced datasets to following best practices for
labeling, you can streamline your model training process and achieve better results.
Microsoft Azure
Azure Form Recognizer
D
H
Train you first custom model
...
My Projects
+ Create a project
0
Share
Import
Delete
Upgrade
P Search
a. On the create project dialog, provide a name for your project, optionally a
description, and select continue.
b. On the next step in the workflow, choose or create a Document Intelligence
resource before you select continue.
1 Important
Custom neural models are only available in a few regions. If you plan on
training a neural model, please select or create a resource in one of these


<!---- Page 424 ---------------------------------------------------------------------------------------------------------------------------------->
supported regions.
Custom extraction model
X
V
Enter project details
Configure service resource
To create a project in Document Intelligence Studio, you'll need an Azure
subscription containing a service resource for usage and billing.
Configure service resource
Subscription *
Connect training data source
Select existing
V
Resource group *
Review and create
Create new or select existing
V
Create new
Document Intelligence or Cognitive Service Resource *
Select existing
Create new resource
Set as default
V
API version *
2023-10-31 (Preview)
V
API version can only be changed by upgrading after the project is created.
3. Next select the storage account you used to upload your custom model training
dataset. The Folder path should be empty if your training documents are in the
root of the container. If your documents are in a subfolder, enter the relative path
from the container root in the Folder path field. Once your storage account is
configured, select continue.
Custom extraction model
X
Connect training data source
Enter project details
Link the Azure Blob Storage account and the folder that contains your training
data. Learn more
Configure service resource
Subscription *
Select existing
V
Connect training data source
Resource group *
Create new or select existing
V
Review and create
Storage account *
Select a storage account
Create new storage account
Set as default
V
Blob container *
Select a blob container
V
Create new
Folder path
Enter folder path
Back
Continue
Cancel
4. Finally, review your project settings and select Create Project to create a new
project. You should now be in the labeling window and see the files in your dataset


<!---- Page 425 ---------------------------------------------------------------------------------------------------------------------------------->
listed.
Label your data
In your project, your first task is to label your dataset with the fields you wish to extract.
The files you uploaded to storage are listed on the left of your screen, with the first file
ready to be labeled.
1. Start labeling your dataset and creating your first field by selecting the plus (+)
button on the top-right of the screen.
Label data
Train
Ju Run layout
V
*
Auto label
V
Draw region
O
0
@
V
+
Add a field
Drag & drop file
here or
Browse for files
Field
Selection mark
Contoso Electronics
Signature
ET
Table
Trey Research.pdf
SOW 546 for Agreement to Perform Consulting Services to Contoso Electronics
Date
March 6, 2019
Services Performed By:
Contoso Electronics
Services Performed For:
Trey Renmarch
1. Microsoft Way,
Redmond, WA 98052
Consolidate ... er.pdf
Best For You ... ny.pdf
Bellows College.pdf
This Statement of Work (SOW) is haued pursuant to the Consultant Services Manter Agreement between Trey
Research ["Client") Contoso 7. (pe "Amusement").
Beach [[Client') and Contoio Electronics ("Contractor"], effective March 7, 2019 (the "Agreement("). This
part thereof. Any term not otherwise defined herein shall have the meaning specified in the Agreement, In the
eveil of ary contact of inconsistency between the terms of this SOW and the terms of this Agreement, the
This SOW # 586 (hereinafter called the "SOW"), effective as of March 7, 2019, is entered into by and between
Contractor and Client, and is subject to the terms and conditions specified below. The Exhibit(s) to this 50W, if
any, shall be deemed to be a part hereof. In the event of any incorahtencin between the terms of the body of
thin SOW and the terms of the Exhibit[s] hereto, the terms of the body of this SOW shall prevail.
Adventure ... es.pdf
Adatum Cor ... on.pdf
Period of Performance
The Services shall commence on March 7, 2019, and shall continue through March 7, 2019.
Scope of Work
Contractor shall provide the Services and Deliverable(s) as follows:
the project one skylines below
O
* Agodny all be conpaten ond ategrating with own cloud technologies.
Contractor Responsibilities
0
work conditions with sespent to its employers, correctors or offer resouroms performing Serviom under the Agreement.
* Initial setup walkthrough on site
Statement of Work for Contaso Electronics "Starch 6 2099
1
2. Enter a name for the field.
3. Assign a value to the field by choosing a word or words in the document. Select
the field in either the dropdown or the field list on the right navigation bar. The
labeled value is below the field name in the list of fields.
4. Repeat the process for all the fields you wish to label for your dataset.
5. Label the remaining documents in your dataset by selecting each document and
selecting the text to be labeled.
You now have all the documents in your dataset labeled. The .labels.json and .ocr.json
files correspond to each document in your training dataset and a new fields.json file.
This training dataset is submitted to train the model.
Train your model


<!---- Page 426 ---------------------------------------------------------------------------------------------------------------------------------->
With your dataset labeled, you're now ready to train your model. Select the train button
in the upper-right corner.
1. On the train model dialog, provide a unique model ID and, optionally, a
description. The model ID accepts a string data type.
2. For the build mode, select the type of model you want to train. Learn more about
the model types and capabilities.
Train a new model
X
Model ID *
Enter model name
Model Description
Enter model description
Build Mode *
To train a custom form model, select Template. To train a custom document model, select Neural. Learn more
about how these types of custom models differ.
Select model build mode
V
Neural
Template
el
3. Select Train to initiate the training process.
4. Template models train in a few minutes. Neural models can take up to 30 minutes
to train.
5. Navigate to the Models menu to view the status of the train operation.
Test the model
Once the model training is complete, you can test your model by selecting the model on
the models list page.
1. Select the model and select on the Test button.
2. Select the + Add button to select a file to test the model.
3. With a file selected, choose the Analyze button to test the model.
4. The model results are displayed in the main window and the fields extracted are
listed in the right navigation bar.
5. Validate your model by evaluating the results for each field.


<!---- Page 427 ---------------------------------------------------------------------------------------------------------------------------------->
6. The right navigation bar also has the sample code to invoke your model and the
JSON results from the API.
Congratulations you learned to train a custom model in the Document Intelligence
Studio! Your model is ready for use with the REST API or the SDK to analyze documents.
Next steps
Now that you learned how to build a training data set, follow a quickstart to train a
custom Document Intelligence model and start using it on your forms.
Learn about custom model types
Learn about accuracy and confidence with custom models
See also
· Train a model and extract document data using the client library or REST API
· What is Document Intelligence?
Feedback
Was this page helpful?
Yes
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 428 ---------------------------------------------------------------------------------------------------------------------------------->
Build and train a custom classification
model
Article · 12/11/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
Custom classification models can classify each page in an input file to identify one or
more documents within. Classifier models can also identify multiple documents or
multiple instances of a single document in the input file. Document Intelligence custom
models require as few as five training documents per document class to get started. To
get started training a custom classification model, you need at least five documents for
each class and two classes of documents.
Custom classification model input
requirements
Make sure your training data set follows the input requirements for Document
Intelligence.
· Supported file formats:
[] Expand table
Model
PDF
Image:
JPEG/JPG, PNG, BMP,
TIFF, HEIF
Microsoft Office:
Word (DOCX ), Excel ( XLSX),
PowerPoint (PPTX ), HTML
Read
✔
✔
✔
Layout
✔
✔
✔
General Document
✔
✔
Prebuilt
✔
✔
Custom extraction
✔
✔
Custom
classification
✔
✔
✔
· For best results, provide one clear photo or high-quality scan per document.

| Model | PDF | Image: JPEG/JPG, PNG, BMP, TIFF, HEIF | Microsoft Office: Word (DOCX ), Excel ( XLSX), PowerPoint (PPTX ), HTML |
| --- | --- | --- | --- |
| Read | :selected: ✔ | :selected: ✔ | :selected: ✔ |
| Layout | :selected: ✔ | :selected: ✔ | ✔ :selected: |
| General Document | :selected: ✔ | :selected: ✔ |  |
| Prebuilt | :selected: ✔ | :selected: ✔ |  |
| Custom extraction | :selected: ✔ | :selected: ✔ |  |
| Custom classification | :selected: ✔ | :selected: ✔ | :selected: ✔ |


<!---- Page 429 ---------------------------------------------------------------------------------------------------------------------------------->
· For PDF and TIFF, up to 2,000 pages can be processed (with a free tier subscription,
only the first two pages are processed).
. The file size for analyzing documents is 500 MB for paid (SO) tier and 4 MB for free
(F0) tier.
· Image dimensions must be between 50 pixels x 50 pixels and 10,000 pixels x
10,000 pixels.
. If your PDFs are password-locked, you must remove the lock before submission.
· The minimum height of the text to be extracted is 12 pixels for a 1024 x 768 pixel
image. This dimension corresponds to about 8 point text at 150 dots per inch
(DPI).
. For custom model training, the maximum number of pages for training data is 500
for the custom template model and 50,000 for the custom neural model.
o For custom extraction model training, the total size of training data is 50 MB for
template model and 1 GB for the neural model.
o For custom classification model training, the total size of training data is 1 GB
with a maximum of 10,000 pages. For 2024-11-30 (GA), the total size of training
data is 2 GB with a maximum of 10,000 pages.
Training data tips
Follow these tips to further optimize your data set for training:
· If possible, use text-based PDF documents instead of image-based documents.
Scanned PDFs are handled as images.
· If your form images are of lower quality, use a larger data set (10-15 images, for
example).
Upload your training data
Once you put together the set of forms or documents for training, you need to upload it
to an Azure blob storage container. If you don't know how to create an Azure storage
account with a container, follow the Azure Storage quickstart for Azure portal. You can
use the free pricing tier (F0) to try the service, and upgrade later to a paid tier for
production. If your dataset is organized as folders, preserve that structure as the Studio
can use your folder names for labels to simplify the labeling process.


<!---- Page 430 ---------------------------------------------------------------------------------------------------------------------------------->
Create a classification project in the Document
Intelligence Studio
The Document Intelligence Studio provides and orchestrates all the API calls required to
complete your dataset and train your model.
1. Start by navigating to the Document Intelligence Studio . The first time you use
the Studio, you need to initialize your subscription, resource group, and resource.
Then, follow the prerequisites for custom projects to configure the Studio to access
your training dataset.
2. In the Studio, select the Custom classification model tile, on the custom models
section of the page and select the Create a project button.
Custom classification model
Build a custom classification model to split
and classify documents.
Create new
a. On the Create Project dialog, provide a name for your project, optionally a
description, and select continue.
b. Next, choose, or select create a Document Intelligence resource before you
continue.


<!---- Page 431 ---------------------------------------------------------------------------------------------------------------------------------->
Custom models
Enter project details
Configure service resource
To create a project in Form Recognizer Studio, you will need an Azure subscription
containing a service resource for usage and billing. Resources are organized in
resource groups. Learn more
Configure service resource
Subscription *
Connect training data source
V
Review and create
V
Resource group *
Create new
Form Recognizer or Cognitive Service Resource *
V
Create new resource
API version *
V
3. Next select the storage account you used to upload your custom model training
dataset. The Folder path should be empty if your training documents are in the
root of the container. If your documents are in a subfolder, enter the relative path
from the container root in the Folder path field. Once your storage account is
configured, select continue.
1 Important
You can either organize the training dataset by folders where the folder name
is the label or class for documents or create a flat list of documents that you
can assign a label to in the Studio.


<!---- Page 432 ---------------------------------------------------------------------------------------------------------------------------------->
Custom models
X
Enter project details
Connect training data source
Link the Azure Blob Storage account and the folder that contains your training
data. Learn more
Configure service resource
Subscription *
Connect training data source
V
Resource group *
Review and create
V
Storage account *
V
Blob container *
V
Folder path
Back
Continue
Cancel
4. Training a custom classifier requires the output from the Layout model for each
document in your dataset. Run layout on all documents before the model training
process.
5. Finally, review your project settings and select Create Project to create a new
project. You should now be in the labeling window and see the files in your dataset
listed.
Label your data
In your project, you only need to label each document with the appropriate class label.


<!---- Page 433 ---------------------------------------------------------------------------------------------------------------------------------->
Applied Al | Form Recognizer Studio
1
®
@
?
5
Form Recognizer Studio > Custom classification model
Custom classification models
Custom classification model welcome message (To be update). Learn more about custom classification model.
My Projects
+ Create a project [] Delete
Search
Create a project
Project name
Description
Created Į
API Version
Create a new project to get started.
You see the files you uploaded to storage in the file list, ready to be labeled. You have a
few options to label your dataset.
1. If the documents are organized in folders, the Studio prompts you to use the
folder names as labels. This step simplifies your labeling down to a single select.
2. To assign a label to a document, select on the add label selection mark to assign
a label.
3. Control select to multi-select documents to assign a label
You should now have all the documents in your dataset labeled. If you look at the
storage account, you find .ocr.json files that correspond to each document in your
training dataset and a new class-name.jsonl file for each class labeled. This training
dataset is submitted to train the model.
Train your model
With your dataset labeled, you're now ready to train your model. Select the train button
in the upper-right corner.
1. On the train model dialog, provide a unique classifier ID and, optionally, a
description. The classifier ID accepts a string data type.
2. Select Train to initiate the training process.
3. Classifier models train in a few minutes.


<!---- Page 434 ---------------------------------------------------------------------------------------------------------------------------------->
4. Navigate to the Models menu to view the status of the train operation.
Test the model
Once the model training is complete, you can test your model by selecting the model on
the models list page.
1. Select the model and select on the Test button.
2. Add a new file by browsing for a file or dropping a file into the document selector.
3. With a file selected, choose the Analyze button to test the model.
4. The model results are displayed with the list of identified documents, a confidence
score for each document identified and the page range for each of the documents
identified.
5. Validate your model by evaluating the results for each document identified.
Training a custom classifier using the SDK or
API
The Studio orchestrates the API calls for you to train a custom classifier. The classifier
training dataset requires the output from the layout API that matches the version of the
API for your training model. Using layout results from an older API version can result in
a model with lower accuracy.
The Studio generates the layout results for your training dataset if the dataset doesn't
contain layout results. When using the API or SDK to train a classifier, you need to add
the layout results to the folders containing the individual documents. The layout results
should be in the format of the API response when calling layout directly. The SDK object
model is different. Make sure that the layout results are the API results and not the
SDK response .
Troubleshoot
The classification model requires results from the layout model for each training
document. If you don't provide the layout results, the Studio attempts to run the layout
model for each document before training the classifier. This process is throttled and can
result in a 429 response.


<!---- Page 435 ---------------------------------------------------------------------------------------------------------------------------------->
In the Studio, before training with the classification model, run the layout model on
each document and upload it to the same location as the original document. Once the
layout results are added, you can train the classifier model with your documents.
Next steps
Learn about custom model types
Learn about accuracy and confidence with custom models
Feedback
Was this page helpful?
Yes
No
Provide product feedback z | Get help at Microsoft Q&A


<!---- Page 436 ---------------------------------------------------------------------------------------------------------------------------------->
Project sharing using Document
Intelligence Studio
Article · 11/19/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Document Intelligence Studio is an online tool to visually explore, understand, train, and
integrate features from the Document Intelligence service into your applications.
Document Intelligence Studio enables project sharing feature within the custom
extraction model. Projects can be shared easily via a project token. The same project
token can also be used to import a project.
Prerequisite
In order to share and import your custom projects seamlessly, both users (user who
shares and user who imports) need an active Azure account 2 . If you don't have one,
you can create a free account . Also, both users need to configure permissions to
grant access to the Document Intelligence and storage resources.
Generally, in the process of creating a custom model project, most of the requirements
should be met for project sharing. However, in cases where the project sharing feature
doesn't work, check permissions.
Granted access and permissions
1 Important
Custom model projects can be imported only if you have the access to the storage
account that is associated with the project you are trying to import. Check your
storage account permission before starting to share or import projects with others.
Virtual networks and firewalls
If your storage account virtual network (VNet) is enabled or if there are any firewall
constraints, the project can't be shared. If you want to bypass those restrictions, ensure
that those settings are turned off.


<!---- Page 437 ---------------------------------------------------------------------------------------------------------------------------------->
A workaround is to manually create a project using the same settings as the project
being shared.
Share a custom extraction model with
Document Intelligence Studio
1 Note
Custom classification model projects can also be shared following the same step,
starting with the following pages . In this guide, we use custom extraction project
as an example to share projects.
Follow these steps to share your project using Document Intelligence Studio:
1. Start by navigating to the Document Intelligence Studio .
2. In the Studio, select the Custom extraction models tile, under the custom models
section.
Custom models
Train custom models to classify documents and extract text, structure and fields from your forms or documents.
Custom extraction model
Custom classification model
Label and build a custom model to extract a
specific schema from your forms and
documents.
Build a custom classification model to split
and classify documents.
Create new
Create new
3. On the custom extraction models page, select the desired model to share and then
select the Share button.


<!---- Page 438 ---------------------------------------------------------------------------------------------------------------------------------->
W-9
Request for Taxpayer
Identification Number and Certification
Dive Form In the
bond to the IRS.
O
Applied Al | Form Recognizer Studio
@ @ ?
+
Form Recognizer Studio > Custom extraction model
Custom extraction models
Extract information from forms and documents with custom extraction models. Train a model by labeling as few as 5 example documents. (The same labelled dataset can train all types of custom extraction models.) Learn more about custom extraction models.
Template (Custom form) models
Neural (Custom document) models
Template models work well when the target documents share a common visual layout. Training only takes a few minutes, and more
than 100 languagei ane supportind.
Neural model can flexibly handle both structured and unstructured documents. Training takes up to half an hour, and currently
only Enginh language documenti are supported. The current version can extract inline field data and checkboomi.
Neural models are available only in select regions. Click here for details.
HOUSE RENTAL AGREEMENT
This House Rental Agreement ("Agreement," "rental agreement," or "lease") is entered
into between Dpay LLC (Landlord)
house, yard, and related facilities located at [] [pier Firmy Eni is (the
"premises"). The date of this Agreement is
1.
Landlord rents to Tensat, unfurnished, the premises on a month to mouth basis,
terminable by either party at the end of any calendar month on at least 30 days notice to the other
party. Tenant shall be entitled to possession of the premises and rent shall commence on April (7
MIRAI
Tenant shall not assign, sublease, or allow anyone other than persons permitted under this
My Projects
+ Create a project
Share
· Import
Delete
Search
Project name
Description
Created 4
API Version
.
custom-extraction
2023-04-04123-58:25.50592412
2021-02-28-previewv
multipage
curtora imi multipage
2022-05-31
application12
curtore het application/12
2022-09-20107:20:13.36471882
2022-08-31
sec-10q
2022-99-28701-17:52.63193642
2022-08-31
Studio Test
2022-09-38700 5332-88488682
2022-38-31
Lienfluent
juribling words
2022-06-10-previewe
test-custom-2
For customer
2022-01-30-preview
4. On the share project dialog, copy the project token for the selected project.
Applied Al | Form Recognizer Studio
-
0
@
?
+
Form Recognizer Studio > Custom extraction model
Custom extraction models
Extract information from forms and documents with custom extraction model. Train a model by labeling as few as 5 example documents. (The same labeled dataset can train all types of custom extraction models.] Learn more about custom extraction models.
Template (Custom form) models
Neural (Custom document) models
Template models work well when the target documents share a common visual layout. Training only takes a few minutes, and more
than 100 languages are supported.
Neural models can flexibly handle both structured and unstructured documents. Training takes up to half an hour and currently
only English language documents are supported. The current version can extract inline field data and checkbones.
Neural models are available only in select regions. Click here for details.
W-9
Request for Taxpayer
Identification Number and Certification
Give Forme to the
send to the IRS.
HOUSE RENTAL AGREEMENT
₱ Go to www.is. por/Form is for instructions and the latest information.
This House Rental Agreement ("Agreement," "rental agreement," or "lease") is entered
into between Dpay [IT] (Landlord) and ENTRE WETTER TWEETY BWIES
Arch(rc)
(Tenants). If more than one person is named as Tenant they shall be jointly and severally hable
and responsible under the terms of this Agreement. This lease Agreement involves a residential
house, yard, and related facilities located at ( Beicon Dan FE [v] [ity : 3 (the
"premises"). The date of this Agreement is ETTES [K] MOD
cooporation
Drawmp
Share project
month to mouth basis,
wys notice to the other
×
Copy the below project token to share the selected project.
permitted
is perfumed finder this
<project-token>
D
My Projects
Close
+ Create a project (4? Share 1 Import
Delete
D Search
Project name
Description
Created
API Version
custom-extraction
2021-04-04123:50:29.0009341Z
2023-02-20-preview
mul:page
custom test multipage
2022-08-20101:22:19.018885XXX
2022-08-31
application12
custom test application12
2022-08-20101:20:13.36471802
2022-08-31
set-10q
custom test noc- 10q
2022-08-31
Stucho Test
Custom Improvements
2022-08-31
jumbling wards
2022-06-30-preview
test-custom-2
For customser
2022-05-17110:34:40.68352232
2022-01-30-preview
Import custom extraction model with
Document Intelligence Studio
Follow these steps to import a project using Document Intelligence Studio.
1. Start by navigating to the Document Intelligence Studio[.
2. In the Studio, select the Custom extraction models tile, under the custom models
section.

| W-9 Request for Taxpayer Identification Number and Certification Dive Form In the bond to the IRS. |  |  |  |  |  |
| --- | --- | --- | --- | --- | --- |
|  |  |  |  |  |  |
|  |  |  |  |  |  |
|  |  | :unselected: :unselected: :unselected: :unselected: |  |  |  |
|  |  |  |  |  |  |
|  |  |  |  |  |  |
|  |  | O |  |  |  |
|  |  |  |  |  |  |


<!---- Page 439 ---------------------------------------------------------------------------------------------------------------------------------->
Custom models
Train custom models to classify documents and extract text, structure and fields from your forms or documents.
Custom extraction model
Custom classification model
Label and build a custom model to extract a
specific schema from your forms and
documents.
Build a custom classification model to split
and classify documents.
Create new
Create new
3. On the custom extraction models page, select the Import button.
Form Recognizer Studio > Custom extraction model
Custom extraction models
Extract information from forms and documents with custom extraction models. Train a model by labeling as few as 5 example documents. (The same labeled dataset can train all types of custom extraction models.) Learn more about custom extraction modeb.
Template (Custom form) models
Neural (Custom document) models
Template models work well when the target documents share a common visual layout. Training only takes a few minutes, and more
than 100 languages are supported.
Neural models can flexibly handle both structured and unstructured documents. Training tabes up to half an hour and cumently
only English language documenti are supported. The current version can odract inline field data and checkbcons.
Neural models are available only in select regions. Click here for details.
W-9
Request for Taxpayer
Identification Number and Certification
De lo wane irs goedFermiall for iretnations and De ladenil information.
HOUSE RENTAL AGREEMENT
This House Rental Agreement ("Agreement," "rental agreement," or "lease") is entered
into between [pay CLK (Landlord) and EXIT ETTER EN FES ERTHE [%] EN1910 11240
(Tenants). If more than one person is named as Tenant they shall be jointly and severally liable
and responsible under the terms of this Agreement. This lease Agreement javolver a residential
house, yard, and related facilities located at ( Beicon Ore E . City i 681133 (the
"premises"). The date of this Agreement is Bn13/2 KJ MIN
1.
Landlord rents to Tenant, wafurnished, the premises on a month to mouth basis.
terminable by either party at the end of any calendar month on at least 30 days notice to the other
Termine shal
arty, Tenant shall be entitled to possession of the premises and reat shall commence on April 17
Tenant shall not assign, sublease, or allow anyone other than persons permitted under this
Arche: ing
My Projects
+ Create a project He Shan
+ Impart
Delete
> Search
Project name
Description
Creabed 4
API Version
Create a new project to get started.
4. On the import project dialog, paste the project token shared with you and select
import.
Form Recognizer Studio > Custom extraction model
Custom extraction models
Extract information from forms and documents with custom extraction models. Train a model by labeling i fem in 5 example documenti. (The same labeled dataset can train all types of custom extraction modeb.) Learn more about custom extraction models.
Template (Custom form) models
Neural (Custom document) models
Template models work well when the target documents share a common visual layout. Training only takes a few minutes, and more
than 100 languages are supported
Neural models can flexibly handle both structured and unstructured documents. Training takes up to half an hour, and currently
only English language documents are supported. The current version can extract inline field data and checkberors.
Neural models are available only in select regions. Click here for details.
W-9
Request for Taxpayer
Identification Number and Certification
Give Form to the
requester. Do not
send to the INS.
HOUSE RENTAL AGREEMENT
This House Rental Agreement ("Agreement," "rental agreement," or "lease") is entered
into between Ppay] [TO (Landlord) and Exem TANTE ET PRETIL mann
(Tenants). If more than one person is named as Tenant they shall be jointly and severally hable
and responsible under the terms of this Agreement. This lease Agreement involves a residential
house, yard, and related facilities located at ( Pirater [r] 5 . [ity 1 (the
"premises"). The date of this Agreement is BULATS [5] Man
Archai ing]
Import project
touch to month basis,
lays notice to the other
Enter the project token to continue working on an existing Form
Recognizer project.
April
is permitted under this
<project-token>
My Projects
Import
Close
+ Create a project LÊ Share + Import [0] Delete
P Search
Project name
Description
Created 1
API Version
Create a new project to get started.


<!---- Page 440 ---------------------------------------------------------------------------------------------------------------------------------->
Next steps
Back up and recover models
Feedback
Was this page helpful?
& Yes
P
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 441 ---------------------------------------------------------------------------------------------------------------------------------->
Compose custom models
Article · 11/19/2024
emphasis style
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
1 Important
Model compose behavior is changed for api-version=2024-11-30 (GA). For more
information refer to composed custom models. The following behavior only
applies to v3.1 and previous versions.
A composed model is created by taking a collection of custom models and assigning
them to a single model ID. You can assign up to 200 trained custom models to a single
composed model ID. When a document is submitted to a composed model, the service
performs a classification step to decide which custom model accurately represents the
form presented for analysis. Composed models are useful when you train several models
and want to group them to analyze similar form types. For example, your composed
model might include custom models trained to analyze your supply, equipment, and
furniture purchase orders. Instead of manually trying to select the appropriate model,
you can use a composed model to determine the appropriate custom model for each
analysis and extraction.
To learn more, see Composed custom models.
In this article, you learn how to create and use composed custom models to analyze
your forms and documents.
Prerequisites
To get started, you need the following resources:
. An Azure subscription. You can create a free Azure subscription .
· A Document Intelligence instance. Once you have your Azure subscription, create
a Document Intelligence resource in the Azure portal to get your key and
endpoint. If you have an existing Document Intelligence resource, navigate directly
to your resource page. You can use the free pricing tier (F0) to try the service, and
upgrade later to a paid tier for production.


<!---- Page 442 ---------------------------------------------------------------------------------------------------------------------------------->
1. After the resource deploys, select Go to resource.
2. Copy the Keys and Endpoint values from the Azure portal and paste them in
a convenient location, such as Microsoft Notepad. You need the key and
endpoint values to connect your application to the Document Intelligence
API.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
*
X
P Search
«
C Regenerate Key1 3 Regenerate Key2
Overview
Activity log
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
* Diagnose and solve problems
/ Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
C
<> Networking
Location/Region
Identity
westus2
0
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
> Monitoring
Automation
Lo
Help
? Tip
For more information, see create a Document Intelligence resource.
. An Azure storage account. If you don't know how to create an Azure storage
account, follow the Azure Storage quickstart for Azure portal. You can use the free
pricing tier (F0) to try the service, and upgrade later to a paid tier for production.
Create your custom models
First, you need a set of custom models to compose. You can use the Document
Intelligence Studio, REST API, or client libraries. The steps are as follows:
· Assemble your training dataset
· Upload your training set to Azure blob storage
. Train your custom models


<!---- Page 443 ---------------------------------------------------------------------------------------------------------------------------------->
Assemble your training dataset
Building a custom model begins with establishing your training dataset. You need a
minimum of five completed forms of the same type for your sample dataset. They can
be of different file types (jpg, png, pdf, tiff) and contain both text and handwriting. Your
forms must follow the input requirements for Document Intelligence.
? Tip
Follow these tips to optimize your data set for training:
· If possible, use text-based PDF documents instead of image-based
documents. Scanned PDFs are handled as images.
· For filled-in forms, use examples that have all of their fields filled in.
· Use forms with different values in each field.
· If your form images are of lower quality, use a larger data set (10-15 images,
for example).
See Build a training data set for tips on how to collect your training documents.
Upload your training dataset
Once you gather a set of training documents, you need to upload your training data to
an Azure blob storage container.
If you want to use manually labeled data, you have to upload the .labels.json and
.ocr.json files that correspond to your training documents.
Train your custom model
When you train your model with labeled data, the model uses supervised learning to
extract values of interest, using the labeled forms you provide. Labeled data results in
better-performing models and can produce models that work with complex forms or
forms containing values without keys.
Document Intelligence uses the prebuilt-layout model API to learn the expected sizes
and positions of typeface and handwritten text elements and extract tables. Then it uses
user-specified labels to learn the key/value associations and tables in the documents.
We recommend that you use five manually labeled forms of the same type (same
structure) to get started with training a new model. Then, add more labeled data, as


<!---- Page 444 ---------------------------------------------------------------------------------------------------------------------------------->
needed, to improve the model accuracy. Document Intelligence enables training a
model to extract key-value pairs and tables using supervised learning capabilities.
Document Intelligence Studio
To create custom models, start with configuring your project:
1. From the Studio homepage, select Create newE from the Custom model
card.
2. Use the + Create a project command to start the new project configuration
wizard.
3. Enter project details, select the Azure subscription and resource, and the Azure
Blob storage container that contains your data.
4. Review, submit your settings, and create the project.
My Projects
+ Create a project
8 Delete
Project name
Description
Created :
API Version
While creating your custom models, you may need to extract data collections from
your documents. The collections may appear one of two formats. Using tables as
the visual pattern:
· Dynamic or variable count of values (rows) for a given set of fields (columns)
· Specific collection of values for a given set of fields (columns and/or rows)
See Document Intelligence Studio: labeling as tables


<!---- Page 445 ---------------------------------------------------------------------------------------------------------------------------------->
Create a composed model
1 Note
the create compose model operation is only available for custom models trained
with labels. Attempting to compose unlabeled models will produce an error.
With the create compose model operation, you can assign up to 100 trained custom
models to a single model ID. When analyze documents with a composed model,
Document Intelligence first classifies the form you submitted, then chooses the best
matching assigned model, and returns results for that model. This operation is useful
when incoming forms may belong to one of several templates.
Document Intelligence Studio
Once the training process is successfully completed, you can begin to build your
composed model. Here are the steps for creating and using composed models:
· Gather your custom model IDs
· Compose your custom models
· Analyze documents
· Manage your composed models
Gather your model IDs
When you train models using the Document Intelligence Studio , the model ID is
located in the models menu under a project:
Cognitive Services | Form Recognizer Studio - Preview
«
Form Recognizer Studio > Custom Form > composed > Models
Custom Form
Models
composed
Compose
Test
Download
Delete
Label data
Model ID
Model Description
Models
8aa 16866-16fe-44ca-b13a-8bfc6ad1d
Test
773fb140-f173-47a2-8aa9-a5fce1ceb
Settings
4c493f98-87c3-4f6d-b0d8-3a1aab49e
Compose your custom models

| « | Form Recognizer Studio > Custom Form > composed > Models |
| --- | --- |
| Custom Form | Models |
| composed | Compose Test Download Delete |
| Label data | :selected: Model ID :unselected: Model Description |
| Models | :selected: 8aa 16866-16fe-44ca-b13a-8bfc6ad1d |
| Test | :selected: 773fb140-f173-47a2-8aa9-a5fce1ceb |
| Settings | :selected: 4c493f98-87c3-4f6d-b0d8-3a1aab49e |


<!---- Page 446 ---------------------------------------------------------------------------------------------------------------------------------->
1. Select a custom models project.
2. In the project, select the Models menu item.
3. From the resulting list of models, select the models you wish to compose.
4. Choose the Compose button from the upper-left corner.
5. In the pop-up window, name your newly composed model and select
Compose.
6. When the operation completes, your newly composed model appears in the
list.
7. Once the model is ready, use the Test command to validate it with your test
documents and observe the results.
Analyze documents
The custom model Analyze operation requires you to provide the modelID in the
call to Document Intelligence. You should provide the composed model ID for the
modelID parameter in your applications.
Applied Al | Form Recognizer Studio - Preview
®
₡
«
Form Recognizer Studio
Custom model
jan-2022-preview > Models
Custom models
Models
jan-2022-preview
Compose \ Test _ Download @ Dele
Delete
Label data
Model ID
Model Description
API Version
Created
Status
Models
composed-fr-model
five custom models composed
2022-01-30-preview
2022-02-12T23:42:46Z
succeeded
Manage your composed models
You can manage your custom models throughout life cycles:
· Test and validate new documents.
· Download your model to use in your applications.
· Delete your model when its lifecycle is complete.


<!---- Page 447 ---------------------------------------------------------------------------------------------------------------------------------->
composed-fr
X
A
Test
Download
Delete
Component model
Build Mode
Field Name
A
Accuracy
fr-3
template
Receipt No
95 %
fr-3
template
Sold To
95 %
fr-3
template
ID #
83.3 %
fr-3
template
Live Delivery?
95 %
fr-3
template
Online Delivery?
95 %
fr-3
template
Video Delivery?
95 %
fr-1
template
Receipt No
95 %
fr-1
template
Sold To
95 %
fr-1
template
ID #
83.3 %
fr-1
template
Live Deliverv?
95 %
4
4
1
Great! You learned the steps to create custom and composed models and use them in
your Document Intelligence projects and applications.
Next steps
Try one of our Document Intelligence quickstarts:
Document Intelligence Studio
REST API
C#
Java
JavaScript
Python
Feedback

| Component model | Build Mode | Field Name | A Accuracy |
| --- | --- | --- | --- |
| fr-3 | template | Receipt No | 95 % |
| fr-3 | template | Sold To | 95 % |
| fr-3 | template | ID # | 83.3 % |
| fr-3 | template | Live Delivery? | 95 % |
| fr-3 | template | Online Delivery? | 95 % |
| fr-3 | template | Video Delivery? | 95 % |
| fr-1 | template | Receipt No | 95 % |
| fr-1 | template | Sold To | 95 % |
| fr-1 | template | ID # | 83.3 % |
| fr-1 | template | Live Deliverv? | 95 % 4 |
| 4 |  |  | 1 |
|  |  |  |  |


<!---- Page 448 ---------------------------------------------------------------------------------------------------------------------------------->
Was this page helpful?
Yes
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 449 ---------------------------------------------------------------------------------------------------------------------------------->
v2.1
Disaster recovery
Article · 02/27/2025
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
(GA)
When you create a Document Intelligence resource in the Azure portal, you specify a region.
From then on, your resource and all of its operations stay associated with that particular Azure
server region. It's rare, but not impossible, to encounter a network issue that hits an entire
region. If your solution needs to always be available, then you should design it to either fail-
over into another region or split the workload between two or more regions. Both approaches
require at least two Document Intelligence resources in different regions and the ability to sync
custom models and classifiers across regions.
The Copy API enables this scenario by allowing you to copy custom models and classifiers from
one Document Intelligence account or into others, which can exist in any supported
geographical region. This guide shows you how to use the Copy REST API with cURL for
custom models. You can also use an HTTP request service to issue the requests.
1 Note
The 2024-11-30 (GA) API custom classification model supports the Copy API. This guide
specifically uses custom models to copy models. For classifier model copy, follow the train
a custom classifier guide.
Business scenarios
If your app or business depends on the use of a Document Intelligence custom model, we
recommend you copy your model to another Document Intelligence account in another region.
If a regional outage occurs, you can then access your model in the region where it was copied.
Prerequisites
1. Two Document Intelligence Azure resources in different Azure subscriptions or regions. If
you don't have them, go to the Azure portal and create a new Document Intelligence
resource .
2. The key, endpoint URL, and subscription ID for your Document Intelligence resource. You
can find these values on the resource's Overview tab in the Azure portal .


<!---- Page 450 ---------------------------------------------------------------------------------------------------------------------------------->
Copy API overview
The process for copying a custom model consists of the following steps:
1. First you issue a copy authorization request to the target resource-that is, the resource
that receives the copied model. You receive back the URL of the newly created target
model that receives the copied model.
2. Next you send the copy request to the source resource-the resource that contains the
model to be copied with the payload (copy authorization) returned from the previous call.
You receive back a URL that you can query to track the progress of the operation.
3. You use your source resource credentials to query the progress URL until the operation is
a success. You can also query the new model ID in the target resource to get the status of
the new model.
Generate Copy authorization request
The following HTTP request gets copy authorization from your target resource. You need to
enter the endpoint and key of your target resource as headers.
HTTP
POST https://<your-resource-
endpoint>/documentintelligence/documentModels:authorizeCopy?api-version=2024-11-30
Ocp-Apim-Subscription-Key: {<your-key>}
Request body
JSON
{
"modelId": "target-model-name",
"description": "Copied from SCUS"
}
You receive a 200 response code with response body that contains the JSON payload required
to initiate the copy.
JSON
{
"targetResourceId":
"/subscriptions/{targetSub}/resourceGroups/{targetRG}/providers/Microsoft.Cognitiv
eServices/accounts/{targetService}",
"targetResourceRegion": "region",


<!---- Page 451 ---------------------------------------------------------------------------------------------------------------------------------->
"targetModelId": "target-model-name",
"targetModelLocation": "model path",
"accessToken": "access token",
"expirationDateTime": "timestamp"
}
Start Copy operation
The following HTTP request starts the copy operation on the source resource. You need to
enter the endpoint and key of your source resource as the url and header. Notice that the
request URL contains the model ID of the source model you want to copy.
HTTP
POST https://<your-resource-
endpoint>/documentintelligence/documentModels/{modelId}:copyTo?api-version=2024-
11-30
Ocp-Apim-Subscription-Key: {<your-key>}
The body of your request is the response from the previous step.
JSON
{
"targetResourceId":
"/subscriptions/{targetSub}/resourceGroups/{targetRG}/providers/Microsoft. Cognitiv
eServices/accounts/{targetService}",
"targetResourceRegion": "region",
"targetModelId": "target-model-name",
"targetModelLocation": "model path",
"accessToken": "access token",
"expirationDateTime": "timestamp"
}
You receive a 202\Accepted response with an Operation-Location header. This value is the URL
that you use to track the progress of the operation. Copy it to a temporary location for the next
step.
HTTP
HTTP/1.1 202 Accepted
Operation-Location: https://<your-resource-
endpoint>.cognitiveservices. azure. com/documentintelligence/operations/ {operation-
id}?api-version=2024-11-30


<!---- Page 452 ---------------------------------------------------------------------------------------------------------------------------------->
1 Note
The Copy API transparently supports the AEK/CMK & feature. This action doesn't require
any special treatment, but note that if you're copying between an unencrypted resource to
an encrypted resource, you need to include the request header x-ms-forms-copy-degrade:
true . If this header isn't included, the copy operation fails and returns a
DataProtectionTransformServiceError .
Track Copy progress
Console
GET https://<your-resource-
endpoint>.cognitiveservices.azure.com/documentintelligence/operations/{‹operation-
id>}?api-version=2024-11-30
Ocp-Apim-Subscription-Key: {<your-key>}
Track the target model ID
You can also use the Get model API to track the status of the operation by querying the target
model. Call the API using the target model ID that you copied down from the Generate copy
authorization request response.
HTTP
GET https://<your-resource-
endpoint>/documentintelligence/documentModels/{modelId}?api-version=2024-11-30" -H
"Ocp-Apim-Subscription-Key: < your-key>
In the response body, you see information about the model. Check the "status" field for the
status of the model.
HTTP
HTTP/1.1 200 OK
Content-Type: application/json; charset=utf-8
{"modelInfo":{"modelId":"33f4d42c-cd2f-4e74-b990-
a1aeafab5a5d", "status":"ready", "createdDateTime":"2020-02-
26T16 : 59 : 28Z", "lastUpdatedDateTime" : "2020-02-26T16: 59: 34Z"}, "trainResult":
{"trainingDocuments": [{"documentName":"0.pdf", "pages":1, "errors":
[],"status": "succeeded"}, {"documentName":"1.pdf", "pages":1, "errors":
[],"status": "succeeded"}, {"documentName":"2.pdf", "pages":1, "errors":
[],"status":"succeeded"}, {"documentName":"3.pdf", "pages":1, "errors":


<!---- Page 453 ---------------------------------------------------------------------------------------------------------------------------------->
[], "status": "succeeded"}, {"documentName": "4. pdf", "pages":1, "errors":
[], "status": "succeeded"} ], "errors": []}}
cURL sample code
The following code snippets use cURL to make API calls. You also need to fill in the model IDs
and subscription information specific to your own resources.
Generate Copy authorization
Request
Bash
curl -i -X POST "<your-resource-
endpoint>/documentintelligence/documentModels: authorizeCopy?api-version=2024-11-
30"
-H "Content-Type: application/json"
-H "Ocp-Apim-Subscription-Key: < YOUR-KEY>"
-- data-ascii "{
'modelId': '{modelId} ',
'description': '{description} '
}"
Successful response
JSON
{
"targetResourceId": "string",
"targetResourceRegion": "string",
"targetModelId": "string",
"targetModelLocation": "string",
"accessToken": "string",
"expirationDateTime": "string"
}
Begin Copy operation
Request
Bash
curl -i -X POST "<your-resource-
endpoint>/documentintelligence/documentModels/{modelId}: copyTo?api-version=2024-


<!---- Page 454 ---------------------------------------------------------------------------------------------------------------------------------->
11-30"
-H "Content-Type: application/json"
-H "Ocp-Apim-Subscription-Key: < YOUR-KEY>"
-- data-ascii "{
'targetResourceId': '{targetResourceId} ',
'targetResourceRegion' : {targetResourceRegion} ',
'targetModelId': '{targetModelId}',
'targetModelLocation': '{targetModelLocation}',
'accessToken' : '{accessToken} ',
'expirationDateTime': '{expirationDateTime} '
}"
Successful response
HTTP
HTTP/1.1 202 Accepted
Operation-Location: https://<your-resource-
endpoint>.cognitiveservices.azure.com/documentintelligence/operations/ {operation-
id}?api-version=2024-11-30
Track copy operation progress
You can use the GET operation API to list all document model operations (succeeded, in-
progress, or failed) associated with your Document Intelligence resource. Operation
information only persists for 24 hours. Here's a list of the operations (operationId) that can be
returned:
· documentModelBuild
· documentModelCompose
· documentModelCopyTo
Track the target model ID
If the operation was successful, the document model can be accessed using the getModel (get
a single model), or GetModels (get a list of models) APIs.
Common error code messages
0
Expand table


<!---- Page 455 ---------------------------------------------------------------------------------------------------------------------------------->
Error
Resolution
400 / Bad Request with
Indicates validation error or badly formed copy request. Common issues
"code:" "1002"
include: a) Invalid or modified copyAuthorization payload. b) Expired value for
expirationDateTimeTicks token (copyAuthorization payload is valid for 24
hours). c) Invalid or unsupported targetResourceRegion. d) Invalid or
malformed targetResourceId string.
Authorization failure
due to missing or invalid
authorization claims.
Occurs when the copyAuthorization payload or content is modified from the
copyAuthorization API. Ensure that the payload is the same exact content that
was returned from the earlier copyAuthorization call.
Couldn't retrieve
authorization metadata.
Indicates that the copyAuthorization payload is being reused with a copy
request. A copy request that succeeds doesn't allow any further requests that
use the same copyAuthorization payload. If you raise a separate error and you
later retry the copy with the same authorization payload, this error gets raised.
The resolution is to generate a new copyAuthorization payload and then
reissue the copy request.
Data transfer request
isn't allowed as it
downgrades to a less
secure data protection
scheme.
Occurs when copying between an AEK enabled resource to a non- AEK
enabled resource. To allow copying encrypted model to the target as
unencrypted, specify x-ms-forms-copy-degrade: true header with the copy
request.
"Couldn't fetch
information for
Cognitive resource with
ID ... ".
Indicates that the Azure resource indicated by the targetResourceId isn't a
valid Cognitive resource or doesn't exist. To resolve this issue, verify and reissue
the copy request.
Ensure the resource is valid and exists in the specified region, such as, westus2
Next steps
In this guide, you learned how to use the Copy API to back up your custom models to a
secondary Document Intelligence resource. Next, explore the API reference docs to see what
else you can do with Document Intelligence.
. REST API reference documentation

| Error | Resolution |
| --- | --- |
| 400 / Bad Request with | Indicates validation error or badly formed copy request. Common issues |
| "code:" "1002" | include: a) Invalid or modified copyAuthorization payload. b) Expired value for |
|  | expirationDateTimeTicks token (copyAuthorization payload is valid for 24 hours). c) Invalid or unsupported targetResourceRegion. d) Invalid or malformed targetResourceId string. |
| Authorization failure due to missing or invalid authorization claims. | Occurs when the copyAuthorization payload or content is modified from the copyAuthorization API. Ensure that the payload is the same exact content that was returned from the earlier copyAuthorization call. |
| Couldn't retrieve authorization metadata. | Indicates that the copyAuthorization payload is being reused with a copy request. A copy request that succeeds doesn't allow any further requests that use the same copyAuthorization payload. If you raise a separate error and you later retry the copy with the same authorization payload, this error gets raised. The resolution is to generate a new copyAuthorization payload and then reissue the copy request. |
| Data transfer request isn't allowed as it downgrades to a less secure data protection scheme. | Occurs when copying between an AEK enabled resource to a non- AEK enabled resource. To allow copying encrypted model to the target as unencrypted, specify x-ms-forms-copy-degrade: true header with the copy request. |
| "Couldn't fetch information for Cognitive resource with ID ... ". | Indicates that the Azure resource indicated by the targetResourceId isn't a valid Cognitive resource or doesn't exist. To resolve this issue, verify and reissue the copy request. |
|  | Ensure the resource is valid and exists in the specified region, such as, westus2 |


<!---- Page 456 ---------------------------------------------------------------------------------------------------------------------------------->
Configure Azure AI services virtual
networks
Article · 01/31/2025
Azure AI services provide a layered security model. This model enables you to secure
your Azure AI services accounts to a specific subset of networks. When network rules are
configured, only applications that request data over the specified set of networks can
access the account. You can limit access to your resources with request filtering, which
allows requests that originate only from specified IP addresses, IP ranges, or from a list
of subnets in Azure Virtual Networks.
An application that accesses an Azure AI services resource when network rules are in
effect requires authorization. Authorization is supported with Microsoft Entra ID
credentials or with a valid API key.
1 Important
Turning on firewall rules for your Azure AI services account blocks incoming
requests for data by default. To allow requests through, one of the following
conditions needs to be met:
· The request originates from a service that operates within an Azure Virtual
Network on the allowed subnet list of the target Azure AI services account.
The endpoint request that originated from the virtual network needs to be set
as the custom subdomain of your Azure AI services account.
· The request originates from an allowed list of IP addresses.
Requests that are blocked include those from other Azure services, from the Azure
portal, and from logging and metrics services.
1 Note
We recommend that you use the Azure Az PowerShell module to interact with
Azure. To get started, see Install Azure PowerShell. To learn how to migrate to the
Az PowerShell module, see Migrate Azure PowerShell from AzureRM to Az.
Scenarios


<!---- Page 457 ---------------------------------------------------------------------------------------------------------------------------------->
To secure your Azure AI services resource, you should first configure a rule to deny
access to traffic from all networks, including internet traffic, by default. Then, configure
rules that grant access to traffic from specific virtual networks. This configuration
enables you to build a secure network boundary for your applications. You can also
configure rules to grant access to traffic from select public internet IP address ranges
and enable connections from specific internet or on-premises clients.
Network rules are enforced on all network protocols to Azure AI services, including REST
and WebSocket. To access data by using tools such as the Azure test consoles, explicit
network rules must be configured. You can apply network rules to existing Azure AI
services resources, or when you create new Azure AI services resources. After network
rules are applied, they're enforced for all requests.
Supported regions and service offerings
Virtual networks are supported in regions where Azure AI services are available . Azure
AI services support service tags for network rules configuration. The services listed here
are included in the CognitiveServicesManagement service tag.
V Anomaly Detector
V Azure OpenAI
V Content Moderator
V Custom Vision
V Face
V Language Understanding (LUIS)
V Personalizer
V Speech service
V Language
V QnA Maker
V Translator
1 Note
If you use Azure OpenAI, LUIS, Speech Services, or Language services, the
CognitiveServicesManagement tag only enables you to use the service by using the
SDK or REST API. To access and use the Azure Al Foundry_portal , LUIS portal,
Speech Studio, or Language Studio from a virtual network, you need to use the
following tags:
· AzureActiveDirectory
· AzureFrontDoor. Frontend


<!---- Page 458 ---------------------------------------------------------------------------------------------------------------------------------->
· AzureResourceManager
· CognitiveServicesManagement
· CognitiveServicesFrontEnd
· Storage (Speech Studio only)
For information on Azure Al Foundry_portal [ configurations, see the Azure Al
Foundry documentation.
Change the default network access rule
By default, Azure AI services resources accept connections from clients on any network.
To limit access to selected networks, you must first change the default action.
A
Warning
Making changes to network rules can impact your applications' ability to connect
to Azure AI services. Setting the default network rule to deny blocks all access to
the data unless specific network rules that grant access are also applied.
Before you change the default rule to deny access, be sure to grant access to any
allowed networks by using network rules. If you allow listing for the IP addresses for
your on-premises network, be sure to add all possible outgoing public IP addresses
from your on-premises network.
Manage default network access rules
You can manage default network access rules for Azure AI services resources through
the Azure portal, PowerShell, or the Azure CLI.
Azure portal
1. Go to the Azure AI services resource you want to secure.
2. Select Resource Management to expand it, then select Networking.


<!---- Page 459 ---------------------------------------------------------------------------------------------------------------------------------->
Home > contoso-rg > contoso-custom-vision
contoso-custom-vision
Custom vision
Networking
*
Directory: Microsoft
0
P Search
«
Firewalls and virtual networks
Private endpoint connections
Overview
Activity log
Save
Discard
Refresh
92 Access control (IAM)
Tags
Access control settings allowing access to Azure Al services account will remain in effect for up to three minutes after saving updated settings restricting access.
* Diagnose and solve problems
Allow access from
Resource Management
All networks
Selected Networks and Private Endpoints
Disabled
Keys and Endpoint
Configure network security for your Azure Al services account. Learn more.
Encryption
Virtual networks
Pricing tier
Secure your Azure Al services account with virtual networks.
+ Add existing virtual network
+ Add new virtual network
<> Networking
Virtual Network
Subnet
Address range
Endpoint Status
Resource group
Subscription
Identity
No network selected.
Cost analysis
Firewall
Properties
Add IP ranges to allow access from the internet or your on-premises networks. Learn more.
Add your client IP address @
Locks
+
Monitoring
Address range
Automation
IP address or CIDR
3. To deny access by default, under Firewalls and virtual networks, select
Selected Networks and Private Endpoints.
With this setting alone, unaccompanied by configured virtual networks or
address ranges, all access is effectively denied. When all access is denied,
requests that attempt to consume the Azure AI services resource aren't
permitted. The Azure portal, Azure PowerShell, or the Azure CLI can still be
used to configure the Azure AI services resource.
4. To allow traffic from all networks, select All networks.
Home > contoso-rg > contoso-custom-vision
contoso-custom-vision
Networking
Custom vision
Directory: Microsoft
0
P Search
«
Firewalls and virtual networks
Private endpoint connections
Overview
Activity log
Save
Discard
Refresh
& Access control (IAM)
Allow access from
Tags
All networks
Selected Networks and Private Endpoints
Disabled
* Diagnose and solve problems
All networks, including the internet, can access this resource. Learn more.
Resource Management
Keys and Endpoint
Encryption
Pricing tier
Networking
+
Identity
5. Select Save to apply your changes.
Grant access from a virtual network


<!---- Page 460 ---------------------------------------------------------------------------------------------------------------------------------->
You can configure Azure AI services resources to allow access from specific subnets only.
The allowed subnets might belong to a virtual network in the same subscription or in a
different subscription. The other subscription can belong to a different Microsoft Entra
tenant. When the subnet belongs to a different subscription, the
Microsoft.CognitiveServices resource provider needs to be also registered for that
subscription.
Enable a service endpoint for Azure AI services within the virtual network. The service
endpoint routes traffic from the virtual network through an optimal path to the Azure AI
service. For more information, see Virtual Network service endpoints.
The identities of the subnet and the virtual network are also transmitted with each
request. Administrators can then configure network rules for the Azure AI services
resource to allow requests from specific subnets in a virtual network. Clients granted
access by these network rules must continue to meet the authorization requirements of
the Azure AI services resource to access the data.
Each Azure AI services resource supports up to 100 virtual network rules, which can be
combined with IP network rules. For more information, see Grant access from an
internet IP range later in this article.
Set required permissions
To apply a virtual network rule to an Azure AI services resource, you need the
appropriate permissions for the subnets to add. The required permission is the default
Contributor role or the Cognitive Services Contributor role. Required permissions can also
be added to custom role definitions.
The Azure AI services resource and the virtual networks that are granted access might
be in different subscriptions, including subscriptions that are part of a different
Microsoft Entra tenant.
1 Note
Configuration of rules that grant access to subnets in virtual networks that are a
part of a different Microsoft Entra tenant are currently supported only through
PowerShell, the Azure CLI, and the REST APIs. You can view these rules in the Azure
portal, but you can't configure them.
Configure virtual network rules


<!---- Page 461 ---------------------------------------------------------------------------------------------------------------------------------->
You can manage virtual network rules for Azure AI services resources through the Azure
portal, PowerShell, or the Azure CLI.
Azure portal
To grant access to a virtual network with an existing network rule:
1. Go to the Azure AI services resource you want to secure.
2. Select Resource Management to expand it, then select Networking.
3. Confirm that you selected Selected Networks and Private Endpoints.
4. Under Allow access from, select Add existing virtual network.
contoso-custom-vision
Networking
Custom vision
Directory: Microsoft
0
Search
«
Firewalls and virtual networks
Private endpoint connections
Overview
Activity log
Save
Discard
Refresh
Access control (IAM)
Allow access from
Tags
All networks
Selected Networks and Private Endpoints
Disabled
* Diagnose and solve problems
Configure network security for your Azure Al services account. Learn more.
V Resource Management
Keys and Endpoint
Virtual networks
Secure your Azure Al services account with virtual networks.
+ Add existing virtual network
+ Add new virtual network
Virtual Network
Subnet
Address range
Endpoint Status
Resource group
Encryption
Pricing tier
No network selected.
Networking
Firewall
Identity
Add IP ranges to allow access from the internet or your on-premises networks. Learn more.
Add your client IP address
Cost analysis
Address range
+
Properties
IP address or CIDR
Locks
5. Select the Virtual networks and Subnets options, and then select Enable.


<!---- Page 462 ---------------------------------------------------------------------------------------------------------------------------------->
Add networks
X
Subscription *
Contoso Subscription
V
Virtual networks *
contoso-rg
V
Subnets *
default (Service endpoint required)
V
i
The following networks don't have service endpoints enabled
for 'Microsoft.CognitiveServices'. Enabling access will take up to
15 minutes to complete. After starting this operation, it is safe
to leave and return later if you do not wish to wait.
Enable
Virtual network
Service endpoint status
V contoso-rg
...
default
Not enabled
...
1 Note
If a service endpoint for Azure AI services wasn't previously configured
for the selected virtual network and subnets, you can configure it as part
of this operation.
Currently, only virtual networks that belong to the same Microsoft Entra
tenant are available for selection during rule creation. To grant access to a
subnet in a virtual network that belongs to another tenant, use
PowerShell, the Azure CLI, or the REST APIs.

| Virtual network | Service endpoint status |  |
| --- | --- | --- |
| V contoso-rg |  | ... |
| default | Not enabled | ... |


<!---- Page 463 ---------------------------------------------------------------------------------------------------------------------------------->
6. Select Save to apply your changes.
To create a new virtual network and grant it access:
1. On the same page as the previous procedure, select Add new virtual network.
contoso-custom-vision
Custom vision
Networking
Directory: Microsoft
0
Search
«
Firewalls and virtual networks
Private endpoint connections
Overview
Activity log
Save
Discard
Refresh
& Access control (IAM)
Tags
Allow access from
All networks
Selected Networks and Private Endpoints
Disabled
Diagnose and solve problems
Configure network security for your Azure Al services account. Learn more.
Resource Management
Virtual networks
Keys and Endpoint
Secure your Azure Al services account with virtual networks.
+ Add existing virtual network
+ Add new virtual network
Virtual Network
Subnet
Address range
Endpoint Status
Resource group
Encryption
Pricing tier
No network selected.
Networking
Firewall
Identity
Add IP ranges to allow access from the internet or your on-premises networks. Learn more.
Add your client IP address
Cost analysis
Address range
+
Properties
IP address or CIDR
Locks
2. Provide the information necessary to create the new virtual network, and then
select Create.


<!---- Page 464 ---------------------------------------------------------------------------------------------------------------------------------->
Create virtual network
X
* Name
widgets-vnet
V
* Address space 0
10.1.0.0/16
10.1.0.0 - 10.1.255.255 (65536 addresses)
* Subscription
widgets-subscription
V
* Resource group
widgets-resource-group
V
Create new
* Location
(US) West US 2
V
Subnet
* Name
default
* Address range 0
10.1.0.0/24
V
10.1.0.0 - 10.1.0.255 (256 addresses)
DDOS protection
0
Basic
Standard
Service endpoint 0
Microsoft.CognitiveServices
Firewall 0
Disabled
Enabled
Create
3. Select Save to apply your changes.
To remove a virtual network or subnet rule:
1. On the same page as the previous procedures, select ... (More options) to
open the context menu for the virtual network or subnet, and select Remove.


<!---- Page 465 ---------------------------------------------------------------------------------------------------------------------------------->
Firewalls and virtual networks
Private endpoint connections
1
Save
Discard
Refresh
Allow access from
All networks
Selected Networks and Private Endpoints
Disabled
Configure network security for your Azure Al services account. Learn more.
Virtual networks
Secure your Azure Al services account with virtual networks.
+ Add existing virtual network + Add new virtual network
Subscription
O
Remove
Firewall
Add IP ranges to allow access from the internet or your on-premises networks. Learn more.
Add your client IP address @
Address range
+
IP address or CIDR
Virtual Network
Subnet
Address range
Endpoint Status
Resource group
> contoso-01-vnet
1
contoso-rg
2. Select Save to apply your changes.
1 Important
Be sure to set the default rule to deny, or network rules have no effect.
Grant access from an internet IP range
You can configure Azure AI services resources to allow access from specific public
internet IP address ranges. This configuration grants access to specific services and on-
premises networks, which effectively block general internet traffic.
You can specify the allowed internet address ranges by using CIDR format (RFC 4632) 2
in the form 192.168.0.0/16 or as individual IP addresses like 192.168.0.1.
? Tip
Small address ranges that use /31 or /32 prefix sizes aren't supported. Configure
these ranges by using individual IP address rules.
IP network rules are only allowed for public internet IP addresses. IP address ranges
reserved for private networks aren't allowed in IP rules. Private networks include
addresses that start with 10 .* , 172.16 .* - 172.31 .* , and 192.168 .* . For more
information, see Private Address Space (RFC 1918) .
Currently, only IPv4 addresses are supported. Each Azure AI services resource supports
up to 100 IP network rules, which can be combined with virtual network rules.

| Virtual Network | Subnet | Address range | Endpoint Status | Resource group |
| --- | --- | --- | --- | --- |
| > contoso-01-vnet | 1 |  |  | contoso-rg |


<!---- Page 466 ---------------------------------------------------------------------------------------------------------------------------------->
Configure access from on-premises networks
To grant access from your on-premises networks to your Azure AI services resource with
an IP network rule, identify the internet-facing IP addresses used by your network.
Contact your network administrator for help.
If you use Azure ExpressRoute on-premises for Microsoft peering, you need to identify
the NAT IP addresses. For more information, see What is Azure ExpressRoute.
For Microsoft peering, the NAT IP addresses that are used are either customer provided
or supplied by the service provider. To allow access to your service resources, you must
allow these public IP addresses in the resource IP firewall setting.
Managing IP network rules
You can manage IP network rules for Azure AI services resources through the Azure
portal, PowerShell, or the Azure CLI.
Azure portal
1. Go to the Azure AI services resource you want to secure.
2. Select Resource Management to expand it, then select Networking.
3. Confirm that you selected Selected Networks and Private Endpoints.
4. Under Firewalls and virtual networks, locate the Address range option. To
grant access to an internet IP range, enter the IP address or address range (in
CIDR format ). Only valid public IP (nonreserved) addresses are accepted.


<!---- Page 467 ---------------------------------------------------------------------------------------------------------------------------------->
Firewalls and virtual networks
Private endpoint connections
Save
Discard
Refresh
Allow access from
All networks
Selected Networks and Private Endpoints
Disabled
Configure network security for your Azure Al services account. Learn more.
Virtual networks
Secure your Azure Al services account with virtual networks.
+ Add existing virtual network
+ Add new virtual network
Firewall
Add IP ranges to allow access from the internet or your on-premises networks. Learn more.
Add your client IP address
Address range
IP address or CIDR
+
Virtual Network
Subnet
Address range
Endpoint Status
Resource group
No network selected.
To remove an IP network rule, select the trash can @ icon next to the address
range.
5. Select Save to apply your changes.
1 Important
Be sure to set the default rule to deny, or network rules have no effect.
Use private endpoints
You can use private endpoints for your Azure AI services resources to allow clients on a
virtual network to securely access data over Azure Private Link. The private endpoint
uses an IP address from the virtual network address space for your Azure AI services
resource. Network traffic between the clients on the virtual network and the resource
traverses the virtual network and a private link on the Microsoft Azure backbone
network, which eliminates exposure from the public internet.
Private endpoints for Azure AI services resources let you:
· Secure your Azure Al services resource by configuring the firewall to block all
connections on the public endpoint for the Azure AI service.
. Increase security for the virtual network, by enabling you to block exfiltration of
data from the virtual network.

| Virtual Network | Subnet | Address range | Endpoint Status | Resource group |
| --- | --- | --- | --- | --- |
| No network selected. |  |  |  |  |


<!---- Page 468 ---------------------------------------------------------------------------------------------------------------------------------->
. Securely connect to Azure Al services resources from on-premises networks that
connect to the virtual network by using Azure VPN Gateway or ExpressRoutes with
private-peering.
Understand private endpoints
A private endpoint is a special network interface for an Azure resource in your virtual
network. Creating a private endpoint for your Azure AI services resource provides secure
connectivity between clients in your virtual network and your resource. The private
endpoint is assigned an IP address from the IP address range of your virtual network.
The connection between the private endpoint and the Azure AI service uses a secure
private link.
Applications in the virtual network can connect to the service over the private endpoint
seamlessly. Connections use the same connection strings and authorization mechanisms
that they would use otherwise. The exception is Speech Services, which require a
separate endpoint. For more information, see Private endpoints with the Speech
Services in this article. Private endpoints can be used with all protocols supported by the
Azure AI services resource, including REST.
Private endpoints can be created in subnets that use service endpoints. Clients in a
subnet can connect to one Azure AI services resource using private endpoint, while
using service endpoints to access others. For more information, see Virtual Network
service endpoints.
When you create a private endpoint for an Azure AI services resource in your virtual
network, Azure sends a consent request for approval to the Azure AI services resource
owner. If the user who requests the creation of the private endpoint is also an owner of
the resource, this consent request is automatically approved.
Azure AI services resource owners can manage consent requests and the private
endpoints through the Private endpoint connection tab for the Azure AI services
resource in the Azure portal .
Specify private endpoints
When you create a private endpoint, specify the Azure AI services resource that it
connects to. For more information on creating a private endpoint, see:
· Create a private endpoint by using the Azure portal
. Create a private endpoint by using Azure PowerShell
. Create a private endpoint by using the Azure CLI


<!---- Page 469 ---------------------------------------------------------------------------------------------------------------------------------->
Connect to private endpoints
O
Note
Azure OpenAI Service uses a different private DNS zone and public DNS zone
forwarder than other Azure AI services. For the correct zone and forwarder names,
see Azure services DNS zone configuration.
Clients on a virtual network that use the private endpoint use the same connection
string for the Azure AI services resource as clients connecting to the public endpoint.
The exception is the Speech service, which requires a separate endpoint. For more
information, see Use private endpoints with the Speech service in this article. DNS
resolution automatically routes the connections from the virtual network to the Azure AI
services resource over a private link.
By default, Azure creates a private DNS zone attached to the virtual network with the
necessary updates for the private endpoints. If you use your own DNS server, you might
need to make more changes to your DNS configuration. For updates that might be
required for private endpoints, see Apply DNS changes for private endpoints in this
article.
Use private endpoints with the Speech service
See Use Speech service through a private endpoint.
Apply DNS changes for private endpoints
When you create a private endpoint, the DNS CNAME resource record for the Azure AI
services resource is updated to an alias in a subdomain with the prefix privatelink. By
default, Azure also creates a private DNS zone that corresponds to the privatelink
subdomain, with the DNS A resource records for the private endpoints. For more
information, see What is Azure Private DNS.
When you resolve the endpoint URL from outside the virtual network with the private
endpoint, it resolves to the public endpoint of the Azure AI services resource. When it's
resolved from the virtual network hosting the private endpoint, the endpoint URL
resolves to the private endpoint's IP address.
This approach enables access to the Azure AI services resource using the same
connection string for clients in the virtual network that hosts the private endpoints and
clients outside the virtual network.


<!---- Page 470 ---------------------------------------------------------------------------------------------------------------------------------->
If you use a custom DNS server on your network, clients must be able to resolve the fully
qualified domain name (FQDN) for the Azure AI services resource endpoint to the
private endpoint IP address. Configure your DNS server to delegate your private link
subdomain to the private DNS zone for the virtual network.
? Tip
When you use a custom or on-premises DNS server, you should configure your
DNS server to resolve the Azure AI services resource name in the privatelink
subdomain to the private endpoint IP address. Delegate the privatelink
subdomain to the private DNS zone of the virtual network. Alternatively, configure
the DNS zone on your DNS server and add the DNS A records.
For more information on configuring your own DNS server to support private endpoints,
see the following resources:
. Name resolution that uses your own DNS server
· DNS configuration
Grant access to trusted Azure services for Azure
OpenAI
You can grant a subset of trusted Azure services access to Azure OpenAI, while
maintaining network rules for other apps. These trusted services will then use managed
identity to authenticate your Azure OpenAI service. The following table lists the services
that can access Azure OpenAI if the managed identity of those services have the
appropriate role assignment.
[ Expand table
Service
Resource provider name
Azure AI Services
Microsoft. CognitiveServices
Azure Machine Learning
Microsoft. MachineLearningServices
Azure AI Search
Microsoft. Search
You can grant networking access to trusted Azure services by creating a network rule
exception using the REST API or Azure portal:

| Service | Resource provider name |
| --- | --- |
| Azure AI Services | Microsoft. CognitiveServices |
| Azure Machine Learning | Microsoft. MachineLearningServices |
| Azure AI Search | Microsoft. Search |


<!---- Page 471 ---------------------------------------------------------------------------------------------------------------------------------->
Using the Azure CLI
Bash
accessToken=$(az account get-access-token -- resource
https://management.azure.com -- query "accessToken" -- output tsv)
rid="/subscriptions/<your subscription id>/resourceGroups/<your resource
group>/providers/Microsoft.CognitiveServices/accounts/<your Azure AI
resource name>"
curl -i -X PATCH https://management.azure.com$rid?api-version=2023-10-01-
preview \
-H "Content-Type: application/json" |
-H "Authorization: Bearer $accessToken" |
-d \
'
{
"properties":
{
"networkAcls": {
"bypass": "AzureServices"
}
}
}
'
To revoke the exception, set networkAcls. bypass to None.
To verify if the trusted service has been enabled from the Azure portal,
1. Use the JSON View from the Azure OpenAI resource overview page
Home
>
my-openai-resource
X
Azure OpenAl
P Search
«
Go to Azure OpenAl Studio
Delete
Overview
A
^ Essentials
View Cost
JSON View
Activity log
Resource group (move) : my-resource-group
API Kind : OpenAl
Access control (IAM)
Status
: Active
Pricing tier
: Standard
· Tags
Location
: East US 2
Endpoints
: Click here to view endpoints
X Diagnose and solve problems
Subscription (move)
: My subscription
Manage keys : Click here to manage keys
Subscription ID
: abcd1234
. Resource visualizer
Tags (edit)
: Add tags
Resource Management
Get Started
Develop
Monitor
+
Keys and Endpoint
Build your own secure copilot and generative Al applications with Azure OpenAl Service
Model deployments
Deploy an Azure OpenAl model and start making API calls. Connect your own data, call
functions, and improve workflow with Azure OpenAl language, image and speech
Encryption
2. Choose your latest API version under API versions. Only the latest API version is
supported, 2023-10-01-preview .


<!---- Page 472 ---------------------------------------------------------------------------------------------------------------------------------->
Resource JSON
X
Resource ID
API Versions
/subscriptions/
/resourceGroups/
/providers/Micros
0
2023-10-01-preview
75
"networkAcls": {
76
"bypass": "AzureServices",
77
"defaultAction": "Deny",
+
78
"virtualNetworkRules": [],
79
"ipRules": []
80
},
Using the Azure portal
1. Navigate to your Azure OpenAI resource, and select Networking from the
navigation menu.
2. Under Exceptions, select Allow Azure services on the trusted services list to
access this cognitive services account.
? Tip
You can view the Exceptions option by selecting either Selected networks
and private endpoints or Disabled under Allow access from.
Dashboard >
test
Azure OpenAl
Networking
...
Search
Hrewalls and virtual networks
Private endpoint connections
Overview
A
Save
Discard
Refresh
Activity log
Access control (IAM)
i
Access control settings allowing access to Azure Al services account will remain in effect for up to three minutes after saving u
Tags
Allow access from
All networks
Selected Networks and Private Endpoints
Disabled
Diagnose and solve problems
Resource visualizer
Configure network security for your Azure Al services account. Learn more.
Favorites
Virtual networks
Networking
Secure your Azure Al services account with virtual networks. + Add existing virtual network + Add new virtual network
Identity
Virtual Network
Subnet
Address range
Enc
Resource Management
No network selected.
Keys and Endpoint
Firewall
Model deployments
Add IP ranges to allow access from the internet or your on-premises networks. Learn more.
Add your client IP address ("
1 1
& Encryption
Pricing tier
Address range
Networking
I
Identity
IM
$ Cost analysis
IP address or CIDR
Properties
Exceptions
Allow Azure services on the trusted services list to access this cognitive services account.
+
Locks

| 75 | :unselected: | "networkAcls": { |  |
| --- | --- | --- | --- |
| 76 |  | "bypass": "AzureServices", |  |
| 77 |  | "defaultAction": "Deny", |  |
| 78 |  | "virtualNetworkRules": [], + |  |
| 79 |  | "ipRules": [] :unselected: |  |
| 80 | :unselected: | }, |  |


<!---- Page 473 ---------------------------------------------------------------------------------------------------------------------------------->
Pricing
For pricing details, see Azure Private Link pricing E .
Next steps
. Explore the various Azure Al services
· Learn more about Virtual Network service endpoints
Feedback
Was this page helpful?
Yes
P
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 474 ---------------------------------------------------------------------------------------------------------------------------------->
Managed identities for Document
Intelligence
Article · 11/19/2024
This content applies to:
v4.0 (GA)
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
Managed identities for Azure resources are service principals that create a Microsoft
Entra identity and specific permissions for Azure managed resources:
Document Intelligence Enable
Resource
System-assigned
managed identity
Assign
Storage Blob
Data Reader
Access
Azure Blob
Storage
· Managed identities grant access to any resource that supports Microsoft Entra
authentication, including your own applications. Unlike security keys and
authentication tokens, managed identities eliminate the need for developers to
manage credentials.
. You can grant access to an Azure resource and assign an Azure role to a managed
identity using Azure role-based access control (Azure RBAC). There's no added cost
to use managed identities in Azure.
1 Important
· Managed identities eliminate the need for you to manage credentials,
including Shared Access Signature (SAS) tokens.
· Managed identities are a safer way to grant access to data without having
credentials in your code.
Private storage account access
Private Azure storage account access and authentication support managed identities for
Azure resources. If you have an Azure storage account, protected by a Virtual Network
(VNet ) or firewall, Document Intelligence can't directly access your storage account data.
However, once a managed identity is enabled, Document Intelligence can access your
storage account using an assigned managed identity credential.


<!---- Page 475 ---------------------------------------------------------------------------------------------------------------------------------->
1 Note
. If you intend to analyze your storage data with the Document Intelligence
Sample Labeling tool (FOTT) , you must deploy the tool behind your VNet
or firewall.
. The Analyze Receipt, Business Card, Invoice, ID document, and Custom
Form APIs can extract data from a single document by posting requests as
raw binary content. In these scenarios, there is no requirement for a managed
identity credential.
Prerequisites
To get started, you need:
. An active Azure account -if you don't have one, you can create a free
account.
. A Document Intelligence or Azure Al services [ resource in the Azure portal.
For detailed steps, see Create an Azure AI services resource.
. An Azure blob storage account in the same region as your Document
Intelligence resource. You also need to create containers to store and organize
your blob data within your storage account.
o If your storage account is behind a firewall, you must enable the following
configuration:
o On your storage account page, select Security + networking - Networking
Security + networking
Networking
Azure CDN
Access keys
Shared access signature
Encryption
Security
from the left menu.


<!---- Page 476 ---------------------------------------------------------------------------------------------------------------------------------->
o In the main window, select Allow access from selected networks.
Firewalls and virtual networks
Private endpoint connections
Custom domain
Save
Discard
Refresh
Firewall settings allowing access to storage services will remain in effect for up to a minute after saving updated settings restricting access.
Allow access from
All networks
Selected networks
Configure network security for your storage accounts. Learn more
o On the selected networks page, navigate to the Exceptions category and make
certain that the Allow Azure services on the trusted services list to access
this storage account checkbox is enabled.
Exceptions
Allow Azure services on the trusted services list to access this storage account. @
Allow read access to storage logging from any network
Allow read access to storage metrics from any network
· A brief understanding of Azure role-based access control (Azure RBAC) using the
Azure portal.
Managed identity assignments
There are two types of managed identity: system-assigned and user-assigned.
Currently, Document Intelligence only supports system-assigned managed identity:
· A system-assigned managed identity is enabled directly on a service instance. It
isn't enabled by default; you must go to your resource and update the identity
setting.
· The system-assigned managed identity is tied to your resource throughout its
lifecycle. If you delete your resource, the managed identity is deleted as well.
In the following steps, we enable a system-assigned managed identity and grant
Document Intelligence limited access to your Azure blob storage account.
Enable a system-assigned managed identity
1 Important


<!---- Page 477 ---------------------------------------------------------------------------------------------------------------------------------->
To enable a system-assigned managed identity, you need
Microsoft.Authorization/roleAssignments/write permissions, such as Owner or
User Access Administrator. You can specify a scope at four levels: management
group, subscription, resource group, or resource.
1. Sign in to the Azure portal using an account associated with your Azure
subscription.
2. Navigate to your Document Intelligence resource page in the Azure portal.
3. In the left rail, Select Identity from the Resource Management list:
Resource Management
Quick start
Keys and Endpoint
Encryption
Pricing tier
Networking
Identity
Billing By Subscription
Properties
Locks
4. In the main window, toggle the System assigned Status tab to On.
Grant access to your storage account
You need to grant Document Intelligence access to your storage account before it can
read blobs. Now that Document Intelligence access is enabled with a system-assigned
managed identity, you can use Azure role-based access control (Azure RBAC), to give
Document Intelligence access to Azure storage. The Storage Blob Data Reader role
gives Document Intelligence (represented by the system-assigned managed identity)
read and list access to the blob container and data.
1. Under Permissions select Azure role assignments:


<!---- Page 478 ---------------------------------------------------------------------------------------------------------------------------------->
System assigned
User assigned
A system assigned managed identity is restricted to one per resource and is tied to the lifecycle of this resource. You can
grant permissions to the managed identity by using Azure role-based access control (Azure RBAC). The managed identity
is authenticated with Azure AD, so you don't have to store any credentials in code. Learn more about Managed identities.
Save
Discard
Refresh
Got feedback?
Status
Off
On
Object ID @
D
Permissions
Azure role assignments
2. On the Azure role assignments page that opens, choose your subscription from
the drop-down menu then select + Add role assignment.
Azure role assignments
+ Add role assignment (Preview)
Refresh
If this identity has role assignments that you don't have permission to read, they won't be shown in the list. Learn more
Subscription *
V
1 Note
If you're unable to assign a role in the Azure portal because the Add > Add
role assignment option is disabled or you get the permissions error, "you do
not have permissions to add role assignment at this scope", check that you're
currently signed in as a user with an assigned a role that has
Microsoft.Authorization/roleAssignments/write permissions such as Owner or
User Access Administrator at the Storage scope for the storage resource.
3. Next, you're going to assign a Storage Blob Data Reader role to your Document
Intelligence service resource. In the Add role assignment pop-up window,
complete the fields as follows and select Save:
Expand table
Field
Value
Scope
Storage

| Field | Value |
| --- | --- |
| Scope | Storage |


<!---- Page 479 ---------------------------------------------------------------------------------------------------------------------------------->
Field
Value
Subscription
The subscription associated with your storage resource.
Resource
The name of your storage resource
Role
Storage Blob Data Reader-allows for read access to Azure Storage blob
containers and data.
Add role assignment (Preview)
×
Scope ®
Storage
V
Subscription
V
Resource @
V
Role @
Storage Blob Data Reader ®
V
Learn more about RBAC
Save
Discard
4. After you receive the Added Role assignment confirmation message, refresh the
page to see the added role assignment.
Added Role assignment
X
5. If you don't see the change right away, wait and try refreshing the page once
more. When you assign or remove role assignments, it can take up to 30 minutes
for changes to take effect.
Azure role assignments
+ Add role assignment (Preview)
Refresh
If this identity has role assignments that you don't have permission to read, they won't be shown in the list. Learn more
Subscription *
V
Role
Resource Name
Resource Type
Assigned To
Storage Blob Data Reader
YourStorage
Storage account
Your-Form-Recognizer-Service
That's it! You completed the steps to enable a system-assigned managed identity. With
managed identity and Azure RBAC, you granted Document Intelligence specific access

| Field | Value |
| --- | --- |
| Subscription | The subscription associated with your storage resource. |
| Resource | The name of your storage resource |
| Role | Storage Blob Data Reader-allows for read access to Azure Storage blob containers and data. |


<!---- Page 480 ---------------------------------------------------------------------------------------------------------------------------------->
rights to your storage resource without having to manage credentials such as SAS
tokens.
Other role assignments for Document Intelligence Studio
If you're going to use Document Intelligence Studio and your storage account is
configured with network restriction such as firewall or virtual network, another role,
Storage Blob Data Contributor, needs to be assigned to your Document Intelligence
service. Document Intelligence Studio requires this role to write blobs to your storage
account when you perform Auto label, Human in the loop, or Project sharing/upgrade
operations.
Azure role assignments
+ Add role assignment (Preview)
Refresh
If this identity has role assignments that you don't have permission to read, they won't be shown in the list. Learn more
Subscription *
Role
Resource Name
Resource Type
Assigned To
Condition
Storage Blob Data Contributor
YourStorage
Storage account
Your-Document-Intelligence-Resource
None
Next steps
Configure secure access with managed identities and private endpoints
Feedback
Was this page helpful?
Yes
P
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 481 ---------------------------------------------------------------------------------------------------------------------------------->
Configure secure access with managed
identities and virtual networks
Article · 11/19/2024
This content applies to:
v4.0 (GA)
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
This how-to guide walks you through the process of enabling secure connections for
your Document Intelligence resource. You can secure the following connections:
· Communication between a client application within a Virtual Network ( VNET ) and
your Document Intelligence Resource.
· Communication between Document Intelligence Studio and your Document
Intelligence resource.F
· Communication between your Document Intelligence resource and a storage
account (needed when training a custom model).
You're setting up your environment to secure the resources:
Azure Virtual Network
Firewalls
CORS
enabled
Allow IP
1
Document Intelligence Studio
Blob Storage
Account
Storage Blob
Data Contributor
3
Allow IP
O-
0
Document Intelligence
Managed Identity
2
o
9
Document
Intelligence Service
</>
Code
Analyze new documents w/ client IP allowed-
4
Prerequisites
To get started, you need:
. An active Azure account -if you don't have one, you can create a free
account.
. A Document Intelligence or Azure Al services [ resource in the Azure portal.
For detailed steps, see Create an Azure AI services resource.


<!---- Page 482 ---------------------------------------------------------------------------------------------------------------------------------->
. An Azure blob storage account in the same region as your Document
Intelligence resource. Create containers to store and organize your blob data
within your storage account.
. An Azure virtual network in the same region as your Document Intelligence
resource. Create a virtual network to deploy your application resources to train
models and analyze documents.
· An Azure data science VM for Windows or Linux/Ubuntu to optionally deploy a
data science VM in the virtual network to test the secure connections being
established.
Configure resources
Configure each of the resources to ensure that the resources can communicate with
each other:
. Configure the Document Intelligence Studio to use the newly created Document
Intelligence resource by accessing the settings page and selecting the resource.
· Ensure and validate that the configuration works by selecting the Read API and
analyzing a sample document. If the resource was configured correctly, the request
successfully completes.
. Add a training dataset to a container in the Storage account you created.
. Select the custom model tile to create a custom project. Ensure that you select the
same Document Intelligence resource and the storage account you created in the
previous step.
. Select the container with the training dataset you uploaded in the previous step.
Ensure that if the training dataset is within a folder, the folder path is set
appropriately.
. Ensure that you have the required permissions, the Studio sets the CORS setting
required to access the storage account. If you don't have the permissions, you
need to make certain that the CORS settings are configured on the Storage
account before you can proceed.
. Ensure and validate that the Studio is configured to access your training data. If
you can see your documents in the labeling experience, all the required
connections are established.


<!---- Page 483 ---------------------------------------------------------------------------------------------------------------------------------->
You now have a working implementation of all the components needed to build a
Document Intelligence solution with the default security model:
CORS
enabled
1
Document Intelligence Studio
Blob Storage
Account
3
O-
2
O-
O
9
Document
Intelligence Service
</>
Code
Analyze new documents
4
Next, complete the following steps:
. Configure managed identity on the Document Intelligence resource.
· Secure the storage account to restrict traffic from only specific virtual networks and
IP addresses.
. Configure the Document Intelligence managed identity to communicate with the
storage account.
· Disable public access to the Document Intelligence resource and create a private
endpoint. Your resource is then only accessible from specific virtual networks and
IP addresses.
. Add a private endpoint for the storage account in a selected virtual network.
. Ensure and validate that you can train models and analyze documents from within
the virtual network.
Setup managed identity for Document
Intelligence
Navigate to the Document Intelligence resource in the Azure portal and select the
Identity tab. Toggle the System assigned managed identity to On and save the changes:


<!---- Page 484 ---------------------------------------------------------------------------------------------------------------------------------->
V
Form recognizer
Search (Ctrl+/)
«
System assigned
User assigned
1
Overview
4
A system assigned managed identity is restricted to one per resource and is tied to the lifecycle of this re
you don't have to store any credentials in code. Learn more about Managed identities.
Activity log
Access control (IAM)
Save
Discard
Refresh
Got feedback?
Tags
Diagnose and solve problems
Status
0
Resource Management
Off
On
Keys and Endpoint
Encryption
Commitment tier pricing
Pricing tier
Networking
Identity
Cost analysis
Properties
Locks
Monitoring
Alerts
Metrics
Secure the Storage account
Start configuring secure communications by navigating to the Networking tab on your
Storage account in the Azure portal.
1. Under Firewalls and virtual networks, choose Enabled from selected virtual
networks and IP addresses from the Public network access list.
2. Ensure that Allow Azure services on the trusted services list to access this storage
account is selected from the Exceptions list.
3. Save your changes.


<!---- Page 485 ---------------------------------------------------------------------------------------------------------------------------------->
Storage account
Search (Ctrl+/)
«
Firewalls and virtual networks
Private endpoint connections
Custom domain
Overview
a
Activity log
P
Save
Discard
Refresh
Tags
Diagnose and solve problems
Access Control (IAM)
Public network access
Enabled from all networks
Enabled from selected virtual networks and IP addresses
Disabled
Data migration
Configure network security for your storage accounts. Learn more '
Events
Virtual networks
Add existing virtual network
Add new virtual network
Storage browser (preview)
Data storage
Virtual Network
Subnet
Address range
Endpoint Status
Containers
No network selected.
File shares
Firewall
Queues
Add IP ranges to allow access from the internet or your on-premises networks. Learn more.
Add your client IP address ('.
®
um Tables
Security + networking
Address range
& Networking
IP address or CIDR
Azure CDN
Resource instances
Access keys
Specify resource instances that will have access to your storage account based on their system-assigned managed identity.
Shared access signature
Resource type
Instance name
Encryption
Select a resource type
Select one or more instances
V
Security
Exceptions
Allow Azure services on the trusted services list to access this storage account. @
Data management
Allow read access to storage logging from any network
Geo-replication
Allow read access to storage metrics from any network
!
Note
Your storage account won't be accessible from the public internet.
Refreshing the custom model labeling page in the Studio will result in an error
message.
Enable access to storage from Document
Intelligence
To ensure that the Document Intelligence resource can access the training dataset, you
need to add a role assignment for your managed identity.
1. Staying on the storage account window in the Azure portal, navigate to the Access
Control (IAM) tab in the left navigation bar.
2. Select the Add role assignment button.


<!---- Page 486 ---------------------------------------------------------------------------------------------------------------------------------->
Add role assignment
Got feedback?
Role
Members
Review + assign
A role definition is a collection of permissions. You can use the built-in roles or you can create your own
custom roles. Learn more "
P Storage Blob
×
Type : All
Category : All
Showing 4 of 40 roles
Name 12
Description 14
Storage Blob Data Contributor
Allows for read, write and delete access to Azure Storage blob containers and data
Storage Blob Data Owner
Allows for full access to Azure Storage blob containers and data, including assigning POSIX access control.
Storage Blob Data Reader
Allows for read access to Azure Storage blob containers and data
Storage Blob Delegator
Allows for generation of a user delegation key which can be used to sign SAS tokens
3. On the Role tab, search for and select the Storage Blob Data Contributor
permission and select Next.
Storage account
P Search (Ctrl+/)
«
Add
Download role assignments
== Edit columns
Refresh
Remove
Got feedback?
Overview
4
Activity log
Check access
Role assignments
Roles
Deny assignments
Classic administrators
Tags
My access
View my level of access to this resource.
Diagnose and solve problems
Grant access to this resource
Access Control (IAM)
View my access
Grant access to resources by assigning a role.
Data migration
Check access
Review the level of access a user, group, service principal, or
managed identity has to this resource. Learn more '
Events
Storage browser (preview)
Find
0
Add role assignment
Learn more
User, group, or service principal
Data storage
Managed identity
View deny assignments
Containers
File shares
Search by name or email address
View the role assignments that have been denied
access to specific actions at this scope.
Queues
Tables
View
Learn more
Security + networking
Networking
4. On the Members tab, select the Managed identity option and choose + Select
members
5. On the Select managed identities dialog window, select the following options:
· Subscription. Select your subscription.
· Managed Identity. Select Form Recognizer.
. Select. Choose the Document Intelligence resource you enabled with a
managed identity.

| Showing 4 of 40 roles |  |
| --- | --- |
| Name 12 | Description 14 |
| Storage Blob Data Contributor | Allows for read, write and delete access to Azure Storage blob containers and data |
| Storage Blob Data Owner | Allows for full access to Azure Storage blob containers and data, including assigning POSIX access control. |
| Storage Blob Data Reader | Allows for read access to Azure Storage blob containers and data |
| Storage Blob Delegator | Allows for generation of a user delegation key which can be used to sign SAS tokens |
|  |  |


<!---- Page 487 ---------------------------------------------------------------------------------------------------------------------------------->
Home > vikurpadwork >
Add role assignment
Select managed identities
A
Got feedback?
Got feedback?
Subscription *
Role
Members
Conditions (optional)
Review + assign
Visual Studio Enterprise Subscription
Managed identity
Form recognizer (1)
Selected role
Storage Blob Data Reader
Select @
Assign access to
User, group, or service principal
Search by name
Managed identity
Members
+ Select members
Mama
Ahiart In
Tura
6. Close the dialog window.
7. Finally, select Review + assign to save your changes.
Great! You configured your Document Intelligence resource to use a managed identity
to connect to a storage account.
? Tip
When you try the Document Intelligence Studio [, you'll see the READ API and
other prebuilt models don't require storage access to process documents. However,
training a custom model requires additional configuration because the Studio can't
directly communicate with a storage account. You can enable storage access by
selecting Add your client IP address from the Networking tab of the storage
account to configure your machine to access the storage account via IP allowlisting.
Configure private endpoints for access from
VNET S
1 Note
. The resources are only accessible from the virtual network.
· Some Document Intelligence features in the Studio like auto label require the
Document Intelligence Studio to have access to your storage account.
. Add our Studio IP address, 20.3.165.95, to the firewall allowlist for both
Document Intelligence and Storage Account resources. This is Document
Intelligence Studio's dedicated IP address and can be safely allowed.
When you connect to resources from a virtual network, adding private endpoints
ensures both the storage account, and the Document Intelligence resource are
accessible from the virtual network.


<!---- Page 488 ---------------------------------------------------------------------------------------------------------------------------------->
Next, configure the virtual network to ensure only resources within the virtual network
or traffic router through the network have access to the Document Intelligence resource
and the storage account.
Enable your firewalls and virtual networks
1. In the Azure portal, navigate to your Document Intelligence resource.
2. Select the Networking tab from the left navigation bar.
3. Enable the Selected Networking and Private Endpoints option from the Firewalls
and virtual networks tab and select save.
1 Note
If you try accessing any of the Document Intelligence Studio features, you'll see an
access denied message. To enable access from the Studio on your machine, select
the Add your client IP address checkbox and Save to restore access.
Form recognizer
P Search (Ctrl+/)
«
i
Updating
Overview
4
Activity log
Firewalls and virtual networks
Private endpoint connections
Access control (IAM)
Save
Discard
Refresh
Tags
Diagnose and solve problems
i
Access control settings allowing access to cognitive service will remain in effect for up to three minutes after saving updated settings restricting access.
Resource Management
Keys and Endpoint
Allow access from
All networks
Selected Networks and Private Endpoints
Disabled
Encryption
Configure network security for your cognitive service. Learn more.
Commitment tier pricing
Virtual networks
Pricing tier
Secure your cognitive service with virtual networks. + Add existing virtual network + Add new virtual network
<> Networking
Virtual Network
Subnet
Address range
Endpoint
Identity
No network selected.
$ Cost analysis
Firewall
Properties
Add IP ranges to allow access from the internet or your on-premises networks. Learn more.
& Locks
Add your client IP address ('
®
Monitoring
Address range
Alerts
IP address or CIDR
Configure your private endpoint
1. Navigate to the Private endpoint connections tab and select the + Private
endpoint. You're navigated to the Create a private endpoint dialog page.
2. On the Create private endpoint dialog page, select the following options:
· Subscription. Select your billing subscription.


<!---- Page 489 ---------------------------------------------------------------------------------------------------------------------------------->
· Resource group. Select the appropriate resource group.
· Name. Enter a name for your private endpoint.
· Region. Select the same region as your virtual network.
· Select Next: Resource.
Create a private endpoint
1
Basics
2
Resource
3
Virtual Network
4
Tags
5
Review + create
Use private endpoints to privately connect to a service or resource. Your private endpoint must be in the same region as your
virtual network, but can be in a different region from the private link resource that you are connecting to. Learn more
Project details
Subscription *
0
Visual Studio Enterprise Subscription
V
Resource group * @
work-related
V
Create new
Instance details
Name *
my-fr-endpoint
V
Region *
Southeast Asia
V
Configure your virtual network
1. On the Resource tab, accept the default values and select Next: Virtual Network.
2. On the Virtual Network tab, make sure that you select the virtual network that you
created.
3. If you have multiple subnets, select the subnet where you want the private
endpoint to connect. Accept the default value to Dynamically allocate IP address.
4. Select Next: DNS
5. Accept the default value Yes to integrate with private DNS zone.


<!---- Page 490 ---------------------------------------------------------------------------------------------------------------------------------->
Create a private endpoint
Basics
Resource
Virtual Network
4
Tags
5
Review + create
Networking
To deploy the private endpoint, select a virtual network subnet. Learn more
Virtual network *
®
Subnet *
®
Private DNS integration
To connect privately with your private endpoint, you need a DNS record. We recommend that you integrate your private
endpoint with a private DNS zone. You can also utilize your own DNS servers or create DNS records using the host files on your
virtual machines. Learn more
Integrate with private DNS zone
Yes
No
Configuration name
Subscription
Resource group
Private DNS zone
privatelink-cognitiveservices-azure-c ...
Visual Studio Enterprise Subscrip ...
work-related
V
(new) privatelink.cognitiveservices.az ...
sample-vnet
V
sample-vnet/private-endpoints (10.0.1.0/24)
V
6. Accept the remaining defaults and select Next: Tags.
7. Select Next: Review + create .
Well done! Your Document Intelligence resource now is only accessible from the virtual
network and any IP addresses in the IP allowlist.
Configure private endpoints for storage
Navigate to your storage account on the Azure portal.
1. Select the Networking tab from the left navigation menu.
2. Select the Private endpoint connections tab.
3. Choose add + Private endpoint.
4. Provide a name and choose the same region as the virtual network.
5. Select Next: Resource.

| sample-vnet | V |
| --- | --- |
|  |  |
| sample-vnet/private-endpoints (10.0.1.0/24) | V |


<!---- Page 491 ---------------------------------------------------------------------------------------------------------------------------------->
Create a private endpoint
1
Basics
2
Resource
3
Virtual Network
4 Tags
5
Review + create
Use private endpoints to privately connect to a service or resource. Your private endpoint must be in the same region as your
virtual network, but can be in a different region from the private link resource that you are connecting to. Learn more
Project details
Subscription *
0
Resource group * @
Create new
Instance details
Name *
my-stg-endpoint
Region *
Southeast Asia
V
Visual Studio Enterprise Subscription
V
work-related
V
6. On the resource tab, select blob from the Target sub-resource list.
7. select Next: Virtual Network.
Create a private endpoint
Basics
2
Resource
3
Virtual Network
4
Tags
5
Review + create
Private Link offers options to create private endpoints for different Azure resources, like your private link service, a SQL server, or
an Azure storage account. Select which resource you would like to connect to using this private endpoint. Learn more
Subscription
Visual Studio Enterprise Subscription (083694cc-cace-414f-9d55-d6b9071db956)
Resource type
Microsoft.Storage/storageAccounts
Resource
vikurpadwork
Target sub-resource * @
blob
V
blob
table
queue
file
web
dfs
8. Select the Virtual network and Subnet. Make sure Enable network policies for all
private endpoints in this subnet is selected and the Dynamically allocate IP
address is enabled.
9. Select Next: DNS.
10. Make sure that Yes is enabled for Integrate with private DNS zone.

| Visual Studio Enterprise Subscription | V |
| --- | --- |
|  |  |
| work-related | V |

| Subscription | Visual Studio Enterprise Subscription (083694cc-cace-414f-9d55-d6b9071db956) |
| --- | --- |
| Resource type | Microsoft.Storage/storageAccounts |
| Resource | vikurpadwork |
| Target sub-resource * @ :unselected: | blob V |
|  | blob |
|  | table |
|  | queue |
|  | file |
|  | web |
|  | dfs |


<!---- Page 492 ---------------------------------------------------------------------------------------------------------------------------------->
11. Select Next: Tags.
12. Select Next: Review + create.
Great work! You now have all the connections between the Document Intelligence
resource and storage configured to use managed identities.
1 Note
The resources are only accessible from the virtual network and allowed IPs.
Studio access and analyze requests to your Document Intelligence resource will fail
unless the request originates from the virtual network or is routed via the virtual
network.
Validate your deployment
To validate your deployment, you can deploy a virtual machine (VM) to the virtual
network and connect to the resources.
1. Configure a Data Science VM in the virtual network.
2. Remotely connect into the VM from your desktop and launch a browser session
that accesses Document Intelligence Studio.
3. Analyze requests and the training operations should now work successfully.
That's it! You can now configure secure access for your Document Intelligence resource
with managed identities and private endpoints.
Common error messages
· Failed to access Blob container:


<!---- Page 493 ---------------------------------------------------------------------------------------------------------------------------------->
Rec
df
Failed to access Blob container
X
Check existence and CORS configuration of selected Blob container.
To enable cross-domain resource sharing on Azure storage account for data
accessing, follow the configuration instruction.
and):
df
ount
Close
%
Resolution:
1. Configure CORS.
2. Make sure the client computer can access Document Intelligence resource
and storage account, either they are in the same VNET, or client IP address is
allowed in Networking > Firewalls and virtual networks setting page of both
Document Intelligence resource and storage account.
· AuthorizationFailure:
AuthorizationFailure
X
This request is not authorized to perform this operation. RequestId:ab58e1c2-
c01e-0049-6adc-497151000000 Time:2022-04-06T17:37:04.4160162Z
Close
Resolution: Make sure the client computer can access Document Intelligence
resource and storage account, either they are in the same VNET, or client IP
address is allowed in Networking > Firewalls and virtual networks setting page of
both Document Intelligence resource and storage account.
· ContentSourceNotAccessible:


<!---- Page 494 ---------------------------------------------------------------------------------------------------------------------------------->
Sold To lobikom Residences
ContentSourceNotAccessible
X
Content is not accessible: Cannot access container URL.
apim-request-id: 60994b3b-40f6-4866-b560-1b0ad4a9f821
Close
Resolution: Make sure you grant your Document Intelligence managed identity the
role of Storage Blob Data Contributor and enabled Trusted services access or
Resource instance rules on the networking tab.
· AccessDenied:
Sold To fabrikcom Residences
AccessDenied
X
Access denied due to Virtual Network/Firewall rules.
apim-request-id: ee0145a1-86b9-44e7-ae25-2ccc4213b085
and):
ount
Close
%
Resolution: Make sure the client computer can access Document Intelligence
resource and storage account, either they are in the same VNET, or client IP
address is allowed in Networking > Firewalls and virtual networks setting page of
both Document Intelligence resource and storage account.
Next steps
Access Azure Storage from a web app using managed identities
Feedback
Was this page helpful?
Yes
P
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 495 ---------------------------------------------------------------------------------------------------------------------------------->
Encrypt data at rest
Article · 03/19/2025
This content applies to:
v4.0 (GA)
v3.1 (GA)
v3.0 (GA)
v2.1 (GA)
1 Important
· Earlier versions of customer managed keys ( CMK ) only encrypted your models.
· Beginning with the 07/31/2023 release, all new resources utilize customer-
managed keys to encrypt both models and document results.
· Delete analyze response. the analyze response is stored for 24 hours from
when the operation completes for retrieval. For scenarios where you want to
delete the response sooner, use the delete analyze response API to delete the
response.
· To upgrade an existing service to encrypt both the models and the data,
disable and reenable the customer managed key.
Azure AI Document Intelligence automatically encrypts your data when persisting it to
the cloud. Document Intelligence encryption protects your data to help you to meet
your organizational security and compliance commitments.
About Azure AI services encryption
Data is encrypted and decrypted using FIPS 140-2-compliant 256-bit AES
encryption. Encryption and decryption are transparent, meaning encryption and access
are managed for you. Your data is secure by default. You don't need to modify your
code or applications to take advantage of encryption.
About encryption key management
By default, your subscription uses Microsoft-managed encryption keys. You can also
manage your subscription with your own keys, which are called customer-managed
keys. When you use customer-managed keys, you have greater flexibility in the way you
create, rotate, disable, and revoke access controls. You can also audit the encryption
keys that you use to protect your data. If customer-managed keys are configured for
your subscription, double encryption is provided. With this second layer of protection,
you can control the encryption key through your Azure Key Vault.


<!---- Page 496 ---------------------------------------------------------------------------------------------------------------------------------->
1 Important
. Customer-managed keys are only available resources created after May 11,
2020. To use customer-managed keys with Document Intelligence, you need
to create a new Document Intelligence resource. Once the resource is created,
you can use Azure Key Vault to set up your managed identity.
· The scope for data encrypted with customer-managed keys includes the
analysis response stored for 24 hours, allowing the operation results to be
retrieved during that 24-hour time period.
Customer-managed keys with Azure Key Vault
When you use customer-managed keys, you must use Azure Key Vault to store them.
You can either create your own keys and store them in a key vault, or you can use the
Key Vault APIs to generate keys. The Azure AI services resource and the key vault must
be in the same region and in the same Microsoft Entra tenant, but they can be in
different subscriptions. For more information about Key Vault, see What is Azure Key
Vault ?.
When you create a new Azure AI services resource, it's always encrypted by using
Microsoft-managed keys. It's not possible to enable customer-managed keys when you
create the resource. Customer-managed keys are stored in Key Vault. The key vault
needs to be provisioned with access policies that grant key permissions to the managed
identity that's associated with the Azure AI services resource. The managed identity is
available only after the resource is created by using the pricing tier that's required for
customer-managed keys.
Enabling customer-managed keys also enables a system-assigned managed identity, a
feature of Microsoft Entra ID. After the system-assigned managed identity is enabled,
this resource is registered with Microsoft Entra ID. After being registered, the managed
identity is given access to the key vault that's selected during customer-managed key
setup.
1 Important
If you disable system-assigned managed identities, access to the key vault is
removed and any data that's encrypted with the customer keys is no longer
accessible. Any features that depend on this data stop working.


<!---- Page 497 ---------------------------------------------------------------------------------------------------------------------------------->
1 Important
Managed identities don't currently support cross-directory scenarios. When you
configure customer-managed keys in the Azure portal, a managed identity is
automatically assigned behind the scenes. If you subsequently move the
subscription, resource group, or resource from one Microsoft Entra directory to
another, the managed identity that's associated with the resource isn't transferred
to the new tenant, so customer-managed keys might no longer work. For more
information, see Transferring a subscription between Microsoft Entra directories
in FAQs and known issues with managed identities for Azure resources.
Configure Key Vault
When you use customer-managed keys, you need to set two properties in the key vault,
Soft Delete and Do Not Purge. These properties aren't enabled by default, but you can
enable them on a new or existing key vault by using the Azure portal, PowerShell, or
Azure CLI.
1 Important
If the Soft Delete and Do Not Purge properties aren't enabled and you delete your
key, you can't recover the data in your Azure AI services resource.
To learn how to enable these properties on an existing key vault, see Azure Key Vault
recovery management with soft delete and purge protection.
Enable customer-managed keys for your
resource
To enable customer-managed keys in the Azure portal, follow these steps:
1. Go to your Azure AI services resource.
2. On the left, select Encryption.
3. Under Encryption type, select Customer Managed Keys, as shown in the following
screenshot.


<!---- Page 498 ---------------------------------------------------------------------------------------------------------------------------------->
CMK-Test - Encryption
Cognitive Services
X
Search (Ctrl+/)
«
Encryption
Overview
A
n
Save
Discard
Activity log
Cognitive services encryption protects your data at rest. Azure Cognitive services encrypts your data as it's written in
our datacenters, and automatically decrypts it for you as you access it.
Access control (IAM)
Tags
By default, data in the cognitive service account is encrypted using Microsoft Managed Keys. You may choose to
bring your own key.
Diagnose and solve problems
Please note that after enabling Cognitive Service Encryption, only new data will be encrypted, and any existing files
in this cognitive service account will retroactively get encrypted by a background encryption process.
RESOURCE MANAGEMENT
Quick start
Learn More about Azure Cognitive services Encryption "
Keys and Endpoint
Encryption
Encryption type
Microsoft Managed Keys
Pricing tier
Customer Managed Keys
i The cognitive service account named 'CMK-Test' will be granted
access to the selected key vault. Both soft delete and purge
protection will be enabled on the key vault and cannot be disabled.
Learn more about customer managed keys "
Virtual network
Identity
Billing By Subscription
Encryption key
Enter key URI
Properties
Select from Key Vault
Locks
Key URI *
Export template
Subscription
AICP-DEV
Specify a key
After you enable customer-managed keys, you can specify a key to associate with the
Azure AI services resource.
Specify a key as a URI
To specify a key as a URI, follow these steps:
1. In the Azure portal, go to your key vault.
2. Under Settings, select Keys.
3. Select the desired key, and then select the key to view its versions. Select a key
version to view the settings for that version.
4. Copy the Key Identifier value, which provides the URI.


<!---- Page 499 ---------------------------------------------------------------------------------------------------------------------------------->
Dashboard > Key vaults > storagesamplekvcli - Keys > customstoragekey > A1bC2dE3fH4iJ5KL6mN7oP8qR9STOuCu
A1bC2dE3fH4iJ5kL6mN7oP8qR9sTOuOu
Key Version
8
Save
Discard
Properties
Key Type
RSA
RSA Key Size
2048
Created
4/9/2019, 12:50:38 PM
Updated
4/9/2019, 12:50:38 PM
Key Identifier
<key-uri>
D
Settings
Set activation date? 6
Set expiration date? 6
Yes
Enabled?
No
Tags
0 tags
>
Permitted operations
Encrypt
Sign
Wrap Key
Decrypt
Verify
Unwrap Key
5. Go back to your Azure AI services resource, and then select Encryption.
6. Under Encryption key, select Enter key URI.
7. Paste the URI that you copied into the Key URI box.
CMK-Test - Encryption
Cognitive Services
X
Search (Ctrl+/)
«
Encryption
®
Overview
4
m
Save
Discard
Activity log
Cognitive services encryption protects your data at rest. Azure Cognitive services encrypts your data as it's written in
our datacenters, and automatically decrypts it for you as you access it.
Access control (IAM)
Tags
By default, data in the cognitive service account is encrypted using Microsoft Managed Keys. You may choose to
bring your own key.
Diagnose and solve problems
Please note that after enabling Cognitive Service Encryption, only new data will be encrypted, and any existing files
in this cognitive service account will retroactively get encrypted by a background encryption process.
RESOURCE MANAGEMENT
Quick start
Learn More about Azure Cognitive services Encryption "
Keys and Endpoint
Encryption
Encryption type
Microsoft Managed Keys
Pricing tier
Customer Managed Keys
The cognitive service account named 'CMK-Test' will be granted
access to the selected key vault. Both soft delete and purge
protection will be enabled on the key vault and cannot be disabled.
Learn more about customer managed keys
Virtual network
Identity
Billing By Subscription
Encryption key
Enter key URI
Select from Key Vault
Properties
Locks
Key URI *
<key uri>|
V
Export template
Subscription
AICP-DEV
V
8. Under Subscription, select the subscription that contains the key vault.
9. Save your changes.


<!---- Page 500 ---------------------------------------------------------------------------------------------------------------------------------->
Specify a key from a key vault
To specify a key from a key vault, first make sure that you have a key vault that contains
a key. Then follow these steps:
1. Go to your Azure AI services resource, and then select Encryption.
2. Under Encryption key, select Select from Key Vault.
3. Select the key vault that contains the key that you want to use.
4. Select the key that you want to use.
Microsoft Azure
Search resources, services, and docs (G+/)
Home > CMKTest01-SB - Encryption > Select key from Azure Key Vault
Select key from Azure Key Vault
Subscription *
AICP-DEV
Key vault *
CMKTest-01SB
Create new
Key *
CMKTest-01SB
Create new
Version * 1
19fc5cfacbd34e47b373709c1e400902
Create new
5. Save your changes.
Update the key version
When you create a new version of a key, update the Azure AI services resource to use
the new version. Follow these steps:
1. Go to your Azure AI services resource, and then select Encryption.
2. Enter the URI for the new key version. Alternately, you can select the key vault and
then select the key again to update the version.
3. Save your changes.
Use a different key
To change the key that you use for encryption, follow these steps:
1. Go to your Azure AI services resource, and then select Encryption.

| Microsoft Azure Search resources, services, and docs (G+/) |  |  |
| --- | --- | --- |
| Home > CMKTest01-SB - Encryption > Select key from Azure Key Vault |  |  |
| Select key from Azure Key Vault |  |  |
| Subscription * |  |  |
|  | AICP-DEV |  |
|  |  |  |
| Key vault * | CMKTest-01SB |  |
|  | Create new |  |
| Key * | CMKTest-01SB |  |
|  | Create new |  |
| Version * 1 | 19fc5cfacbd34e47b373709c1e400902 |  |
|  | Create new |  |


<!---- Page 501 ---------------------------------------------------------------------------------------------------------------------------------->
2. Enter the URI for the new key. Alternately, you can select the key vault and then
select a new key.
3. Save your changes.
Rotate customer-managed keys
You can rotate a customer-managed key in Key Vault according to your compliance
policies. When the key is rotated, you must update the Azure AI services resource to use
the new key URI. To learn how to update the resource to use a new version of the key in
the Azure portal, see Update the key version.
Rotating the key doesn't trigger re-encryption of data in the resource. No further action
is required from the user.
Revoke access to customer-managed keys
To revoke access to customer-managed keys, use PowerShell or Azure CLI. For more
information, see Azure Key Vault PowerShell « or Azure Key Vault CLI. Revoking access
effectively blocks access to all data in the Azure AI services resource, because the
encryption key is inaccessible by Azure AI services.
Disable customer-managed keys
When you disable customer-managed keys, your Azure AI services resource is then
encrypted with Microsoft-managed keys. To disable customer-managed keys, follow
these steps:
1. Go to your Azure AI services resource, and then select Encryption.
2. Clear the checkbox that's next to Use your own key.
Next steps
. Learn more about Azure Key Vault
Feedback
Was this page helpful?
Yes
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 502 ---------------------------------------------------------------------------------------------------------------------------------->
Authenticate requests to Azure AI
services
Article · 02/07/2025
Each request to an Azure AI service must include an authentication header. This header
passes along a resource key or authentication token, which is used to validate your
subscription for a service or group of services. In this article, you'll learn about three
ways to authenticate a request and the requirements for each.
· Authenticate with a single-service or multi-service resource key.
· Authenticate with a token.
· Authenticate with Microsoft Entra ID.
Prerequisites
Before you make a request, you need an Azure account and an Azure AI services
subscription. If you already have an account, go ahead and skip to the next section. If
you don't have an account, we have a guide to get you set up in minutes: Create an
Azure AI services resource.
Go to your resource in the Azure portal. The Keys & Endpoint section can be found in
the Resource Management section. Copy your endpoint and access key as you'll need
both for authenticating your API calls. You can use either KEY1 or KEY2. Always having
two keys allows you to securely rotate and regenerate keys without causing a service
disruption. The length of the key can vary depending on the API version used to create
or regenerate the key.
Authentication headers
Let's quickly review the authentication headers available for use with Azure AI services.
@ Expand table
Header
Description
Ocp-Apim-
Subscription-Key
Use this header to authenticate with a resource key for a specific service or
a multi-service resource key.
Ocp-Apim-
This header is only required when using a multi-service resource key with
the Azure AI Translator service. Use this header to specify the resource
Subscription-

| Header | Description |
| --- | --- |
| Ocp-Apim- Subscription-Key | Use this header to authenticate with a resource key for a specific service or a multi-service resource key. |
| Ocp-Apim- | This header is only required when using a multi-service resource key with the Azure AI Translator service. Use this header to specify the resource |
| Subscription- |  |


<!---- Page 503 ---------------------------------------------------------------------------------------------------------------------------------->
Header
Description
Region
region.
Authorization
Use this header if you are using an access token. The steps to perform a
token exchange are detailed in the following sections. The value provided
follows this format: Bearer <TOKEN>.
Authenticate with a single-service resource key
The first option is to authenticate a request with a resource key for a specific service, like
Azure AI Translator. The keys are available in the Azure portal for each resource that
you've created. Go to your resource in the Azure portal. The Keys & Endpoint section
can be found in the Resource Management section. Copy your endpoint and access key
as you'll need both for authenticating your API calls. You can use either KEY1 or KEY2.
Always having two keys allows you to securely rotate and regenerate keys without
causing a service disruption.
To use a resource key to authenticate a request, it must be passed along as the Ocp-
Apim-Subscription-Key header. This is a sample call to the Azure AI Translator service:
This is a sample call to the Translator service:
CURL
curl -X POST 'https://api.cognitive.microsofttranslator.com/translate?api-
version=3.0&from=en&to=de' \
-H 'Ocp-Apim-Subscription-Key: YOUR_SUBSCRIPTION_KEY' |
-H 'Content-Type: application/json' |
-- data-raw '[{ "text": "How much for the cup of coffee?" } ]' | json_pp
Authenticate with a multi-service resource key
You can use a multi-service resource key to authenticate requests. The main difference is
that the multi-service resource key isn't tied to a specific service, rather, a single key can
be used to authenticate requests for multiple Azure AI services. See Azure AI services
pricing for information about regional availability, supported features, and pricing.
The resource key is provided in each request as the Ocp-Apim-Subscription-Key header.
Supported regions

| Header | Description |
| --- | --- |
| Region | region. |
| Authorization | Use this header if you are using an access token. The steps to perform a token exchange are detailed in the following sections. The value provided follows this format: Bearer <TOKEN>. |


<!---- Page 504 ---------------------------------------------------------------------------------------------------------------------------------->
When using the Azure AI services multi-service resource key to make a request to
api.cognitive.microsoft.com, you must include the region in the URL. For example:
westus.api.cognitive.microsoft.com.
When using a multi-service resource key with Azure AI Translator, you must specify the
resource region with the Ocp-Apim-Subscription-Region header.
Multi-service resource authentication is supported in these regions:
· australiaeast
· brazilsouth
· canadacentral
· centralindia
· eastasia
· eastus
· japaneast
· northeurope
. southcentralus
· southeastasia
· uksouth
. westcentralus
· westeurope
. westus
. westus2
. francecentral
· koreacentral
. northcentralus
. southafricanorth
· uaenorth
· switzerlandnorth
Sample requests
This is a sample call to the Azure AI Translator service:
CURL
curl -X POST 'https://api. cognitive.microsofttranslator. com/translate?api -
version=3.0&from=en&to=de' \
-H 'Ocp-Apim-Subscription-Key: YOUR_SUBSCRIPTION_KEY' |
-H 'Ocp-Apim-Subscription-Region: YOUR_SUBSCRIPTION_REGION' |


<!---- Page 505 ---------------------------------------------------------------------------------------------------------------------------------->
-H 'Content-Type: application/json' |
-- data-raw '[{ "text": "How much for the cup of coffee?" } ]' | json_pp
Authenticate with an access token
Some Azure AI services accept, and in some cases require, an access token. Currently,
these services support access tokens:
. Text Translation API
· Speech Services: Speech to text API
· Speech Services: Text to speech API
A
Warning
The services that support access tokens may change over time, please check the
API reference for a service before using this authentication method.
Both single service and multi-service resource keys can be exchanged for authentication
tokens. Authentication tokens are valid for 10 minutes. They're stored in JSON Web
Token (JWT) format and can be queried programmatically using the JWT libraries.
Access tokens are included in a request as the Authorization header. The token value
provided must be preceded by Bearer, for example: Bearer YOUR_AUTH_TOKEN .
Sample requests
Use this URL to exchange a resource key for an access token: https: //YOUR-
REGION.api.cognitive.microsoft.com/sts/v1.0/issueToken.
CURL
curl -v -X POST |
"https://YOUR-REGION.api.cognitive.microsoft. com/sts/v1.0/issueToken" |
-H "Content-type: application/x-www-form-urlencoded" |
-H "Content-length: 0" \
-H "Ocp-Apim-Subscription-Key: YOUR_SUBSCRIPTION_KEY"
These multi-service regions support token exchange:
. australiaeast
· brazilsouth
. canadacentral


<!---- Page 506 ---------------------------------------------------------------------------------------------------------------------------------->
· centralindia
· eastasia
· eastus
· japaneast
· northeurope
· southcentralus
· southeastasia
· uksouth
· westcentralus
· westeurope
· westus
. westus2
After you get an access token, you'll need to pass it in each request as the
Authorization header. This is a sample call to the Azure AI Translator service:
CURL
curl -X POST 'https://api.cognitive.microsofttranslator.com/translate?api-
version=3.0&from=en&to=de' \
-H 'Authorization: Bearer YOUR_AUTH_TOKEN' \
-H 'Content-Type: application/json' \
-- data-raw '[{ "text": "How much for the cup of coffee?" } ]' | json_pp
Authenticate with Microsoft Entra ID
1 Important
Microsoft Entra authentication always needs to be used together with custom
subdomain name of your Azure resource. Regional endpoints do not support
Microsoft Entra authentication.
In the previous sections, we showed you how to authenticate against Azure AI services
using a single-service or multi-service subscription key. While these keys provide a quick
and easy path to start development, they fall short in more complex scenarios that
require Azure role-based access control (Azure RBAC). Let's take a look at what's
required to authenticate using Microsoft Entra ID.
In the following sections, you'll use either the Azure Cloud Shell environment or the
Azure CLI to create a subdomain, assign roles, and obtain a bearer token to call the


<!---- Page 507 ---------------------------------------------------------------------------------------------------------------------------------->
Azure AI services. If you get stuck, links are provided in each section with all available
options for each command in Azure Cloud Shell/Azure CLI.
1 Important
If your organization is doing authentication through Microsoft Entra ID, you should
disable local authentication (authentication with keys) so that users in the
organization must always use Microsoft Entra ID.
Create a resource with a custom subdomain
The first step is to create a custom subdomain. If you want to use an existing Azure AI
services resource which does not have custom subdomain name, follow the instructions
in Azure AI services custom subdomains to enable custom subdomain for your resource.
1. Start by opening the Azure Cloud Shell. Then select a subscription:
PowerShell
Set-AzContext -SubscriptionName <SubscriptionName>
2. Next, create an Azure AI services resource with a custom subdomain. The
subdomain name needs to be globally unique and cannot include special
characters, such as: ".", "!", ",".
PowerShell
$account = New-AzCognitiveServicesAccount -ResourceGroupName
<RESOURCE_GROUP_NAME> -name <ACCOUNT_NAME> -Type <ACCOUNT_TYPE> -
SkuName <SUBSCRIPTION_TYPE> -Location <REGION> -CustomSubdomainName
<UNIQUE_SUBDOMAIN>
3. If successful, the Endpoint should show the subdomain name unique to your
resource.
Assign a role to a service principal
Now that you have a custom subdomain associated with your resource, you're going to
need to assign a role to a service principal.
1 Note


<!---- Page 508 ---------------------------------------------------------------------------------------------------------------------------------->
Keep in mind that Azure role assignments may take up to five minutes to
propagate.
1. First, let's register an Microsoft Entra application.
PowerShell
$SecureStringPassword = ConvertTo-SecureString -String <YOUR_PASSWORD>
-AsPlainText -Force
$app = New-AzureADApplication -DisplayName <APP_DISPLAY_NAME> -
IdentifierUris <APP_URIS> -PasswordCredentials $SecureStringPassword
You're going to need the ApplicationId in the next step.
2. Next, you need to create a service principal for the Microsoft Entra application.
PowerShell
New-AzADServicePrincipal -ApplicationId <APPLICATION_ID>
1 Note
If you register an application in the Azure portal, this step is completed for
you.
3. The last step is to assign the "Cognitive Services User" role to the service principal
(scoped to the resource). By assigning a role, you're granting service principal
access to this resource. You can grant the same service principal access to multiple
resources in your subscription.
1 Note
The ObjectId of the service principal is used, not the ObjectId for the
application. The ACCOUNT_ID will be the Azure resource Id of the Azure AI
services account you created. You can find Azure resource Id from
"properties" of the resource in Azure portal.
Azure CLI
New-AzRoleAssignment -ObjectId <SERVICE_PRINCIPAL_OBJECTID> -Scope
<ACCOUNT_ID> -RoleDefinitionName "Cognitive Services User"


<!---- Page 509 ---------------------------------------------------------------------------------------------------------------------------------->
Sample request
In this sample, a password is used to authenticate the service principal. The token
provided is then used to call the Computer Vision API.
1. Get your TenantId:
PowerShell
$context=Get-AzContext
$context. Tenant. Id
2. Get a token:
PowerShell
$tenantId = $context. Tenant. Id
$clientId = $app. ApplicationId
$clientSecret = "<YOUR_PASSWORD>"
$resourceUrl = "https://cognitiveservices.azure.com/"
$tokenEndpoint =
"https://login.microsoftonline.com/$tenantId/oauth2/token"
$body = @{
grant_type
= "client_credentials"
client_id
= $clientId
client_secret
= $clientSecret
resource
= $resourceUrl
}
$responseToken = Invoke-RestMethod -Uri $tokenEndpoint -Method Post -
Body $body
$accessToken = $responseToken. access_token
1 Note
Anytime you use passwords in a script, the most secure option is to use the
PowerShell Secrets Management module and integrate with a solution such as
Azure Key Vault.
3. Call the Computer Vision API:
PowerShell
$url = $account. Endpoint+"vision/v1.0/models"
$result = Invoke-RestMethod -Uri $url -Method Get -Headers


<!---- Page 510 ---------------------------------------------------------------------------------------------------------------------------------->
@{"Authorization"="Bearer $accessToken"} -Verbose
$result | ConvertTo-Json
Alternatively, the service principal can be authenticated with a certificate. Besides service
principal, user principal is also supported by having permissions delegated through
another Microsoft Entra application. In this case, instead of passwords or certificates,
users would be prompted for two-factor authentication when acquiring token.
Authorize access to managed identities
Azure AI services support Microsoft Entra authentication with managed identities for
Azure resources. Managed identities for Azure resources can authorize access to Azure
AI services resources using Microsoft Entra credentials from applications running in
Azure virtual machines (VMs), function apps, virtual machine scale sets, and other
services. By using managed identities for Azure resources together with Microsoft Entra
authentication, you can avoid storing credentials with your applications that run in the
cloud.
Enable managed identities on a VM
Before you can use managed identities for Azure resources to authorize access to Azure
AI services resources from your VM, you must enable managed identities for Azure
resources on the VM. To learn how to enable managed identities for Azure Resources,
see:
· Azure portal
· Azure PowerShell
· Azure CLI
· Azure Resource Manager template
· Azure Resource Manager client libraries
For more information about managed identities, see Managed identities for Azure
resources.
Use Azure key vault to securely access
credentials
You can use Azure Key Vault to securely develop Azure AI services applications. Key Vault
enables you to store your authentication credentials in the cloud, and reduces the


<!---- Page 511 ---------------------------------------------------------------------------------------------------------------------------------->
chances that secrets may be accidentally leaked, because you won't store security
information in your application.
Authentication is done via Microsoft Entra ID. Authorization may be done via Azure role-
based access control (Azure RBAC) or Key Vault access policy. Azure RBAC can be used
for both management of the vaults and access data stored in a vault, while key vault
access policy can only be used when attempting to access data stored in a vault.
Related content
· What are Azure Al services?
· Azure Al services pricing
· Custom subdomains
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A


<!---- Page 512 ---------------------------------------------------------------------------------------------------------------------------------->
Install and run containers
Article · 04/04/2025
This content applies to:
v3.0 (GA)
v3.1 (GA)
v4.0 (GA)
Azure AI Document Intelligence is an Azure AI service that lets you build automated data processing
software using machine-learning technology. Document Intelligence enables you to identify and extract
text, key/value pairs, selection marks, table data, and more from your documents. The results are delivered
as structured data that .. /includes the relationships in the original file. Containers process only the data
provided to them and solely utilize the resources they're permitted to access. Containers can't process
data from other regions.
In this article you can learn how to download, install, and run Document Intelligence containers.
Containers enable you to run the Document Intelligence service in your own environment. Containers are
great for specific security and data governance requirements.
· Layout model is supported by Document Intelligence v4.0 containers.
. Read, Layout, ID Document, Receipt, and Invoice models are supported by Document Intelligence
v3.1 containers.
. Read, Layout, General Document, Business Card, and Custom models are supported by Document
Intelligence v3.0 containers.
Version support
Support for containers is currently available with Document Intelligence version v3.0: 2022-08-31 (GA) for
all models, v3.1 2023-07-31 (GA) for Read, Layout, ID Document, Receipt, and Invoice models, and v4.0
2024-11-30 (GA) for Layout:
· REST API v3.0: 2022-08-31 (GA)
· REST API v3.1: 2023-07-31 (GA)
· REST API v4.0: 2024-11-30 (GA)
· Client libraries targeting REST API v3.0: 2022-08-31 (GA)
· Client libraries targeting REST API v3.1: 2023-07-31 (GA)
· Client libraries targeting REST API v4.0: 2024-11-30 (GA)
Prerequisites
To get started, you need an active Azure account . If you don't have one, you can create a free
account.
You also need the following to use Document Intelligence containers:
" Expand table


<!---- Page 513 ---------------------------------------------------------------------------------------------------------------------------------->
Required
Purpose
Familiarity with
Docker
You should have a basic understanding of Docker concepts, like registries, repositories, containers,
and container images, as well as knowledge of basic docker terminology and commands.
Docker Engine
installed
. You need the Docker Engine installed on a host computer. Docker provides packages that
configure the Docker environment on macOS z, Windows 7, and Linux . For a primer on
Docker and container basics, see the Docker overview 2.
. Docker must be configured to allow the containers to connect with and send billing data to
Azure.
· On Windows, Docker must also be configured to support Linux containers.
Document
Intelligence
resource
A single-service Azure Al Document Intelligence [ or multi-service [ resource in the Azure
portal. To use the containers, you must have the associated key and endpoint URI. Both values are
available on the Azure portal Document Intelligence Keys and Endpoint page:
· {FORM_RECOGNIZER_KEY}: one of the two available resource keys.
· {FORM_RECOGNIZER_ENDPOINT_URI}: the endpoint for the resource used to track billing
information.
[] Expand table
Optional
Purpose
Azure CLI
(command-line
interface)
The Azure CLI enables you to use a set of online commands to create and manage Azure
resources. It's available to install in Windows, macOS, and Linux environments and can be run
in a Docker container and Azure Cloud Shell.
Host computer requirements
The host is a x64-based computer that runs the Docker container. It can be a computer on your premises
or a Docker hosting service in Azure, such as:
· Azure Kubernetes Service.
· Azure Container Instances.
· A Kubernetes cluster deployed to Azure Stack. For more information, see Deploy Kubernetes to
Azure Stack.
1 Note
The Studio container can't be deployed and run in Azure Kubernetes Service. The Studio container is
only supported to run on local machines.
Container requirements and recommendations
Required supporting containers

| Required | Purpose |
| --- | --- |
| Familiarity with Docker | You should have a basic understanding of Docker concepts, like registries, repositories, containers, and container images, as well as knowledge of basic docker terminology and commands. |
| Docker Engine installed | . You need the Docker Engine installed on a host computer. Docker provides packages that configure the Docker environment on macOS z, Windows 7, and Linux . For a primer on Docker and container basics, see the Docker overview 2. . Docker must be configured to allow the containers to connect with and send billing data to Azure. · On Windows, Docker must also be configured to support Linux containers. |
| Document Intelligence resource | A single-service Azure Al Document Intelligence [ or multi-service [ resource in the Azure portal. To use the containers, you must have the associated key and endpoint URI. Both values are available on the Azure portal Document Intelligence Keys and Endpoint page: · {FORM_RECOGNIZER_KEY}: one of the two available resource keys. · {FORM_RECOGNIZER_ENDPOINT_URI}: the endpoint for the resource used to track billing information. |

| Optional | Purpose |
| --- | --- |
| Azure CLI (command-line interface) | The Azure CLI enables you to use a set of online commands to create and manage Azure resources. It's available to install in Windows, macOS, and Linux environments and can be run in a Docker container and Azure Cloud Shell. |


<!---- Page 514 ---------------------------------------------------------------------------------------------------------------------------------->
The following table lists one or more supporting containers for each Document Intelligence container you
download. For more information, see the Billing section.
@ Expand table
Feature container
Supporting containers
Read
Not required
Layout
Not required
Business Card
Read
General Document
Layout
Invoice
Layout
Receipt
Read or Layout
ID Document
Read
Custom Template
Layout
Recommended CPU cores and memory
1 Note
The minimum and recommended values are based on Docker limits and not the host machine
resources.
Document Intelligence containers
" Expand table
Container
Minimum
Recommended
Read
8 cores, 10-GB memory
8 cores, 24-GB memory
Layout
8 cores, 16-GB memory
8
cores, 24-GB memory
Business Card
8
cores, 16-GB memory
8
cores, 24-GB memory
General Document
8
cores, 12-GB memory
8
cores, 24-GB memory
ID Document
8 cores, 8-GB memory
8
cores, 24-GB memory
Invoice
8 cores, 16-GB memory
8
cores, 24-GB memory
Receipt
8 cores, 11-GB memory
8
cores, 24-GB memory
Custom Template
8 cores, 16-GB memory
8 cores, 24-GB memory

| Feature container | Supporting containers |
| --- | --- |
| Read | Not required |
| Layout | Not required |
| Business Card | Read |
| General Document | Layout |
| Invoice | Layout |
| Receipt | Read or Layout |
| ID Document | Read |
| Custom Template | Layout |

| Container | Minimum | Recommended |
| --- | --- | --- |
| Read | 8 cores, 10-GB memory | 8 cores, 24-GB memory |
| Layout | 8 cores, 16-GB memory | 8 cores, 24-GB memory |
| Business Card | 8 cores, 16-GB memory | 8 cores, 24-GB memory |
| General Document | 8 cores, 12-GB memory | 8 cores, 24-GB memory |
| ID Document | 8 cores, 8-GB memory | 8 cores, 24-GB memory |
| Invoice | 8 cores, 16-GB memory | 8 cores, 24-GB memory |
| Receipt | 8 cores, 11-GB memory | 8 cores, 24-GB memory |
| Custom Template | 8 cores, 16-GB memory | 8 cores, 24-GB memory |


<!---- Page 515 ---------------------------------------------------------------------------------------------------------------------------------->
· Each core must be at least 2.6 gigahertz (GHz) or faster.
· Core and memory correspond to the -- cpus and -- memory settings, which are used as part of the
docker compose or docker run command.
Tip
You can use the docker images " command to list your downloaded container images. For example,
the following command lists the ID, repository, and tag of each downloaded container image,
formatted as a table:
docker
docker images -format "table {. ID}\t{. Repository}\t{. Tag}"
IMAGE ID
REPOSITORY
TAG
<image-id>
<repository-path/name>
<tag-name>
Run the container with the docker-compose up
command
. Replace the {ENDPOINT_URI} and {API_KEY} values with your resource Endpoint URI and the key from
the Azure resource page.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
P Search
«
Regenerate Key1
Regenerate Key2
Overview
Activity log
0
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Access control (IAM)
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
................................
D
& Encryption
KEY 2
Pricing tier
................................
P
Networking
Location/Region
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
> Automation
18
Help
. Ensure that the EULA value is set to accept.
. The EULA, Billing, and Apikey values must be specified; otherwise the container can't start.


<!---- Page 516 ---------------------------------------------------------------------------------------------------------------------------------->
1 Important
The keys are used to access your Document Intelligence resource. Don't share your keys. Store them
securely, for example, using Azure Key Vault. We also recommend regenerating these keys regularly.
Only one key is necessary to make an API call. When regenerating the first key, you can use the
second key for continued access to the service.
Layout
The following code sample is a self-contained docker compose example to run the Document
Intelligence Layout container. With docker compose, you use a YAML file to configure your
application's services. Then, with docker-compose up command, you create and start all the services
from your configuration. Enter {FORM_RECOGNIZER_ENDPOINT_URI} and {FORM_RECOGNIZER_KEY}
values for your Layout container instance.
yml
version: "3.9"
services :
azure-form-recognizer-layout :
container_name: azure-form-recognizer-layout
image: mcr.microsoft.com/azure-cognitive-services/form-recognizer/layout-4.0
environment:
- EULA=accept
- billing={FORM_RECOGNIZER_ENDPOINT_URI}
- apiKey={FORM_RECOGNIZER_KEY}
ports :
- "5000 : 5000"
networks:
- ocrvnet
networks:
ocrvnet:
driver: bridge
Now, you can start the service with the docker compose [ command:
Bash
docker-compose up
Create a docker compose file
1. Name this file docker-compose.yml
2. The following code sample is a self-contained docker compose example to run Document Intelligence
Layout, Studio, and Custom template containers together. With docker compose, you use a YAML file


<!---- Page 517 ---------------------------------------------------------------------------------------------------------------------------------->
to configure your application's services. Then, with docker-compose up command, you create and
start all the services from your configuration.
yml
version: '3.3'
services :
nginx:
image: nginx : alpine
container_name: reverseproxy
depends_on:
- layout
- custom-template
volumes :
- $ {NGINX_CONF_FILE} :/etc/nginx/nginx.conf
ports:
- "5000 : 5000"
layout :
container_name: azure-cognitive-service-layout
image: mcr .microsoft. com/azure-cognitive-services/form-recognizer/layout-3.1: latest
environment:
eula: accept
apikey: ${FORM_RECOGNIZER_KEY}
billing: ${FORM_RECOGNIZER_ENDPOINT_URI}
Logging: Console: LogLevel : Default: Information
SharedRootFolder: /share
Mounts : Shared: /share
Mounts : Output : /logs
volumes :
- type: bind
source: ${SHARED_MOUNT_PATH}
target: /share
- type: bind
source: ${OUTPUT_MOUNT_PATH}
target: /logs
expose:
- "5000"
custom-template:
container_name: azure-cognitive-service-custom-template
image: mcr.microsoft. com/azure-cognitive-services/form-recognizer/custom-template-
3.1: latest
restart: always
depends_on :
- layout
environment:
AzureCognitiveServiceLayoutHost: http://azure-cognitive-service-layout : 5000
eula: accept
apikey: ${FORM_RECOGNIZER_KEY}
billing: ${FORM_RECOGNIZER_ENDPOINT_URI}
Logging: Console: LogLevel : Default: Information
SharedRootFolder: /share
Mounts : Shared: /share
Mounts : Output: /logs
volumes :
- type: bind
source: ${SHARED_MOUNT_PATH}
target: /share
- type: bind
source: ${OUTPUT_MOUNT_PATH}


<!---- Page 518 ---------------------------------------------------------------------------------------------------------------------------------->
target: /logs
expose:
- "5000"
studio:
container_name: form-recognizer-studio
image: mcr.microsoft.com/azure-cognitive-services/form-recognizer/studio: 3.1
environment:
ONPREM_LOCALFILE_BASEPATH: /onprem_folder
STORAGE_DATABASE_CONNECTION_STRING: /onprem_db/Application.db
volumes :
- type: bind
source: ${FILE_MOUNT_PATH} # path to your local folder
target: /onprem_folder
- type: bind
source: ${DB_MOUNT_PATH} # path to your local folder
target: /onprem_db
ports:
- "5001:5001"
user: "1000:1000" # echo $(id -u):$(id -g)
The custom template container and Layout container can use Azure Storage queues or in memory queues.
The Storage: ObjectStore: AzureBlob: ConnectionString and queue : azure: connectionstring environment
variables only need to be set if you're using Azure Storage queues. When running locally, delete these
variables.
Ensure the service is running
To ensure that the service is up and running. Run these commands in an Ubuntu shell.
Bash
$cd <folder containing the docker-compose file>
$source .env
$docker-compose up
Custom template containers require a few different configurations and support other optional
configurations.
" Expand table
Setting
Required Description
EULA
Yes
License acceptance Example: Eula=accept
Billing
Yes
Billing endpoint URI of the FR resource
ApiKey
Yes
The endpoint key of the FR resource
Queue:Azure:ConnectionString
No
Azure Queue connection string

| Setting | Required | Description |
| --- | --- | --- |
| EULA | Yes | License acceptance Example: Eula=accept |
| Billing | Yes | Billing endpoint URI of the FR resource |
| ApiKey | Yes | The endpoint key of the FR resource |
| Queue:Azure:ConnectionString | No | Azure Queue connection string |


<!---- Page 519 ---------------------------------------------------------------------------------------------------------------------------------->
Setting
Required
Description
Storage:ObjectStore:AzureBlob:ConnectionString
No
Azure Blob connection string
HealthCheck:MemoryUpperboundInMB
No
Memory threshold for reporting unhealthy to liveness.
Default: Same as recommended memory
StorageTime ToLiveInMinutes
No
TTL duration to remove all intermediate and final files.
Default: Two days, TTL can set between five minutes to
seven days
Task:MaxRunningTimeSpanInMinutes
No
Maximum running time for treating request as time out.
Default: 60 minutes
HTTP_PROXY_BYPASS_URLS
No
Specify URLs for bypassing proxy Example:
HTTP_PROXY_BYPASS_URLS = abc.com, xyz.com
AzureCognitiveServiceReadHost (Receipt,
IdDocument Containers Only)
Yes
Specify Read container uri
Example:AzureCognitiveServiceReadHost=http://onprem-
frread:5000
AzureCognitiveServiceLayoutHost (Document,
Invoice Containers Only)
Yes
Specify Layout container uri
Example:AzureCognitiveServiceLayoutHost=http://onprem-
frlayout:5000
Use the Document Intelligence Studio to train a model
. Gather a set of at least five forms of the same type. You use this data to train the model and test a
form. You can use a sample data set (download and extract sample_data.zip).
. Once you can confirm that the containers are running, open a browser and navigate to the endpoint
where you have the containers deployed. If this deployment is your local machine, the endpoint is
[http://localhost:5001] (http://localhost:5001).
. Select the custom extraction model tile.
. Select the Create project option.
· Provide a project name and optionally a description
. On the "configure your resource" step, provide the endpoint to your custom template model. If you
deployed the containers on your local machine, use this URL [http: //localhost:5000]
(http://localhost:5000).
· Provide a subfolder for where your training data is located within the files folder.
. Finally, create the project
You should now have a project created, ready for labeling. Upload your training data and get started
labeling. If you're new to labeling, see build and train a custom model.
Using the API to train

| Setting | Required | Description |
| --- | --- | --- |
| Storage:ObjectStore:AzureBlob:ConnectionString | No | Azure Blob connection string |
| HealthCheck:MemoryUpperboundInMB :unselected: | No | Memory threshold for reporting unhealthy to liveness. Default: Same as recommended memory |
| StorageTime ToLiveInMinutes :unselected: | No | TTL duration to remove all intermediate and final files. Default: Two days, TTL can set between five minutes to seven days |
| Task:MaxRunningTimeSpanInMinutes :unselected: | No | Maximum running time for treating request as time out. Default: 60 minutes |
| HTTP_PROXY_BYPASS_URLS :unselected: | No | Specify URLs for bypassing proxy Example: HTTP_PROXY_BYPASS_URLS = abc.com, xyz.com |
| AzureCognitiveServiceReadHost (Receipt, :unselected: IdDocument Containers Only) | Yes | Specify Read container uri Example:AzureCognitiveServiceReadHost=http://onprem- frread:5000 |
| AzureCognitiveServiceLayoutHost (Document, :unselected: Invoice Containers Only) | Yes | Specify Layout container uri Example:AzureCognitiveServiceLayoutHost=http://onprem- frlayout:5000 |


<!---- Page 520 ---------------------------------------------------------------------------------------------------------------------------------->
If you plan to call the APIs directly to train a model, the custom template model train API requires a
base64 encoded zip file that is the contents of your labeling project. You can omit the PDF or image files
and submit only the JSON files.
Once you have your dataset labeled and *. ocr.json, *. labels.json and fields.json files added to a zip, use the
PowerShell commands to generate the base64 encoded string.
PowerShell
$bytes = [System. IO. File] : : ReadAllBytes("<your_zip_file>.zip")
$b64String = [System.Convert] : : ToBase64String($bytes,
[System. Base64FormattingOptions ] : : None)
Use the build model API to post the request.
HTTP
POST http://localhost:5000/formrecognizer/documentModels:build?api-version=2023-07-31
{
"modelId": "mymodel",
"description": "test model",
"buildMode": "template",
"base64Source": "<Your base64 encoded string>",
"tags": {
"additionalProp1": "string",
"additionalProp2": "string",
"additionalProp3": "string"
}
}
Validate that the service is running
There are several ways to validate that the container is running:
. The container provides a homepage at \ as a visual validation that the container is running.
. You can open your favorite web browser and navigate to the external IP address and exposed port of
the container in question. Use the listed request URLs to validate the container is running. The listed
example request URLs are http://localhost:5000, but your specific container can vary. Keep in mind
that you're navigating to your container's External IP address and exposed port.
[] Expand table
Request URL
Purpose
http://localhost:5000/
The container provides a home page.

| Request URL | Purpose |
| --- | --- |
| http://localhost:5000/ | The container provides a home page. |


<!---- Page 521 ---------------------------------------------------------------------------------------------------------------------------------->
Request URL
Purpose
http://localhost:5000/ready
Requested with GET, this request provides a verification that the container is
ready to accept a query against the model. This request can be used for
Kubernetes liveness and readiness probes.
http://localhost:5000/status
Requested with GET, this request verifies if the api-key used to start the
container is valid without causing an endpoint query. This request can be used
for Kubernetes liveness and readiness probes.
http://localhost:5000/swagger
The container provides a full set of documentation for the endpoints and a Try
it out feature. With this feature, you can enter your settings into a web-based
HTML form and make the query without having to write any code. After the
query returns, an example CURL command is provided to demonstrate the
required HTTP headers and body format.
localhost:5000
Q =
:
A Microsoft Azure
Your Azure Cognitive
Service Container is up
and running
Azure Cognitive Service Containers allow you to
harness the power of Al everywhere.
Service API Description
Learn more
>
Stop the containers
To stop the containers, use the following command:
Console
docker-compose down
Billing
The Document Intelligence containers send billing information to Azure by using a Document Intelligence
resource on your Azure account.

| Request URL | Purpose |
| --- | --- |
| http://localhost:5000/ready | Requested with GET, this request provides a verification that the container is ready to accept a query against the model. This request can be used for Kubernetes liveness and readiness probes. |
| http://localhost:5000/status | Requested with GET, this request verifies if the api-key used to start the container is valid without causing an endpoint query. This request can be used for Kubernetes liveness and readiness probes. |
| http://localhost:5000/swagger | The container provides a full set of documentation for the endpoints and a Try it out feature. With this feature, you can enter your settings into a web-based HTML form and make the query without having to write any code. After the query returns, an example CURL command is provided to demonstrate the required HTTP headers and body format. |
|  |  |


<!---- Page 522 ---------------------------------------------------------------------------------------------------------------------------------->
Queries to the container are billed at the pricing tier of the Azure resource used for the API Key . Billing is
calculated for each container instance used to process your documents and images.
If you receive the following error: Container isn't in a valid state. Subscription validation failed with status
'OutOfQuota' API key is out of quota. It's an indicator that your containers aren't communication wit the
billing endpoint.
Connect to Azure
The container needs the billing argument values to run. These values allow the container to connect to the
billing endpoint. The container reports usage about every 10 to 15 minutes. If the container doesn't
connect to Azure within the allowed time window, the container continues to run, but doesn't serve
queries until the billing endpoint is restored. The connection is attempted 10 times at the same time
interval of 10 to 15 minutes. If it can't connect to the billing endpoint within the 10 tries, the container
stops serving requests. See the Azure AI container FAQ for an example of the information sent to
Microsoft for billing.
Billing arguments
The docker-compose up command starts the container when all three of the following options are
provided with valid values:
0
Expand table
Option
Description
ApiKey
The key of the Azure AI services resource used to track billing information.
The value of this option must be set to a key for the provisioned resource specified in Billing .
Billing
The endpoint of the Azure AI services resource used to track billing information.
The value of this option must be set to the endpoint URI of a provisioned Azure resource.
Eula
Indicates that you accepted the license for the container.
The value of this option must be set to accept.
For more information about these options, see Configure containers.
Summary
That's it! In this article, you learned concepts and workflows for downloading, installing, and running
Document Intelligence containers. In summary:
· Document Intelligence provides seven Linux containers for Docker.
· Container images are downloaded from mcr.
· Container images run in Docker.
· The billing information must be specified when you instantiate a container.

| Option | Description |
| --- | --- |
| ApiKey | The key of the Azure AI services resource used to track billing information. |
|  | The value of this option must be set to a key for the provisioned resource specified in Billing . |
| Billing | The endpoint of the Azure AI services resource used to track billing information. The value of this option must be set to the endpoint URI of a provisioned Azure resource. |
| Eula | Indicates that you accepted the license for the container. The value of this option must be set to accept. |


<!---- Page 523 ---------------------------------------------------------------------------------------------------------------------------------->
1 Important
Azure AI containers aren't licensed to run without being connected to Azure for metering. Customers
need to enable the containers to always communicate billing information with the metering service.
Azure AI containers don't send customer data (for example, the image or text that is being analyzed)
to Microsoft.
Next steps
· Document Intelligence container configuration settings
· Azure container instance recipe


<!---- Page 524 ---------------------------------------------------------------------------------------------------------------------------------->
Configure Document Intelligence
containers
Article · 11/19/2024
Document Intelligence doesn't support containers for v4.0. Support for containers is
currently available with Document Intelligence version 2022-08-31 (GA) for all models
and 2023-07-31 (GA) for Read, Layout, Invoice, Receipt, and ID Document models:
· REST API 2022-08-31 (GA)
· REST API 2023-07-31 (GA)
· Client libraries targeting REST API 2022-08-31 (GA)
· Client libraries targeting REST API 2023-07-31 (GA)
V See Configure Document Intelligence v3.0 containers or Configure Document
Intelligence v3.1 containers for supported versions of container documentation.
Feedback
Was this page helpful?
Yes
No
Provide product feedback 2 | Get help at Microsoft Q&A


<!---- Page 525 ---------------------------------------------------------------------------------------------------------------------------------->
Document Intelligence container tags
Article · 04/03/2025
This content applies to:
v4.0 (GA)
Microsoft container registry (MCR)
Document Intelligence container images can be found within the Microsoft Artifact
Registry (also know as Microsoft Container Registry(MCR) , the primary registry for all
Microsoft published container images.
The following containers support DocumentIntelligence v3.1 models and features:
[] Expand table
Container
image
name
Layout 4.0 2
mcr.microsoft.com/azure-cognitive-services/form-recognizer/layout-
4.0: latest
Feedback
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A

| Container | image |
| --- | --- |
| name |  |
| Layout 4.0 2 |  |
|  | mcr.microsoft.com/azure-cognitive-services/form-recognizer/layout- |
|  | 4.0: latest |


<!---- Page 526 ---------------------------------------------------------------------------------------------------------------------------------->
Containers in disconnected (offline)
environments
Article · 11/19/2024
Document Intelligence doesn't support containers for v4.0. Support for containers is
currently available with Document Intelligence version 2022-08-31 (GA) for all models
and 2023-07-31 (GA) for Read, Layout, Invoice, Receipt, and ID Document models:
· REST API 2022-08-31 (GA)
· REST API 2023-07-31 (GA)
· Client libraries targeting REST API 2022-08-31 (GA)
· Client libraries targeting REST API 2023-07-31 (GA)
See Document Intelligence v3.0 containers in disconnected environments or
Document Intelligence v3.1 containers in disconnected environments for supported
versions of container documentation.
Feedback
Was this page helpful?
Yes
No
Provide product feedback | Get help at Microsoft Q&A


<!---- Page 527 ---------------------------------------------------------------------------------------------------------------------------------->
Transparency note and use cases for
Document Intelligence
Article · 03/12/2024
What is a transparency note?
An AI system includes not only the technology, but also the people who will use it, the
people who will be affected by it, and the environment in which it is deployed. Creating
a system that is fit for its intended purpose requires an understanding of how the
technology works, its capabilities and limitations, and how to achieve the best
performance.
Microsoft provides transparency notes to help you understand how our AI technology
works. This includes the choices system owners can make that influence system
performance and behavior, and the importance of thinking about the whole system,
including the technology, the people, and the environment. You can use transparency
notes when developing or deploying your own system, or share them with the people
who will use or be affected by your system.
Transparency notes are part of a broader effort at Microsoft to put our AI principles into
practice. To find out more, see Microsoft's AI principles .
The basics of Document Intelligence
Introduction
Document Intelligence is accessed via a set of APIs and allows developers to easily
extract text, structure, and fields from their documents. It is composed of features like:
. Read for text extraction.
. Layout and General Documents for structural insights and general ke-values and
entities such as names, places, and things.
· Prebuilt models for specific document types like invoices, receipts, business cards,
W2s, and IDs.
· Custom models for building models specific to your document types.
Document Intelligence supports one or more languages and locales for each of the
features, as listed in the Supported languages article.


<!---- Page 528 ---------------------------------------------------------------------------------------------------------------------------------->
Key terms
[] Expand table
Term
Definition
Read
This feature extracts text lines, words, and their locations from images and
documents, along with other information such as detected languages.
Layout
This feature extracts text, selection marks, and table structure (the row and column
numbers associated with the text). See Document Intelligence Layout.
General
Documents
Analyze documents and associate values to keys and entries to tables that it
discovers. For more information, see Document Intelligence General Documents.
Prebuilt
models
Prebuilt models are document-specific models for unique form types. These
models don't require custom training before use. For example, the prebuilt invoice
model extracts key fields from invoices. For more information, see Document
Intelligence prebuilt invoice model.
Custom
models
Document Intelligence allows you to train a custom model that's tailored to your
forms and documents. This model extracts text, key-value pairs, selection marks,
and table data. Custom models can be improved with human feedback by
applying human review, updating the labels, and retraining the model by using the
API.
Confidence
value
All Get Analysis Results operations return confidence values in the range between
0 and 1 for all extracted words and key-value mappings. This value represents the
service's estimate of how many times it correctly extracts the word out of 100 or
correctly maps the key-value pairs. For example, a word that's estimated to be
extracted correctly 82% of the time results in a confidence value of 0.82.
Add-on
features
Document Intelligence offers a set of add-on features to extend the results to
include more elements from your documents. Some add-on features incur an extra
cost and can be enabled and disabled depending on the scenario of the document
extraction. We currently offer high resolution, formula, styleFont, barcodes,
languages, keyValuePairs, and queryFields extraction capabilities. For more
information, see Document Intelligence Add-on capabilities.
Capabilities
System behavior
Azure AI Document Intelligence is a cloud-based Azure AI service that's built by using
optical character recognition (OCR), Text Analytics, and Custom Text from Azure AI
services. Custom models currently use Azure OpenAI service's GPT-3.5 model. OCR is
used to extract typeface and handwritten text documents. Document Intelligence uses

| Term | Definition |
| --- | --- |
| Read | This feature extracts text lines, words, and their locations from images and documents, along with other information such as detected languages. |
| Layout | This feature extracts text, selection marks, and table structure (the row and column numbers associated with the text). See Document Intelligence Layout. |
| General Documents | Analyze documents and associate values to keys and entries to tables that it discovers. For more information, see Document Intelligence General Documents. |
| Prebuilt models | Prebuilt models are document-specific models for unique form types. These models don't require custom training before use. For example, the prebuilt invoice model extracts key fields from invoices. For more information, see Document Intelligence prebuilt invoice model. |
| Custom models | Document Intelligence allows you to train a custom model that's tailored to your forms and documents. This model extracts text, key-value pairs, selection marks, and table data. Custom models can be improved with human feedback by applying human review, updating the labels, and retraining the model by using the API. |
| Confidence value | All Get Analysis Results operations return confidence values in the range between 0 and 1 for all extracted words and key-value mappings. This value represents the service's estimate of how many times it correctly extracts the word out of 100 or correctly maps the key-value pairs. For example, a word that's estimated to be extracted correctly 82% of the time results in a confidence value of 0.82. |
| Add-on features | Document Intelligence offers a set of add-on features to extend the results to include more elements from your documents. Some add-on features incur an extra cost and can be enabled and disabled depending on the scenario of the document extraction. We currently offer high resolution, formula, styleFont, barcodes, languages, keyValuePairs, and queryFields extraction capabilities. For more information, see Document Intelligence Add-on capabilities. |


<!---- Page 529 ---------------------------------------------------------------------------------------------------------------------------------->
OCR to detect and extract information from forms and documents supported by AI to
provide more structure and information to the text extraction.
Use cases
Intended uses
Document Intelligence includes features that enable customers from various industries
to extract data from their documents. The following scenarios are examples of
appropriate use cases:
. Accounts payable: A company can increase the efficiency of its accounts payable
clerks by using the prebuilt invoice model and custom forms to speed up invoice
data entry with a human in the loop. The prebuilt invoice model can extract key
fields, such as Invoice Total and Shipping Address.
. Insurance form processing: A customer can train a model by using custom forms
to extract a key-value pair in insurance forms and then feeds the data to their
business flow to improve the accuracy and efficiency of their process. For their
unique forms, customers can build their own model that extracts key values by
using custom forms. These extracted values then become actionable data for
various workflows within their business.
. Bank form processing: A bank can use the prebuilt ID model and custom forms to
speed up the data entry for "know your customer" documentation, or to speed up
data entry for a mortgage packet. If a bank requires their customers to submit
personal identification as part of a process, the prebuilt ID model can extract key
values, such as Name and Document Number, speeding up the overall time for
data entry.
· Robotic process automation (RPA): Using the custom extraction model, customers
can extract specific data needed from various types of documents. The key-value
pair extracted can then be entered into various systems such as databases, or CRM
systems, through RPA, replacing manual data entry. Customers can also use
custom classification model to categorize documents based on their content and
file them in proper location. As such, an organized set of data extracted from the
custom model can be an essential first step to document RPA scenarios for
businesses that handle large volumes of documents regularly.
Considerations when choosing other use cases


<!---- Page 530 ---------------------------------------------------------------------------------------------------------------------------------->
Consider the following factors when you choose a use case:
· Carefully consider applying human review when sensitive data or scenarios are
involved: It's important to include a human in the loop for a manual review when
you're dealing with high-stakes scenarios (e.g affecting someone's consequential
rights) or sensitive data. Machine learning models aren't perfect. Consider carefully
when to include a manual review step for certain workflows. For example, identity
verification at a port of entry such as airports should include human oversight.
· Carefully consider when using for awarding or denying of benefits: Doc
intelligence was not designed or evaluated for the award or denial of benefits, and
use in these scenarios may have unintended consequences. These scenarios
include:
o Medical insurance: This would include using healthcare records and medical
prescriptions as the basis for decisions on insurance reward or denial.
o Loan approvals: These include applications for new loans or refinancing of
existing ones.
· Carefully consider the supported document types and locales: Prebuilt models
have a predefined list of supported fields and are built for specific locales. Be sure
to carefully check the officially supported locales and document types to ensure
the best results. For example, see Document Intelligence prebuilt receipt locales.
. Legal and regulatory considerations: Organizations need to evaluate potential
specific legal and regulatory obligations when using any AI services and solutions,
which may not be appropriate for use in every industry or scenario. Additionally, AI
services or solutions are not designed for and may not be used in ways prohibited
in applicable terms of service and relevant codes of conduct.
Limitations
Technical limitations, operational factors, and ranges
Prebuilt model limitations
Document Intelligence prebuilt models are used for processing specific document types
and are pretrained on thousands of forms. This capability allows developers to get
started and get results within minutes, with no training data or labeling required. For
prebuilt models, it's important to note the list of input requirements, supported
document types, and locales for each prebuilt model for optimal results. For example,
refer to the prebuilt Invoice input requirements.


<!---- Page 531 ---------------------------------------------------------------------------------------------------------------------------------->
Custom model limitations
Document Intelligence custom models are trained using your own training data so that
the model can train to your specific forms and documents. This capability is heavily
dependent on the way you label the data, as well as the type of training data set you
provide. For custom models, it's important to note the limits of training data set size,
document page limits, and minimum number of samples needed for each type of
document. Custom models currently use Azure OpenAI Service's GPT-3.5 model. Further
information on the Azure OpenAI models can be found in the Azure OpenAI
Transparency Note.
The Service limits page contains more information on Azure AI Document Intelligence
service quotas and limits for all pricing tiers. It also contains model limitations and best
practices for model usage and avoiding request throttling.
Feature support
See the Analysis features table for a list of the different operations that Document
Intelligence models can perform.
System performance
Accuracy
Text is composed of lines and words at the foundational level and entities such as
names, prices, amounts, company names, and products at the document understanding
level.
Word-level accuracy
A popular measure of accuracy for OCR is word error rate (WER), or how many words
were incorrectly output in the extracted results. The lower the WER, the higher the
accuracy.
WER is defined as:
WER =
S+D+I
N
S+D+I
S+D+C
Where:


<!---- Page 532 ---------------------------------------------------------------------------------------------------------------------------------->
[ Expand table
Term
Definition
Example
S
Count of incorrect words
("substituted") in the output.
"Velvet" gets extracted as "Veivet" because "l" is detected
as "i."
D
Count of missing ("deleted")
words in the output.
For the text "Company Name: Microsoft," Microsoft isn't
extracted because it's handwritten or hard to read.
I
Count of nonexistent
("inserted") words in the
output.
"Department" gets incorrectly segmented into three
words as "Dep artm ent." In this case, the result is one
deleted word and three inserted words.
C
Count of correctly extracted
words in the output.
All words that are correctly extracted.
N
Count of total words in the
reference (N=S+D+C)
excluding I because those
words were missing from the
original reference and were
incorrectly predicted as
present.
Consider an image with the sentence, "Microsoft,
headquartered in Redmond, WA announced a new
product called Velvet for finance departments." Assume
the OCR output is ", headquartered in Redmond, WA
announced a new product called Veivet for finance dep
artm ents." In this case, S (Velvet) = 1, D (Microsoft) = 1, I
(dep artm ents) = 3, C (11), and N = S + D + C = 13.
Therefore, WER = (S + D + I) / N = 5 / 13 = 0.38 or 38%
(out of 100).
Using a confidence value
As covered in an earlier section, the service provides a confidence value for each
predicted word in the OCR output. Customers use this value to calibrate custom
thresholds for their content and scenarios to route the content for straight-through
processing or forwarding to the human-in-the-loop process. The resulting
measurements determine the scenario-specific accuracy.
OCR system performance implications can vary by scenarios where the OCR technology
is applied. We'll review a few examples to illustrate that concept.
. Medical device compliance: In this first example, a multinational pharmaceutical
company with a diverse product portfolio of patents, devices, medications, and
treatments needs to analyze FDA-compliant product label information and analysis
results documents. The company might prefer a low confidence value threshold for
applying human-in-the-loop because the cost of incorrectly extracted data can
have significant impact for consumers and fines from regulatory agencies.
· Image and documents processing: In this second example, a company performs
insurance and loan application processing. The customer using OCR might prefer a
medium confidence value threshold because the automated text extraction is

| Term | Definition | Example |
| --- | --- | --- |
| S | Count of incorrect words ("substituted") in the output. | "Velvet" gets extracted as "Veivet" because "l" is detected as "i." |
| D | Count of missing ("deleted") words in the output. | For the text "Company Name: Microsoft," Microsoft isn't extracted because it's handwritten or hard to read. |
| I | Count of nonexistent ("inserted") words in the output. | "Department" gets incorrectly segmented into three words as "Dep artm ent." In this case, the result is one deleted word and three inserted words. |
| C :unselected: | Count of correctly extracted words in the output. | All words that are correctly extracted. |
| N :selected: | Count of total words in the reference (N=S+D+C) excluding I because those words were missing from the original reference and were incorrectly predicted as present. | Consider an image with the sentence, "Microsoft, headquartered in Redmond, WA announced a new product called Velvet for finance departments." Assume the OCR output is ", headquartered in Redmond, WA announced a new product called Veivet for finance dep artm ents." In this case, S (Velvet) = 1, D (Microsoft) = 1, I (dep artm ents) = 3, C (11), and N = S + D + C = 13. Therefore, WER = (S + D + I) / N = 5 / 13 = 0.38 or 38% (out of 100). |


<!---- Page 533 ---------------------------------------------------------------------------------------------------------------------------------->
combined downstream with other information inputs and human-in-the-loop
steps for a holistic review of applications.
. Content moderation: For a large volume of e-commerce catalog data imported
from suppliers at scale, the customer might prefer a high confidence value
threshold with high accuracy because even a small percentage of falsely flagged
content can generate a lot of overhead for their human review teams and
suppliers.
Document and entity-level accuracy
At the document level, for example, in the case of an invoice or receipt, an error of only
one character in the entire document might be rated insignificant. But if that error is in
the text that represents the paid amount, the entire invoice or receipt might get flagged
as incorrect.
Another useful metric is the entity error rate (EER). It's the percentage of incorrectly
extracted entities, such as names, prices, amounts, and phone numbers, out of the total
number of the corresponding entities in one or more documents. For example, for a
total of 30 words representing 10 names, 2 incorrect words out of 30 equals 0.06 (6%)
WER. But if that results in 2 names out of 10 as incorrect, the Name EER is 0.20 (20%),
which is much higher than the WER.
Measuring both WER and EER is a useful exercise to get a full perspective on document
understanding accuracy.
Best practices for improving system performance
Consider the following points about limitations and performance:
The service supports images and documents. For the allowable limits for number of
pages, image sizes, paper sizes, and file sizes, see What is Document Intelligence ?.
. Many variables can affect the accuracy of the OCR results upon which Document
Intelligence depends. These variables include document scan quality, resolution,
contrast, light conditions, rotation, and text attributes such as size, color, and
density. For example, we recommend that the image be at least 50 x 50 pixels.
Refer to the product specifications and test the service on your documents to
validate the fit for your situation.
. Note the limitations of each service with regard to currently supported inputs,
languages and locales, and document types. For example, refer to the Layout
supported languages.


<!---- Page 534 ---------------------------------------------------------------------------------------------------------------------------------->
Best practices to improve custom model quality
When you're using the Document Intelligence custom model, you provide your own
training data so that the model can train to your specific forms and documents. The
following list uses the custom form model type to share starter tips for improving your
model quality.
· For filled-in forms, use examples that have all of their fields filled in.
. Use forms with real-world values that you expect to see for each field.
. If your form images are of lower quality, use a larger data set (at least 10-15
images, for example).
For a full guide and input requirements, see Build a training data set for a custom
model.
Evaluation of Document Intelligence
Document Intelligence's performance will vary depending on the real-world solutions
for which it's implemented. To ensure optimal performance in their scenarios, customers
should conduct their own evaluations. The service provides a confidence value in the
range between 0 and 1 for each extracted word and key-value mapping. Customers
should run a pilot or a proof of concept representing their use case to understand the
range of confidence values and the extraction quality from Document Intelligence. They
can then estimate the confidence value thresholds for the results to be either sent for
straight-through processing (STP) or reviewed by a human. For example, the customer
might submit results with confidence values greater than or equal to .80 for straight-
through processing and apply human review to results with confidence values less than
.80.
Evaluating and integrating Document Intelligence for
your use
Microsoft wants to help you responsibly develop and deploy solutions that use
Document Intelligence. We're taking a principled approach to upholding personal
agency and dignity by considering the AI systems' fairness, reliability and safety, privacy
and security, inclusiveness, transparency, and human accountability. These
considerations are in line with our commitment to developing Responsible AI.
When you're getting ready to deploy AI-powered products or features, the following
activities help to set you up for success:


<!---- Page 535 ---------------------------------------------------------------------------------------------------------------------------------->
. Understand what it can do: Fully assess the potential of Document Intelligence to
understand its capabilities and limitations. Understand how it will perform in your
particular scenario and context. For example, if you're using the prebuilt invoice
model, test with real-world invoices from your business processes to analyze and
benchmark the results against your existing process metrics.
. Respect an individual's right to privacy: Only collect data and information from
individuals for lawful and justifiable purposes. Only use data and information that
you have consent to use for this purpose.
· Legal review: Obtain appropriate legal review, particularly if you plan to use it in
sensitive or high-risk applications. Understand what restrictions you might need to
work within, and your responsibility to resolve any issues that might come up in
the future.
. Human-in-the-loop: Keep a human in the loop, and include human oversight as a
consistent pattern area to explore. This means ensuring constant human oversight
of the AI-powered product or feature and to maintain the role of humans in
decision-making. Ensure that you can have real-time human intervention in the
solution to prevent harm. A human in the loop enables you to manage situations
when Document Intelligence does not perform as required.
· Security: Ensure your solution is secure and that it has adequate controls to
preserve the integrity of your content and prevent unauthorized access.
Recommendations for preserving privacy
A successful privacy approach empowers individuals with information and provides
controls and protection to preserve their privacy.
· If Document Intelligence is part of a solution designed to incorporate personally
identifiable information (PII), think carefully about whether and how to record that
data. Follow applicable national and regional regulations on privacy and sensitive
data.
. Privacy managers should consider the retention policies on the extracted text and
values, and the underlying documents or images of those documents. The
retention policies will be tied to the intended use of each application.
Learn more about responsible AI
. Microsoft Al principles
· Microsoft responsible Al resources


<!---- Page 536 ---------------------------------------------------------------------------------------------------------------------------------->
· Microsoft Azure Learning courses on responsible Al
Learn more about Document Intelligence
· Document Intelligence overview
· Data, privacy, and security for Document Intelligence


<!---- Page 537 ---------------------------------------------------------------------------------------------------------------------------------->
Data, privacy, and security for
Document Intelligence
Article · 07/18/2023
This article provides details regarding how Document Intelligence processes your data.
Document Intelligence is designed with compliance, privacy, and security in mind.
However, you are responsible for its use and the implementation of this technology. It's
your responsibility to comply with all applicable laws and regulations in your jurisdiction.
How does Document Intelligence process data?
Authenticate (with subscription or API keys)
The most common way to authenticate access to Document Intelligence is by using the
customer's Document Intelligence API key. Each request to the service URL must include
an authentication header. This header passes along an API key (or token if applicable),
which is used to validate your subscription for a service or group of services. For more
information, see Authenticate requests to Azure AI services.
Secure data in transit (for scanning)
All Azure AI services endpoints, including the Document Intelligence API URLs, use
HTTPS URLs for encrypting data during transit. The client operating system needs to
support Transport Layer Security (TLS) 1.2 for calling the endpoints. For more
information, see Azure AI services security.
Encrypts input data for processing
The incoming data is processed in the same region where the Document Intelligence
resource was created. When you submit your documents to a Document Intelligence
operation, it starts the process of analyzing the document to extract all text and identify
structure and key values in a document. Your data and results are then temporarily
encrypted and stored in Azure Storage.
Retrieve the results
The "Get Analyze Results" operation is authenticated against the same API key that was
used to call the "Analyze" operation to ensure no other customer can access your data.


<!---- Page 538 ---------------------------------------------------------------------------------------------------------------------------------->
It returns the analysis job completion status, When the status shows as completed, the
operation also returns the extracted results in JSON format.
Data stored by Document Intelligence
For all analysis: To facilitate asynchronous analysis and checking the completion status
and returning the extracted results to the customer upon completion, the data and
extracted results are stored temporarily in Azure Storage in the same region. All
customers in the same region share the temporary storage. The customer's data is
logically isolated from other customers with their Azure subscription and API credentials.
For customer trained models: The Custom model feature allows customers to build
custom models from training data stored in customer's Azure blob storage locations.
The interim outputs after analysis and labeling are stored in the same location. The
trained custom models are stored in Azure storage in the same region and logically
isolated with their Azure subscription and API credentials.
Deletes data: For all features, the input data and results are deleted within 24 hours and
not used for any other purpose. For customer trained models, the customers can delete
their models and associated metadata at any time by using the API.
To learn more about privacy and security commitments, see the Microsoft Trust
Center Z .


<!---- Page 539 ---------------------------------------------------------------------------------------------------------------------------------->
Create a Document Intelligence Logic Apps
workflow
Article · 11/19/2024
This content applies to:
v4.0 (GA) | Previous versions:
v3.1 (GA)
v3.0 (GA)
1 Important
This tutorial and the Logic App Document intelligence connector targets Document
intelligence REST API v3.0 and forward.
Azure Logic Apps is a cloud-based platform that can be used to automate workflows without
writing a single line of code. The platform enables you to easily integrate Microsoft and your
applications with your apps, data, services, and systems. A Logic App is the Azure resource you
create when you want to develop a workflow. Here are a few examples of what you can do with
a Logic App:
· Create business processes and workflows visually.
· Integrate workflows with software as a service (Saas) and enterprise applications.
· Automate enterprise application integration (EAI), business-to-business (B2B), and
electronic data interchange (EDI) tasks.
For more information, see Logic Apps Overview.
In this tutorial, we show you how to build a Logic App connector flow to automate the
following tasks:
V Detect when an invoice as been added to a OneDrive folder.
V Process the invoice using the Document Intelligence prebuilt-invoice model.
V Send the extracted information from the invoice to a pre-specified email address.
Choose a workflow using a file from either your Microsoft OneDrive account or Microsoft
ShareDrive site:
Prerequisites
To complete this tutorial, you need the following resources:
. An Azure subscription. You can create a free Azure subscription|zz
. A free OneDrive or OneDrive for Business cloud storage account.


<!---- Page 540 ---------------------------------------------------------------------------------------------------------------------------------->
!
Note
o OneDrive is intended for personal storage.
· OneDrive for Business is part of Office 365 and is designed for organizations. It
provides cloud storage where you can store, share, and sync all work files.
. A free Outlook online ? or Office 365 z email account **.
. A sample invoice to test your Logic App. You can download and use our sample invoice
document for this tutorial.
. A Document Intelligence resource. Once you have your Azure subscription, create a
Document Intelligence resource in the Azure portal to get your key and endpoint. If
you have an existing Document Intelligence resource, navigate directly to your resource
page. You can use the free pricing tier (F0) to try the service, and upgrade later to a paid
tier for production.
o After the resource deploys, select Go to resource. Copy the Keys and Endpoint values
from your resource in the Azure portal and paste them in a convenient location, such
as Microsoft Notepad. You need the key and endpoint values to connect your
application to the Document Intelligence API. For more information, see create a
Document Intelligence resource.
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
0
P Search
«
C' Regenerate Key1 & Regenerate Key2
Overview
Activity log
Access control (IAM)
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
D
& Encryption
KEY 2
Pricing tier
D
Networking
Location/Region
Identity
westus2
n
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
₼
Locks
Monitoring
Automation
Ls
Help
V
Create a OneDrive folder


<!---- Page 541 ---------------------------------------------------------------------------------------------------------------------------------->
Before we jump into creating the Logic App, we have to set up a OneDrive folder.
1. Sign in to your OneDrive or OneDrive for Business & home page.
2. Select the + Add New button in the upper-left corner sidebar and select Folder.
OneDrive
+
Add New
Folder
3. Enter a name for your new folder and select Create.
Create a folder
×
Name
logic-app-tutorial
Folder color
Create
Cancel
4. You see the new folder in your files.
+ Add New
My files
Name v
Home
logic-app-tutorial
My files
Shared
taskbar
5. We're done with OneDrive for now.


<!---- Page 542 ---------------------------------------------------------------------------------------------------------------------------------->
Create a Logic App resource
At this point, you should have a Document Intelligence resource and a OneDrive folder all set.
Now, it's time to create a Logic App resource.
1. Navigate to the Azure portal 22 .
2. Select + Create a resource from the Azure home page.
Azure services
+
Create a
resource
3. Search for and choose Logic App from the search bar.
4. Select the create button
Home > Marketplace >
Logic App « ...
Microsoft
Logic App
Add to Favorites
Microsoft | Azure Service
3.5 (83 ratings)
Plan
Logic App
V
Create
5. Next, you're going to fill out the Create Logic App fields with the following values:
· Subscription. Select your current subscription.
· Resource group. The Azure resource group that contains your resource. Choose the
same resource group you have for your Document Intelligence resource.
· Type. Select Consumption. The Consumption resource type runs in global, multi-
tenant Azure Logic Apps and uses the Consumption billing model.


<!---- Page 543 ---------------------------------------------------------------------------------------------------------------------------------->
· Logic App name. Enter a name for your resource. We recommend using a
descriptive name, for example YourNameLogicApp.
· Publish. Select Workflow.
· Region. Select your local region.
· Enable log analytics. For this project, select No.
· Plan Type. Select Consumption. The Consumption resource type runs in global,
multi-tenant Azure Logic Apps and uses the Consumption billing model.
· Zone Redundancy. Select disabled.
6. When you're done, you have something similar to the following image (Resource group,
Logic App name, and Region may be different). After checking these values, select Review
+ create in the bottom-left corner.
Home > Create a resource > Logic App >
Create Logic App
...
-
Create a logic app, which lets you group workflows as a logical unit for easier management, deployment and sharing of
resources. Workflows let you connect your business-critical apps and services with Azure Logic Apps, automating your
workflows without writing a single line of code.
Project Details
Select a subscription to manage deployed resources and costs. Use resource groups like folders to organize and manage
all your resources.
Subscription * 1
your-subscription
V
Resource Group * @
your-resource-group
V
Create new
Instance Details
Type *
Consumption
Standard
Looking for the classic consumption create experience? Click here
Logic App name *
Logic App name
Region *
West US
V
Enable log analytics *
Yes
No
Review + create
< Previous
Next : Tags >
7. A short validation check runs. After it completes successfully, select Create in the bottom-
left corner.
8. Next, you're redirected to a screen that says Deployment in progress. Give Azure some
time to deploy; it can take a few minutes. After the deployment is complete, you see a


<!---- Page 544 ---------------------------------------------------------------------------------------------------------------------------------->
banner that says, Your deployment is complete. When you reach this screen, select Go to
resource.
9. Finally, you're redirected to the Logic Apps Designer page. There's a short video for a
quick introduction to Logic Apps available on the home screen. When you're ready to
begin designing your Logic App, select the Blank Logic App button from the Templates
section.
Start with a common trigger
Pick from one of the most commonly used triggers, then orchestrate any number of actions using the rich collection of connectors
When a message is
received in a Service
Bus queue
When a HTTP
request is received
When a new tweet is
posted
E
When an Event Grid
resource event
occurs
Recurrence
When a new file is
created on OneDrive
When a file is added
to FTP server
J
When a new email is
received in
Outlook.com
Templates
Choose a template below to create your Logic App.
Category :
All
Sort by :
Popularity
Blank Logic App
Azure Monitor -
Metrics Alert
Handler
Auto tier Azure
blobs based on the
last modified time.
Delete old Azure
blobs
J
,
10
10
01
10. You see a screen that looks similar to the following image. Now, you're ready to start
designing and implementing your Logic App.
Logic app designer
Save
Discard
Run Trigger
Search connectors and triggers
For You
All
Built-in
Standard
Enterprise
Custom
Recent
Clear
Office 365
Outlook
Control
Form
Recognizer
OneDrive for
Business
OneDrive


<!---- Page 545 ---------------------------------------------------------------------------------------------------------------------------------->
Create an automation flow
Now that you have the Logic App connector resource set up and configured, let's create the
automation flow and test it out!
1. Search for and select OneDrive or OneDrive for Business in the search bar. Then, select
the When a file is created trigger.
onedrive
For You
All
Built-in
Standard
x
OneDrive
Excel Online
(Business)
Excel Online
(OneDrive)
Triggers
Actions
When a file is created
OneDrive
2. Next, a pop-up window appears, prompting you to log into your OneDrive account.
Select Sign in and follow the prompts to connect your account.
? Tip
If you try to sign into the OneDrive connector using an Office 365 account, you may
receive the following error: Sorry, we can't sign you in here with your
@MICROSOFT.COM account.
· This error happens because OneDrive is a cloud-based storage for personal use
that can be accessed with an Outlook.com or Microsoft Live account not with
Office 365 account.
. You can use the OneDrive for Business connector if you want to use an Office
365 account. Make sure that you have created a OneDrive Folder for this
project in your OneDrive for Business account.


<!---- Page 546 ---------------------------------------------------------------------------------------------------------------------------------->
3. After your account is connected, select the folder you created earlier in your OneDrive or
OneDrive for Business account. Leave the other default values in place.
When a file is created
...
* Folder
The unique identifier of the folder.
Include subfolders
No
Infer Content Type
Yes
How often do you want to check for items?
3
Minute
Add new parameter
Change connection.
4. Next, we're going to add a new step to the workflow. Select the + New step button
underneath the newly created OneDrive node.
When a file is created
i
* Folder
/logic-app-tutorial
Include subfolders
No
Infer Content Type
Yes
How often do you want to check for items?
3
Minute
Add new parameter
Change connection.
+ New step
5. A new node is added to the Logic App designer view. Search for Form Recognizer
(Document Intelligence forthcoming) in the Choose an operation search bar and select
Analyze Document for Prebuilt or Custom models (v3.0 API) from the list.


<!---- Page 547 ---------------------------------------------------------------------------------------------------------------------------------->
V
Triggers
Actions
Analyze Business Card
A
Form Recognizer
O
Analyze Custom Form
Form Recognizer
O
Analyze Document for Prebuilt or Custom models (v3.0 API)
Form Recognizer
O
Analyze ID Document
Form Recognizer
0
Analyze Invoice
Form Recognizer
O
Analyze Layout
Form Recognizer
O
4
6. Now, you see a window to create your connection. Specifically, you're going to connect
your Document Intelligence resource to the Logic Apps Designer Studio:
· Enter a Connection name. It should be something easy to remember.
· Enter the Document Intelligence resource Endpoint URL and Account Key that you
copied previously. If you skipped this step earlier or lost the strings, you can
navigate back to your Document Intelligence resource and copy them again. When
you're done, select Create.
+
Form Recognizer
i
* Connection name
Enter name for connection
* Endpoint URL @
Form Recognizer Endpoint Url (Example: https://your-formrecognizer-
* Account Key @
Form Recognizer Account Key
Create
Cancel
!
Note


<!---- Page 548 ---------------------------------------------------------------------------------------------------------------------------------->
If you already logged in with your credentials, the prior step is skipped.
7. Next, you see the selection parameters window for the Analyze Document for Prebuilt or
Custom Models (v3.0 API) connector.
1
Analyze Document for Prebuilt or Custom models (v3.0 API)
...
* Model Identifier
Prebuilt modellds: prebuilt-read, prebuilt-layout, prebuilt-document, prebuilt-
Document/Image File
Content
A PDF document or image (JPG, PNG, BMP, TIFF) file to analyze.
Document/Image URL
Url path for input file. Alternative to Document/Image File Content.
Add new parameter
V
Connected to logic-app-connector. Change connection.
8. Complete the fields as follows:
. Model Identifier. Specify which model you want to call, in this case we're calling the
prebuilt invoice model, so enter prebuilt-invoice.
· Document/Image File Content. Select this field. A dynamic content pop-up appears.
If it doesn't, select the Add dynamic content button below the field and choose File
content. This step is essentially sending the file(s) to be analyzed to the Document
Intelligence prebuilt-invoice model. Once you see the File content badge show in
the Document /Image file content field, you've completed this step correctly.
· Document/Image URL. Skip this field for this project because we're already pointing
to the file content directly from the OneDrive folder.
. Add new parameter. Skip this field for this project.


<!---- Page 549 ---------------------------------------------------------------------------------------------------------------------------------->
When a file is created
...
Analyze Document for Prebuilt or Custom models (v3.0 API)
...
* Model Identifier
prebuilt-invoice
Document/Image File
Content
&
File content x
Document/Image URL
Url path for input file. Alternative to Document/Image File Content.
Add new parameter
Connected to logic-app-connector. Change connection.
9. We need to add a few more steps. Once again, select the + New step button to add
another action.
10. In the Choose an operation search bar, enter Control and select the Control tile.
I
Choose an operation
X
control
For You
All
Built-in
Standard
Enterprise
Custom
PKI
C.
Control
AIHW
MyHospital ...
Azure Data
Explorer
Azure
DevOps
Bitbucket
Cloud PKI
Management
Cloudmersiv
e File ...
11. Scroll down and select the For each Control tile from the Control list.


<!---- Page 550 ---------------------------------------------------------------------------------------------------------------------------------->
Control
X
Search connectors and actions
Triggers
Actions
Condition
Control
0
For each
Control
O
Scope
Control
0
12. In the For each step window, there's a field labeled Select an output from previous steps.
Select this field. A dynamic content pop-up appears. If it doesn't, select the Add dynamic
content button below the field and choose documents.
Dynamic content
Expression
...
Search dynamic content
A
documents
Add dynamic content
+
Extracted documents.
boundingRegions
Bounding regions covering the entity.
13. Now, select Add an Action from within the For each step window.
14. In the Choose an operation search bar, enter Outlook and select Outlook.com (personal)
or Office 365 Outlook (work).
15. In the actions list, scroll down until you find Send an email (V2) and select this action.


<!---- Page 551 ---------------------------------------------------------------------------------------------------------------------------------->
For each
...
* Select an output from previous steps
documents
Send an email (V2)
...
* Body
Font
4
12 BIU / EEEPC
Specify the body of the mail
* Subject
Specify the subject of the mail
* To
Specify email addresses separated by semicolons like someone@contoso.com
Importance
Normal
V
X
Add new parameter
V
Change connection.
Add an action
16. Just like with OneDrive, you're asked to sign into your Outlook or Office 365 Outlook
account. After you sign in, you see a window where we're going to format the email with
dynamic content that Document Intelligence extracts from the invoice.
17. We're going to use the following expression to complete some of the fields:
PowerApps Formula
items('For_each' ) ?[' fields ' ]?[ ' FIELD-NAME ' ]?[ ' content' ]
1. In order to access a specific field, we select the add the dynamic content button and
select the Expression tab.


<!---- Page 552 ---------------------------------------------------------------------------------------------------------------------------------->
Dynamic content
Expression
fx
OK
A
2. In the fx box, copy and paste the above formula and replace FIELD-NAME with the name
of the field we want to extract. For the full list of available fields, refer to the concept page
for the given API. In this case, we use the prebuilt-invoice model field extraction values.
3. We're almost done! Make the following changes to the following fields:
· To. Enter your personal or business email address or any other email address you
have access to.
· Subject. Enter Invoice received from: and then add the following expression:
PowerApps Formula
items('For_each' )?[' fields ' ]?['VendorName' ]?[ ' content' ]
· Body. We're going to add specific information about the invoice:
o Type Invoice ID: and, using the same method as before, append the following
expression:
PowerApps Formula
items('For_each' )?['fields' ]?['InvoiceId' ]?[' content' ]
· On a new line type Invoice due date: and append the following expression:
PowerApps Formula
items('For_each' )?['fields' ]?[ 'DueDate' ]? [ ' content' ]
o Type Amount due: and append the following expression:


<!---- Page 553 ---------------------------------------------------------------------------------------------------------------------------------->
PowerApps Formula
items('For_each' )?['fields' ]?['AmountDue' ]?[ ' content' ]
Lastly, because the amount due is an important number, we also want to send
the confidence score for this extraction in the email. To do this type Amount due
(confidence): and append the following expression:
PowerApps Formula
items('For_each' )?['fields' ]?['AmountDue' ]?[' confidence' ]
· When you're done, the window looks similar to the following image:
For each
...
* Select an output from previous steps
documents
Send an email (V2)
O
...
* Body
Font
V 12 B I U / SEE 8
Q
invoice:
fx
items( ... )
due:
fx
items( ... ) x
amount:
fx
items( ... )
confidence:
fx
items( ... ) x
Add dynamic content
+
* Subject
fx
items( ... ) x
* To
your-email-address
Importance
X
Normal
V
Add new parameter
V
Connected to lajanuar@microsoft.com. Change connection.


<!---- Page 554 ---------------------------------------------------------------------------------------------------------------------------------->
4. Select Save in the upper left corner.
Logic Apps Designer
...
Save
Discard
Run Trigger
1 Note
· This current version only returns a single invoice per PDF.
· The "For each loop" is required around the send email action to enable an output
format that may return more than one invoice from PDFs in the future.
Test the automation flow
Let's quickly review what we completed before we test our flow:
V We created a trigger-in this scenario. The trigger is activated when a file is created in a
pre-specified folder in our OneDrive account.
V We added a Document Intelligence action to our flow. In this scenario, we decided to use
the invoice API to automatically analyze an invoice from the OneDrive folder.
V We added an Outlook.com action to our flow. We sent some of the analyzed invoice data
to a pre-determined email address.
Now that we created the flow, the last thing to do is to test it and make sure that we're getting
the expected behavior.
1. To test the Logic App, first open a new tab and navigate to the OneDrive folder you set
up at the beginning of this tutorial. Add this file to the OneDrive folder Sample invoice."
2. Return to the Logic App designer tab and select the Run trigger button and select Run
from the drop-down menu.
Logic Apps Designer
...
Save
Discard
Run Trigger
Run


<!---- Page 555 ---------------------------------------------------------------------------------------------------------------------------------->
3. You see a message in the upper=right corner indicating that the trigger was successful:
i
Successfully checked the trigger
Successfully checked the trigger of logic app for new data.
4. Navigate to your Logic App overview page by selecting your app name link in the upper-
left corner.
Home >
my-logic-app
>
Logic Apps Designer
...
5. Check the status, to see if the run succeeded or failed. You can select the status indicator
to check which steps were successful.
Status
Succeeded
!
Failed
6. If your run failed, check the failed step to ensure that you entered the correct information.
1s
When a file is created
Analyze Document for Prebuilt or Custom models (v3.0 API)
!
1s
For each
×
Os
i
Skipped.
7. After a workflow run succeeds, check your email. There's a new email with the information
we specified.


<!---- Page 556 ---------------------------------------------------------------------------------------------------------------------------------->
Invoice received from:CONTOSO LTD.
Invoice ID: INV-100
Invoice due date:12/15/2019
Amount due: $610.00
Amount due (confidence):0.967
8. After you're done, disable or delete your logic app so that usage stops.
Run Trigger
Refresh
Edit
Delete
Disable
Congratulations! You completed this tutorial.
Next steps
Learn more about Document Intelligence models


<!---- Page 557 ---------------------------------------------------------------------------------------------------------------------------------->
Create a document processing custom
model
Article · 04/21/2025
After you review the requirements, you can get started creating your document processing
model.
Create your model with the wizard
You can create a document processing model by using the Create custom model wizard. The
wizard guides you through the process of creating a model to extract information from
documents.
1. Sign in to Power Apps or Power AutomateZ.
2. On the left pane, select ... More > AI hub.
(Optional) To keep AI models permanently on the menu for easy access, select the pin
icon next to AI hub.
3. Under Discover an AI capability, select AI models.
4. Select Extract custom information from documents.
5. Select Create custom model.
6. A step-by-step wizard walks you through the process by asking you to list all data you
want to extract from your document.
Learn more in the Select the type of document section in this article.
If you want to create your model by using your own documents, make sure you have at
least five examples that use the same layout. Otherwise, you can use sample data to
create the model.
7. Select Train.
8. Test the model by selecting Quick test.
Select the type of document
On the Choose document type step, select the type of document you want to build an AI
model to automate data extraction. There are three options: Fixed template documents,


<!---- Page 558 ---------------------------------------------------------------------------------------------------------------------------------->
General documents, and Invoices.
Power Apps | Al Builder
Choose document type
Choose information to extract
Add collections of documents
Tag documents
Model summary
Select the type of documents your model will process
Fixed template
documents
General documents
Invoices
· Fixed template documents: Previously known as structured, this option is ideal when, for
a given layout, the fields, tables, checkboxes, signatures, and other items can be found in
similar places. You can teach this model to extract data from structured documents that
have different layouts. This model has a quick training time.
. General documents: Previously known as unstructured, this option is ideal for any kind of
documents, especially when there's no set structure, or when the format is complex. You
can teach this model to extract data from structured or unstructured documents that
have different layouts. This model is powerful, but has long training time.
· Invoices: Augment the behaviors of the prebuilt invoice processing model by adding new
fields to be extracted in addition to the ones by default, or samples of documents not
properly extracted.
Understand document intelligence versions
The document intelligence model is available in two versions: v4.0 and v3.1. The version of your
model depends on when you last edited the model.
Document Intelligence v4.0 - General Availability (GA)
In addition to the features listed in this article, v4.0 retains all the capabilities of v3.1.
· Overlapping fields: v4.0 supports overlapping fields in custom models, which let you
extract information more effectively from documents with complex layouts.
· Signature detection: v4.0 detects signatures in documents, which is especially useful for
contracts, agreements, and other signed forms.
. Confidence scores for tables: v4.0 provides confidence scores for table and their cells.


<!---- Page 559 ---------------------------------------------------------------------------------------------------------------------------------->
· OCR engine improvements: v4.0 improves the optical character recognition (OCR)
engine, enhancing text recognition accuracy and supporting more document types and
formats.
Document Intelligence v3.1 General Availability (GA)
· v3.1 supports custom models trained to recognize specific data patterns, such as unique
text fields or structures.
. v3.1 includes custom template models that let users create templates based on their
document layout and structure.
Check the model version
You can verify the version used to train and publish your model. To do this, select Settings >
Published model version > Last trained model version.
Model settings
Edit model
Share
Settings
Delete
Model name
Models > Cycles Invoices
Cycles Invoices
Document Processing . Fixed template documents . Published
Model id
Published version
Last trained version
Published model version
GA (v3.1)
Accuracy score
O
More details
Information to extract
Table 1
Last trained model version
GA (v4.0)
99%
Excellent
Created date
1/9/2025, 1:26:45 PM
This model correctly predicted 99% of actual results and
may be ready to be used. To improve the accuracy score,
review full evaluation.
Publish
Quick test
You can move a model from v3.1 to v4.0 by editing, retraining, and publishing it. Re-tagging
and other specific modification aren't necessary. Learn more in FAQ for document processing.
Define information to extract


<!---- Page 560 ---------------------------------------------------------------------------------------------------------------------------------->
On the Choose information to extract screen, define the fields, tables, and checkboxes you
want to teach your model to extract. To start defining these, select +Add.
Add
X
Text field
Recipient's / Lender's
Contoso, Ltd.
4567 Main St.
Buffalo, NY 90852
123
Number field
Mortgage
Statement
Date field
1.Mortgage interest
$ 13,345.34
Recipient's / Lender's TIN
Payer's/Borrower's TIN
306-14-5298
2.Outstanding mortgage
$ 764,321.33
3.Mortgage origination date
10/08/2019
Checkbox
750-01-1829
Payer's/Borrower's name
4.Refund of overpaid
$ 15.01
S.Mortgage insurance
$ 1,222.05
Table
Helena Wilcox
Street address
123 Main St Apt. 10
6. Address
City or Town, state or province, country, and ZIP or foreign postal code
Brooklyn, NY 90852
8.Number of properties securing
the mortgage
1025 Fifth St.
Sunnyvale, CA 27673
9.Other
2
Overview
Use this to extract printed or handwritten text from your documents.
For example: Name, Address, Phone number.
Next
Cancel
1. For each text field, provide a name for the field to use in the model.
2. For each number field, provide a name for the field to use in the model.
Define the format dot (.) or comma (,) as a decimal separator.
3. For each Date field, provide a name for the field to use in the model.
Also, define the format (Year, Month, Day), or (Monthly, Day, Year), or (Day, Month, Year).
4. For each checkbox, provide a name for the checkbox to use in the model.
Define separate checkboxes for each item that can be checked in a document.
5. For each table, provide the name for the table.
Define the different columns that the model should extract.
!
Note


<!---- Page 561 ---------------------------------------------------------------------------------------------------------------------------------->
The custom invoices model comes with default fields that can't be edited.
Group documents by collections
A collection is a group of documents that share the same layout. Create as many collections as
document layouts that you want your model to process. For example, if you're building an AI
model to process invoices from two different vendors, each having their own invoice template,
create two collections.
Power Automate
=
Choose information to extract
My multiple invoices model
A
Save and close
®
4 fields, 1 table
Add collections of documents
?
Quick tips
9
Add collections of documents
Add sample documents for your model to study. Put similar documents into the same collection.
Tag documents
Create a collection for each layout
1
Improve the performance of your
model
+
Model summary
New collection
You must add at least five examples
of your document type. Adding up
to 20 examples could yield better
results.
₼
0
What does analyze do?
Your uploaded documents will be
analyzed to identify their overall
structure, detect form fields, and
N
m
extract field values. This may take a
few minutes.
Get help or send feedback
Get the answers you need, or tell us
about your experiences.
2
Get help
18
Back
Analyze
Add at least five sample documents for each collection
For each collection that you create, you need to upload at least five sample documents per
collection. Files with formats JPG, PNG, and PDF files are accepted.

|  | Power Automate |  |  |  |
| --- | --- | --- | --- | --- |
| = |  | My multiple |  | invoices model A Save and close |
| ® | :selected: Choose information to extract 4 fields, 1 table | Add collections of documents |  | ? Quick tips |
| 9 :unselected: | :selected: Add collections of documents | Add sample documents for your model to study. Put similar documents into the same collection. |  |  |
|  | :unselected: Tag documents | Create a collection for each layout 1 |  | Improve the performance of your model |
| + | :unselected: Model summary | New collection |  | You must add at least five examples of your document type. Adding up to 20 examples could yield better results. |
| ₼ |  |  |  |  |
| 0 :unselected: |  |  |  | What does analyze do? |
|  |  |  |  | Your uploaded documents will be analyzed to identify their overall |
| N :selected: m :unselected: |  |  |  | structure, detect form fields, and extract field values. This may take a few minutes. Get help or send feedback Get the answers you need, or tell us about your experiences. |
|  |  |  |  | 2 Get help |
|  |  | 18 |  |  |
|  |  |  |  |  |
|  |  | Back | Analyze Add at least five sample documents for each collection |  |


<!---- Page 562 ---------------------------------------------------------------------------------------------------------------------------------->
Power Automate
=
My multiple invoices model
B
Save and close
Choose information to extract
4 fields, 1 table
Your model will process documents with different layouts. This capability is still in preview. L
Learn more
×
?
Quick tips
Add collections of documents
Q
Add collections of documents
Add sample documents for your model to study. Put similar documents into the same collection.
Improve the performance of your
model
Tag documents
+
Create a collection for each layout
You must add at least five examples
of your document type. Adding up
to 20 examples could yield better
results.
Model summary
New collection
Add documents
1
0
₹
Q
What does analyze do?
Your uploaded documents will be
analyzed to identify their overall
structure, detect form fields, and
extract field values. This may take a
few minutes.
+
+
Get help or send feedback
Adatum
0 documents
Contoso
0 documents
Get the answers you need, or tell us
about your experiences.
A
Get help
4
Back
Analyze
Add at least five sample documents for each collection
1 Note
You can create up to 200 collections per model.
Next step
Tag documents in a document processing model
Related information
· Training: Process custom documents with Al Builder (module)
· Interpret confidence score for tables and table cells
· FAQ for document processing


<!---- Page 563 ---------------------------------------------------------------------------------------------------------------------------------->
Use the invoice processing prebuilt
model in Power Automate
Article · 11/20/2024
1. Sign in to Power Automate .
2. Select My flows in the left pane, and then select New flow > Instant cloud flow.
3. Name your flow, select Manually trigger a flow under Choose how to trigger this
flow, and then select Create.
4. Expand Manually trigger a flow, and then select +Add an input > File as the input
type.
5. Replace File Content with My invoice (also known as the title).
6. Select +New step > AI Builder, and then select Extract information from invoices
in the list of actions.
7. Specify My invoice from the trigger in the Invoice file input.
£
Manually trigger a flow
?
...
0
My invoice
Please select file or image
...
+ Add an input
B
Extract information from invoices
?
...
* Invoice file
S
My invoice x
Show advanced options
8. In the successive actions, you can use any of the invoice values from the model
output.


<!---- Page 564 ---------------------------------------------------------------------------------------------------------------------------------->
Manually trigger a flow
?
...
Extract information from invoices
?
...
x
Add a row into a table
?
...
* Location
OneDrive for Business
* Document Library
OneDrive
* File
* Table
/ Invoices.xlsx
Table1
Invoice id
Invoice ID
Subtotal
Subtotal (num
Tax amount
Total tax (num ..
Total amount
Invoice total (n
Due date
Due date (date)
Bill to address
Billing address
Show advanced options
Congratulations! You've created a flow that uses the AI Builder invoice processing
model. Select Save on the top right, and then select Test to try out your flow.
Page range
For large documents, it's possible to specify the page range to process.

|  | Manually trigger a flow |  | ? ... |
| --- | --- | --- | --- |
|  |  |  |  |
|  | Extract information from invoices |  | ? ... |
|  |  |  |  |
| x | Add a row into a table |  | ? ... |
|  |  |  |  |
| * Location |  | OneDrive for Business |  |
|  |  |  |  |
| * Document Library |  | OneDrive |  |
|  |  |  |  |
| * File |  | / Invoices.xlsx |  |
|  |  |  |  |
| * Table |  | Table1 |  |
|  |  |  |  |
| Invoice id |  | Invoice ID :selected: |  |
|  |  |  |  |
| Subtotal |  | Subtotal (num :selected: |  |
|  |  |  |  |
| Tax amount |  | Total tax (num .. :selected: |  |
|  |  |  |  |
| Total amount |  | Invoice total (n :selected: |  |
|  |  |  |  |
| Due date |  | Due date (date) :selected: |  |
|  |  |  |  |
| Bill to address |  | Billing address :selected: |  |
| Show advanced options |  |  |  |


<!---- Page 565 ---------------------------------------------------------------------------------------------------------------------------------->
Manually trigger a flow
?
...
Extract information from invoices
?
...
* Invoice file
Pages
S
My invoice x
Enter page range (Ex: 1 or 3-5). Only returns results of a single invoice.
Hide advanced options
You can enter a page value or page range in the Pages parameter. Example: 1 or 3-5.
1 Note
If you have a large document with only one invoice, we strongly recommend to use
the Pages parameter to aim at your invoice, and therefore reduce the cost of
model prediction and increase performance. However, the page range should
contain a unique invoice for the action to return correct data.
Example: A document contains a first invoice in page 2 and a second invoice that
spans over pages 3 and 4:
. If you enter page range 2, it will return the data of the first invoice.
· If you enter page range 3-4, it will only return the data of the second invoice.
· If you enter page range 2-4, it will return partial data of the first and second
invoices (should be avoided).
Parameters
Input
[] Expand table
Name
Required
Type
Description
Receipt file
Yes
file
The invoice file to process
Pages
No
string
Page range to process

| Name | Required | Type | Description |
| --- | --- | --- | --- |
| Receipt file | Yes | file | The invoice file to process |
| Pages | No | string | Page range to process |


<!---- Page 566 ---------------------------------------------------------------------------------------------------------------------------------->
Output
0
Expand table
Name
Type
Definition
Amount due (text)
string
Amount due as it's written on the invoice
Amount due (number)
float
Amount due in standardized number format. Example:
1234.98
Confidence of amount due
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Billing address
string
Billing address
Confidence of billing
address
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Billing address recipient
string
Billing address recipient
Confidence of billing
address recipient
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Customer address
string
Customer address
Confidence of customer
address
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Customer address
recipient
string
Customer address recipient
Confidence of customer
address recipient
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Customer ID
string
Customer ID
Confidence of customer ID
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Customer name
string
Customer name
Confidence of customer
name
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Due date (text)
string
Due date as it's written on the invoice
Due date (date)
Due date in standardized date format. Example: 2019-05-
31T00:00:00Z
Confidence of due date
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).

| Name | Type | Definition |
| --- | --- | --- |
| Amount due (text) | string | Amount due as it's written on the invoice |
| Amount due (number) | float | Amount due in standardized number format. Example: 1234.98 |
| Confidence of amount due | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Billing address | string | Billing address |
| Confidence of billing address | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Billing address recipient | string | Billing address recipient |
| Confidence of billing address recipient | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Customer address | string | Customer address |
| Confidence of customer address | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Customer address recipient | string | Customer address recipient |
| Confidence of customer address recipient | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Customer ID | string | Customer ID |
| Confidence of customer ID | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Customer name | string | Customer name |
| Confidence of customer name | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Due date (text) | string | Due date as it's written on the invoice |
| Due date (date) |  | Due date in standardized date format. Example: 2019-05- 31T00:00:00Z |
| Confidence of due date | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |


<!---- Page 567 ---------------------------------------------------------------------------------------------------------------------------------->
Name
Type
Definition
Invoice date (text)
string
Invoice date as it's written on the invoice
Invoice date (date)
date
Invoice date in standardized date format. Example: 2019-
05-31T00:00:00Z
Confidence of invoice date
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Invoice ID
string
Invoice ID
Confidence of invoice ID
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Invoice total (text)
string
Invoice total as it's written on the invoice
Invoice total (number)
float
Invoice total in standardized date format. Example: 2019-
05-31T00:00:00Z
Confidence of invoice total
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Purchase order
string
Purchase order
Confidence of purchase
order
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Remittance address
string
Remittance address
Confidence of remittance
address
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Remittance address
recipient
string
Remittance address recipient
Confidence of remittance
address recipient
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Service address
string
Service address
Confidence of service
address
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Service address recipient
string
Service address recipient
Confidence of service
address recipient
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Shipping address
string
Shipping address
Confidence of shipping
float
How confident the model is in its prediction. Score

| Name | Type | Definition |
| --- | --- | --- |
| Invoice date (text) | string | Invoice date as it's written on the invoice |
| Invoice date (date) | date | Invoice date in standardized date format. Example: 2019- 05-31T00:00:00Z |
| Confidence of invoice date | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Invoice ID | string | Invoice ID |
| Confidence of invoice ID | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Invoice total (text) | string | Invoice total as it's written on the invoice |
| Invoice total (number) | float | Invoice total in standardized date format. Example: 2019- 05-31T00:00:00Z |
| Confidence of invoice total | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Purchase order | string | Purchase order |
| Confidence of purchase order | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Remittance address | string | Remittance address |
| Confidence of remittance address | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Remittance address recipient | string | Remittance address recipient |
| Confidence of remittance address recipient | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Service address | string | Service address |
| Confidence of service address | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Service address recipient | string | Service address recipient |
| Confidence of service address recipient | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Shipping address | string | Shipping address |
| Confidence of shipping | float | How confident the model is in its prediction. Score |


<!---- Page 568 ---------------------------------------------------------------------------------------------------------------------------------->
Name
Type
Definition
address
between 0 (low confidence) and 1 (high confidence).
Shipping address recipient
string
Shipping address recipient
Confidence of shipping
address recipient
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Subtotal (text)
string
Subtotal as it's written on the invoice
Subtotal (number)
float
Subtotal in standardized number format. Example: 1234.98
Confidence of subtotal
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Total tax (text)
string
Total tax as it's written on the invoice
Total tax (number)
float
Total tax in standardized number format. Example: 1234.98
Confidence of total tax
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Vendor address
string
Vendor address
Confidence of vendor
address
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Vendor address recipient
string
Vendor address recipient
Confidence of vendor
address recipient
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Vendor name
string
Vendor name
Confidence of vendor
name
float
How confident the model is in its prediction. Score
between 0 (low confidence) and 1 (high confidence).
Detected text
string
Line of recognized text from running OCR on an invoice.
Returned as a part of a list of text.
Page number of detected
text
integer
Which page the line of recognized text is found on.
Returned as a part of a list of text.
Related topics
Invoice processing overview
Feedback

| Name | Type | Definition |
| --- | --- | --- |
| address |  | between 0 (low confidence) and 1 (high confidence). |
| Shipping address recipient | string | Shipping address recipient |
| Confidence of shipping address recipient | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Subtotal (text) | string | Subtotal as it's written on the invoice |
| Subtotal (number) | float | Subtotal in standardized number format. Example: 1234.98 |
| Confidence of subtotal | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Total tax (text) | string | Total tax as it's written on the invoice |
| Total tax (number) | float | Total tax in standardized number format. Example: 1234.98 |
| Confidence of total tax | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Vendor address | string | Vendor address |
| Confidence of vendor address | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Vendor address recipient | string | Vendor address recipient |
| Confidence of vendor address recipient | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Vendor name | string | Vendor name |
| Confidence of vendor name | float | How confident the model is in its prediction. Score between 0 (low confidence) and 1 (high confidence). |
| Detected text | string | Line of recognized text from running OCR on an invoice. Returned as a part of a list of text. |
| Page number of detected text | integer | Which page the line of recognized text is found on. Returned as a part of a list of text. |


<!---- Page 569 ---------------------------------------------------------------------------------------------------------------------------------->
Was this page helpful?
Yes
No
Provide product feedback


<!---- Page 570 ---------------------------------------------------------------------------------------------------------------------------------->
Use the receipt processing prebuilt
model in Power Automate
Article · 01/27/2025
1. Sign in to Power Automate .
2. Select My flows in the left pane, and then select New flow > Instant cloud flow.
3. Name your flow, select Manually trigger a flow under Choose how to trigger this
flow, and then select Create.
4. Expand Manually trigger a flow, and then select +Add an input > File as the input
type.
5. Replace File Content with My receipt (also known as the title).
6. Select +New step > AI Builder, and then select Extract information from receipts
in the list of actions.
7. Select the Receipt file input, and then select My receipt from the Dynamic content
list:
Manually trigger a flow
?
...
0
My receipt
Please select file or image
...
+ Add an input
Extract information from receipts
?
...
* Receipt file
£
My receipt x
Show advanced options
8. In the successive actions, you can use any of the receipt values from the model
output section below.


<!---- Page 571 ---------------------------------------------------------------------------------------------------------------------------------->
Manually trigger a flow
?
...
Extract information from receipts
?
...
x
Add a row into a table
?
...
* Location
OneDrive for Business
* Document Library
* Table
OneDrive
* File
/Receipts.xlsx
Table1
Date
Transaction date
Merchant name
Merchant name
Amount
Total >
Show advanced options
1 Note
Receipt values are returned as strings. To manipulate them as numbers, you can use
the float or int conversion functions.
Congratulations! You've created a flow that uses the AI Builder receipt processing
model. Select Save on the top right, and then select Test to try out your flow.
Page range
For large documents, it's possible to specify the page range to process.

|  | Manually trigger a flow |  | ? ... |
| --- | --- | --- | --- |
|  |  |  |  |
|  | Extract information from receipts |  | ? ... |
|  |  |  |  |
| x | Add a row into a table |  | ? ... |
| * Location OneDrive for Business |  |  |  |
|  |  |  |  |
|  |  |  |  |
| * | Document Library | OneDrive |  |
| * File |  |  |  |
|  |  | /Receipts.xlsx |  |
|  |  |  |  |
| * Table |  | Table1 |  |
|  |  |  |  |
| Date |  | Transaction date :selected: |  |
|  |  |  |  |
| Merchant name |  | Merchant name :selected: |  |
|  |  |  |  |
| Amount |  | Total > :selected: |  |
| Show advanced options |  |  |  |


<!---- Page 572 ---------------------------------------------------------------------------------------------------------------------------------->
Manually trigger a flow
?
...
Extract information from receipts
?
...
* Receipt file
S
My receipt x
Pages
Enter page range (Ex: 1 or 3-5). Only returns results of a single receipt.
Hide advanced options
You can enter a page value or page range in the Pages parameter. Example: 1 or 3-5.
1 Note
If you have a large document with only one receipt, we strongly recommend to use
the Pages parameter to aim at your receipt, and therefore reduce the cost of model
prediction and increase performance. However, note that only the data of the first
receipt within the page range will be returned and that multi-page receipts are not
supported.
Example: A document contains a first receipt in page 2 and a second receipt that
spans overs page 3 and 4:
. If you enter value 2, it will return the data of the first receipt
. If you enter value 3-4, it will only return the data of the first page of the
second receipt
. If you enter value 2-4, it will only return data of the first receipt, not the data
of the second receipt
Parameters
Input
Expand table
Name
Required
Type
Description
Receipt file
Yes
string
The receipt file to process

| Name | Required | Type | Description |
| --- | --- | --- | --- |
| Receipt file | Yes | string | The receipt file to process |


<!---- Page 573 ---------------------------------------------------------------------------------------------------------------------------------->
Name
Required
Type
Description
Pages
No
string
Page range to process
Output
[] Expand table
Name
Type
Description
Merchant name
string
Merchant name
Merchant address
string
Merchant address
Merchant phone number
string
Merchant phone number
Transaction date
string
Transaction date
Transaction time
string
Transaction time
Purchased item name
string
Purchased item name. Returned as a part of a list of
items.
Purchased item quantity
string
Purchased item quantity. Returned as a part of a list of
items.
Purchased item price
string
Purchased item price. Returned as a part of a list of
items.
Purchased item total price
string
Purchased item total price. Returned as a part of a list of
items.
Subtotal
string
Subtotal
Tax
string
Tax
Tip
string
Tip
Total
string
Total
Confidence of merchant
name
float
How confident the model is in its detection
Confidence of merchant
address
float
How confident the model is in its detection
Confidence of merchant
phone number
float
How confident the model is in its detection
Confidence of transaction
float
How confident the model is in its detection

| Name | Required | Type | Description |
| --- | --- | --- | --- |
| Pages | No | string | Page range to process |

| Name | Type | Description |
| --- | --- | --- |
| Merchant name | string | Merchant name |
| Merchant address | string | Merchant address |
| Merchant phone number | string | Merchant phone number |
| Transaction date | string | Transaction date |
| Transaction time | string | Transaction time |
| Purchased item name | string | Purchased item name. Returned as a part of a list of items. |
| Purchased item quantity | string | Purchased item quantity. Returned as a part of a list of items. |
| Purchased item price | string | Purchased item price. Returned as a part of a list of items. |
| Purchased item total price | string | Purchased item total price. Returned as a part of a list of items. |
| Subtotal | string | Subtotal |
| Tax | string | Tax |
| Tip | string | Tip |
| Total | string | Total |
| Confidence of merchant name | float | How confident the model is in its detection |
| Confidence of merchant address | float | How confident the model is in its detection |
| Confidence of merchant phone number | float | How confident the model is in its detection |
| Confidence of transaction | float | How confident the model is in its detection |


<!---- Page 574 ---------------------------------------------------------------------------------------------------------------------------------->
Name
Type
Description
date
Confidence of transaction
time
float
How confident the model is in its detection
Confidence of purchased
item name
float
How confident the model is in its detection. Returned as
a part of a list of items.
Confidence of purchased
item quantity
float
How confident the model is in its detection. Returned as
a part of a list of items.
Confidence of purchased
item price
float
How confident the model is in its detection. Returned as
a part of a list of items.
Confidence of purchased
item total price
float
How confident the model is in its detection. Returned as
a part of a list of items.
Confidence of subtotal
float
How confident the model is in its detection
Confidence of tax
float
How confident the model is in its detection
Confidence of tip
float
How confident the model is in its detection
Confidence of total
float
How confident the model is in its detection
Detected text
string
Line of recognized text. Returned as a part of a list of
text.
Page number of detected
text
integer
Which page the line of recognized text is found on.
Returned as a part of a list of text.
Height of detected text
float
Height of the line of text. Returned as a part of a list of
text.
Left position of detected
text
float
Left position of the line of text. Returned as a part of a
list of text.
Top position of detected text
float
Top position of the line of text. Returned as a part of a
list of text.
Width of detected text
float
Width of the line of text. Returned as a part of a list of
text.
Related information
· Receipt processing overview
· Training: Process receipts with Al Builder (module)

| Name | Type | Description |
| --- | --- | --- |
| date |  |  |
| Confidence of transaction time | float | How confident the model is in its detection |
| Confidence of purchased item name | float | How confident the model is in its detection. Returned as a part of a list of items. |
| Confidence of purchased item quantity | float | How confident the model is in its detection. Returned as a part of a list of items. |
| Confidence of purchased item price | float | How confident the model is in its detection. Returned as a part of a list of items. |
| Confidence of purchased item total price | float | How confident the model is in its detection. Returned as a part of a list of items. |
| Confidence of subtotal | float | How confident the model is in its detection |
| Confidence of tax | float | How confident the model is in its detection |
| Confidence of tip | float | How confident the model is in its detection |
| Confidence of total | float | How confident the model is in its detection |
| Detected text | string | Line of recognized text. Returned as a part of a list of text. |
| Page number of detected text | integer | Which page the line of recognized text is found on. Returned as a part of a list of text. |
| Height of detected text | float | Height of the line of text. Returned as a part of a list of text. |
| Left position of detected text | float | Left position of the line of text. Returned as a part of a list of text. |
| Top position of detected text | float | Top position of the line of text. Returned as a part of a list of text. |
| Width of detected text | float | Width of the line of text. Returned as a part of a list of text. |


<!---- Page 575 ---------------------------------------------------------------------------------------------------------------------------------->
Feedback
Was this page helpful?
Yes
P No
Provide product feedback


<!---- Page 576 ---------------------------------------------------------------------------------------------------------------------------------->
Use the ID reader prebuilt model in
Power Automate
Article · 01/27/2025
1. Sign in to Power Automate .
2. In the left pane, select My flows, and then select New flow > Instant cloud flow in
the menu at the top.
3. Name your flow, select Manually trigger a flow under Choose how to trigger this
flow, and then select Create.
4. Expand Manually trigger a flow, and then select +Add an input > File as the input
type.
5. Select +New step > AI Builder > Extract information from identity documents.
6. Specify File Content as the identity document file you want to process in your flow.
Manually trigger a flow
?
...
File Content
Please select file or image
...
+ Add an input
B
Extract information from identity documents
?
...
* Identity document file
I
File Content x
Congratulations! You've created a flow that uses the ID reader model. Select Save, and
then select Test in the upper-right corner to try out your flow.
Example flow that adds extracted information
to an Excel worksheet
In the following example, you'll add steps to your flow to enter the extracted
information in an Excel worksheet. First, you'll prepare a table to use in your flow. The


<!---- Page 577 ---------------------------------------------------------------------------------------------------------------------------------->
table must match the information you want to extract. Then you'll add an Excel
connector to your flow.
Create an Excel table
1. Create an Excel workbook in a Microsoft OneDrive or SharePoint folder.
2. In the first row of the worksheet, enter the following values, one to a column: First
name, Last name, Identity document number, and Country. These values are the
column headers for your table.
3. Select the cells and format them as a table, with the first row as the header.
A
B
C
D
1
First name
Last name
Identity document number
Country
2
3
4. Save and close the workbook.
Enter the extracted data in the table
1. Use the ID reader flow you created, or create another one for this example.
2. Select +New step > Excel Online (Business) > Add a row into a table.
3. Select a Location, Document Library, and File to specify where to find your Excel
workbook.
4. Select the Table that you created in the previous step.
5. In First name, Last name, and Identity document number, select the matching
value in the dynamic content list.
6. In Country, select Country/Region in the dynamic content list.


<!---- Page 578 ---------------------------------------------------------------------------------------------------------------------------------->
Add a row into a table
?
. ..
* Location
OneDrive for Business
* Document Library
OneDrive
* File
/ID Documents/ID Documents.xlsx
* Table
Table1
First name
First name x
Last name
Last name x
Identity document
number
Identity docum ...
Country
Country/Region
Show advanced options
7. Select Save.
Test the flow
1. Select Test, select Manually, and then select Test to trigger the action.
2. In File Content, select an identity document file or image, and then select Import.
File Content
Please select file or image
Import
3. Select Run Flow.
The flow may take a few seconds to execute while AI Builder extracts the data and adds
a new entry to the table in Excel. Open your Excel workbook to confirm the extracted
information has been entered.
A
B
C
D
1
First name
Last name
Identity document number
Country
2
JENNIFER
BROOKS
340020013 USA
3

|  | A | B | C | D |
| --- | --- | --- | --- | --- |
| 1 | First name | Last name | Identity document number | Country |
| 2 | JENNIFER | BROOKS | 340020013 | USA |
| 3 |  |  |  |  |


<!---- Page 579 ---------------------------------------------------------------------------------------------------------------------------------->
Example flow that sends extracted information
in an email
The following example shows how to set up a flow to send the extracted information in
an email. You can add the Send an email notification connector to the flow you created
earlier or create an ID reader flow for this example.
Manually trigger a flow
?
...
Extract information from identity documents
?
...
Send an email notification (V3)
?
...
* To
*Subject
olivia@contoso.com
Passport processed
* Body
Font
V 14 B I U / E QQ <>
A passport has been processed:
First name:
T
First name x
Last name:
F
Last name x
Identity document number:
F
Identity document number x
Country:
country
Related information
ID reader prebuilt model overview
Feedback
Was this page helpful?
Yes
No

|  | Manually trigger a flow |  | ? ... |  |
| --- | --- | --- | --- | --- |
|  |  |  |  |  |
|  | Extract information from identity documents ? ... |  |  |  |
|  |  |  |  |  |
|  | Send an email notification (V3) |  | ? ... |  |
| * To *Subject |  |  |  |  |
|  |  | olivia@contoso.com |  |  |
|  |  |  |  |  |
|  |  | Passport processed |  |  |
|  |  |  |  |  |
| * Body |  | Font V 14 B I U / E QQ <> |  |  |
|  |  | A passport has been processed: |  |  |
|  |  | First name: T First name x :selected: |  |  |
|  |  | Last name: F Last name x :selected: |  |  |
|  |  | Identity document number: F Identity document number x :selected: |  |  |
|  |  | Country: country :selected: |  |  |


<!---- Page 580 ---------------------------------------------------------------------------------------------------------------------------------->
Provide product feedback


<!---- Page 581 ---------------------------------------------------------------------------------------------------------------------------------->
Use the business card reader prebuilt
model in Power Automate
Article · 01/27/2025
1. Sign in to Power Automate .
2. Select My flows in the left pane, and then select New flow > Instant cloud flow.
3. Name your flow, select Manually trigger a flow under Choose how to trigger this
flow, and then select Create.
4. Expand Manually trigger a flow, and then select +Add an input > File as the input
type.
5. Replace File Content with My image (also known as the title).
6. Select + New step > AI Builder, and then select Read business card information in
the list of actions.
7. Specify My Image from the trigger in the Business card input for your flow.
8. Select Show advanced options and verify that Detect automatically is in the
Image type input.
Manually trigger a flow
?
...
0
My image
Please select file or image
...
+ Add an input
Read business card information
?
...
* Business card
G
My image x
* Image type
Detect automatically
V
Hide advanced options
Congratulations! You've created a flow that uses the business card reader AI model.
Select Save, and then select Test in the upper-right cornerto try out your flow.


<!---- Page 582 ---------------------------------------------------------------------------------------------------------------------------------->
Example business card reader flow
The following example shows a new contact being created in Microsoft Dataverse using
the business card data.
To add the Add a new row step, select + New step > Microsoft Dataverse > Add a new
row.
Manually trigger a flow
?
...
Read business card information
?
...
Add a new row
?
...
* Table name
Contacts
* Last Name
Last name x
Address 1: City
City x
Address 1: Street 1
Full address x
Address 1: Street 2
Type the second line of the primary address.
Address 1: ZIP/Postal
Code
Postal Code x
Business Phone
Work phone a ...
Company Name
(Accounts)
Company Name x
Company Name
(Contacts)
Select the parent account or parent contact for the contact to provide a quick li
Description
Type additional information to describe the contact, such as an excerpt from the
Email
Type the primary email address for the contact.
First Name
Full name x
Job Title
Title x
Mobile Phone
Mobile phone x
Parameters

|  | Manually trigger a flow |  | ? ... |  |
| --- | --- | --- | --- | --- |
|  |  |  |  |  |
|  | Read business card information |  | ? ... |  |
|  |  |  |  |  |
|  | Add a new row |  | ? ... |  |
| * Table name |  |  |  |  |
|  |  | Contacts |  |  |
|  |  |  |  |  |
| * Last Name |  | Last name x :selected: |  |  |
| Address 1: City |  |  |  |  |
|  |  | City x :selected: |  |  |
| Address 1: Street 1 |  |  |  |  |
|  |  | Full address x :selected: |  |  |
|  |  |  |  |  |
| Address 1: Street 2 |  | Type the second line of the primary address. |  |  |
|  |  |  |  |  |
| Address 1: ZIP/Postal Code |  | Postal Code x :selected: |  |  |
|  |  |  |  |  |
| Business Phone |  | Work phone a ... :selected: |  |  |
|  |  |  |  |  |
| Company Name (Accounts) |  | Company Name x :selected: |  |  |
|  |  |  |  |  |
| Company Name (Contacts) |  | Select the parent account or parent contact for the contact to provide a quick li |  |  |
|  |  |  |  |  |
| Description |  | Type additional information to describe the contact, such as an excerpt from the |  |  |
|  |  |  |  |  |
| Email |  | Type the primary email address for the contact. |  |  |
|  |  |  |  |  |
| First Name |  | Full name x :selected: |  |  |
|  |  |  |  |  |
| Job Title |  | Title x :selected: |  |  |
| Mobile Phone |  |  |  |  |
|  |  | Mobile phone x :selected: |  |  |


<!---- Page 583 ---------------------------------------------------------------------------------------------------------------------------------->
Input
0
Expand table
Name
Required
Type
Description
Values
Image
type
Yes
string
Mime type of the
image
"auto" as default value. This column being
obsolete, any value will be accepted.
Image
Yes
file
Image file to
analyze
Output
0
Expand table
Name
Type
Description
City
string
The city address
Country
string
The country address
Postal Code
string
The postal code address
PO Box
string
The post office box address
State
string
The state address
Street
string
The street address
Work phone or other phone
string
The first phone or fax number
Company name
string
The company name
Department
string
The organization department found
Email
string
The contact email found in the business card, if any
Fax
string
The third phone or fax number
First name
string
The contact first name
Full address
string
The contact full address
Full name
string
The contact full name
Title
string
The contact job title
Last name
string
The contact last name

| Name | Required | Type | Description | Values |
| --- | --- | --- | --- | --- |
| Image type | Yes | string | Mime type of the image | "auto" as default value. This column being obsolete, any value will be accepted. |
| Image | Yes | file | Image file to analyze |  |

| Name | Type | Description |
| --- | --- | --- |
| City | string | The city address |
| Country | string | The country address |
| Postal Code | string | The postal code address |
| PO Box | string | The post office box address |
| State | string | The state address |
| Street | string | The street address |
| Work phone or other phone | string | The first phone or fax number |
| Company name | string | The company name |
| Department | string | The organization department found |
| Email | string | The contact email found in the business card, if any |
| Fax | string | The third phone or fax number |
| First name | string | The contact first name |
| Full address | string | The contact full address |
| Full name | string | The contact full name |
| Title | string | The contact job title |
| Last name | string | The contact last name |


<!---- Page 584 ---------------------------------------------------------------------------------------------------------------------------------->
Name
Type
Description
Mobile phone
string
The second phone or fax number
Website
string
The website
Related information
Business card reader overview
Feedback
Was this page helpful?
Yes
P
No
Provide product feedback

| Name | Type | Description |
| --- | --- | --- |
| Mobile phone | string | The second phone or fax number |
| Website | string | The website |


<!---- Page 585 ---------------------------------------------------------------------------------------------------------------------------------->
Tutorial: Use Azure Functions and Python
to process stored documents
Article · 03/19/2025
Document Intelligence can be used as part of an automated data processing pipeline built with
Azure Functions. This guide will show you how to use Azure Functions to process documents
that are uploaded to an Azure blob storage container. This workflow extracts table data from
stored documents using the Document Intelligence layout model and saves the table data in a
.csv file in Azure. You can then display the data using Microsoft Power BI (not covered here).
Form Recognizer
Layout API
Extract tables
Trigger
Save table data
Visualize
Blob storage
Azure Function
Azure Table
PowerBI
In this tutorial, you learn how to:
V Create an Azure Storage account.
V Create an Azure Functions project.
V Extract layout data from uploaded forms.
V Upload extracted layout data to Azure Storage.
Prerequisites
· Azure subscription - Create one for free
· A Document Intelligence resource. Once you have your Azure subscription, create a
Document Intelligence resource in the Azure portal to get your key and endpoint. You
can use the free pricing tier (F0) to try the service, and upgrade later to a paid tier for
production.
o After your resource deploys, select Go to resource. You need the key and endpoint
from the resource you create to connect your application to the Document Intelligence
API. You'll paste your key and endpoint into the code below later in the tutorial:


<!---- Page 586 ---------------------------------------------------------------------------------------------------------------------------------->
Home > Contoso-DI
Contoso-DI | Keys and Endpoint
X
Search
«
Regenerate Key1
Regenerate Key2
Overview
Activity log
Access control (IAM)
O
These keys are used to access your Azure Al service API. Do not share your keys. Store them securely- for
example, using Azure Key Vault. We also recommend regenerating these keys regularly. Only one key is necessary
to make an API call. When regenerating the first key, you can use the second key for continued access to the
service.
Tags
* Diagnose and solve problems
V Resource Management
Show Keys
Keys and Endpoint
KEY 1
P
Encryption
KEY 2
Pricing tier
<> Networking
Location/Region @
Identity
westus2
D
Cost analysis
Endpoint
Properties
https://contoso-di.cognitiveservices.azure.com/
D
Locks
Monitoring
Automation
ho
Help
· Python 3.6.x, 3.7.x, 3.8.x or 3.9.x (Python 3.10.x isn't supported for this project).
. The latest version of Visual Studio Code (VS Code) with the following extensions
installed:
o Azure Functions extension . Once it's installed, you should see the Azure logo in the
left-navigation pane.
o Azure Functions Core Tools version 3.x (Version 4.x isn't supported for this project).
o Python Extension for Visual Studio code. For more information, see Getting Started
with Python in VS Code Z
. Azure Storage Explorer installed.
. A local PDF document to analyze. You can use our sample pdf document [ for this
project.
Create an Azure Storage account
1. Create a general-purpose v2 Azure Storage account in the Azure portal. If you don't
know how to create an Azure storage account with a storage container, follow these
quickstarts:
. Create a storage account. When you create your storage account, select Standard
performance in the Instance details > Performance field.


<!---- Page 587 ---------------------------------------------------------------------------------------------------------------------------------->
. Create a container. When you create your container, set Public access level to
Container (anonymous read access for containers and files) in the New Container
window.
2. On the left pane, select the Resource sharing (CORS) tab, and remove the existing CORS
policy if any exists.
3. Once your storage account has deployed, create two empty blob storage containers,
named input and output.
Create an Azure Functions project
1. Create a new folder named functions-app to contain the project and choose Select.
2. Open Visual Studio Code and open the Command Palette (Ctrl+Shift+P). Search for and
choose Python:Select Interpreter -> choose an installed Python interpreter that is version
3.6.x, 3.7.x, 3.8.x or 3.9.x. This selection will add the Python interpreter path you selected
to your project.
3. Select the Azure logo from the left-navigation pane.
. You'll see your existing Azure resources in the Resources view.
· Select the Azure subscription that you're using for this project and below you should
see the Azure Function App.


<!---- Page 588 ---------------------------------------------------------------------------------------------------------------------------------->
AZURE
V RESOURCES Remote
V
Your Azure Subscription
>
App Services
>
Azure Cosmos DB
3
>
Function App
>
ET
PostgreSQL servers (Flexible)
>
CT
PostgreSQL servers (Standard)
>
</>
Static Web Apps
>
Storage accounts
>
Virtual machines
¢
4. Select the Workspace (Local) section located below your listed resources. Select the plus
symbol and choose the Create Function button.
AZURE
...
RESOURCES
V WORKSPACE Local
+ CO
Create Function ...
5. When prompted, choose Create new project and navigate to the function-app directory.
Choose Select.
6. You'll be prompted to configure several settings:
. Select a language - choose Python.
. Select a Python interpreter to create a virtual environment -> select the interpreter
you set as the default earlier.
. Select a template - choose Azure Blob Storage trigger and give the trigger a
name or accept the default name. Press Enter to confirm.
. Select setting -> choose +Create new local app setting from the dropdown menu.


<!---- Page 589 ---------------------------------------------------------------------------------------------------------------------------------->
· Select subscription - choose your Azure subscription with the storage account you
created -> select your storage account - then select the name of the storage input
container (in this case, input/{name} ). Press Enter to confirm.
. Select how your would like to open your project - choose Open the project in the
current window from the dropdown menu.
7. Once you've completed these steps, VS Code will add a new Azure Function project with a
_init _. py Python script. This script will be triggered when a file is uploaded to the input
storage container:
Python
import logging
import azure.functions as func
def main(myblob: func.InputStream) :
logging. info(f"Python blob trigger function processed blob \n"
f"Name: {myblob.name}\n"
f"Blob Size: {myblob. length} bytes")
Test the function
1. Press F5 to run the basic function. VS Code will prompt you to select a storage account to
interface with.
2. Select the storage account you created and continue.
3. Open Azure Storage Explorer and upload the sample PDF document to the input
container. Then check the VS Code terminal. The script should log that it was triggered by
the PDF upload.


<!---- Page 590 ---------------------------------------------------------------------------------------------------------------------------------->
OUTPUT
TERMINAL
DEBUG CONSOLE
...
1: Task - host
+00 ^ x
[19-03-2020 13:26:48] Executing 'Functions. BlobTrigger1' (Reason='New bl
ob detected: test/1025872, batch J5UAC91.pdf', Id=ba6e1371-be13-4f7f-8b9
9-094659600047)
[19-03-2020 13:26:48] INFO: Received FunctionInvocationRequest, request
ID: 75e17627-bb9c-4506-baf2-7eec06b557e0, function ID: 9a00bc1f-ac2a-42
a4-80fa-fda26ec6b29f, invocation ID: ba6e1371-be13-4f7f-8b99-09465960004
7
[19-03-2020 13:26:48] Python blob trigger function processed blob
Name: test/1025872, batch J5UAC91.pdf
Blob Size: 626862 bytes
[19-03-2020 13:26:48] INFO: Successfully processed FunctionInvocationRe
quest, request ID: 75e17627-bb9c-4506-baf2-7eec06b557e0, function ID: 9a
00bc1f-ac2a-42a4-80fa-fda26ec6b29f, invocation ID: ba6e1371-be13-4f7f-8b
99-094659600047
[19-03-2020 13:26:48] Executed 'Functions. BlobTrigger1' (Succeeded, Id=b
a6e1371-be13-4f7f-8b99-094659600047)
4. Stop the script before continuing.
Add document processing code
Next, you'll add your own code to the Python script to call the Document Intelligence service
and parse the uploaded documents using the Document Intelligence layout model.
1. In VS Code, navigate to the function's requirements.txt file. This file defines the
dependencies for your script. Add the following Python packages to the file:
txt
cryptography
azure-functions
azure-storage-blob
azure-identity
requests
pandas
numpy
2. Then, open the_init _. py script. Add the following import statements:
Python
import logging
from azure.storage.blob import BlobServiceClient
import azure. functions as func
import json
import time
from requests import get, post


<!---- Page 591 ---------------------------------------------------------------------------------------------------------------------------------->
import os
import requests
from collections import OrderedDict
import numpy as np
import pandas as pd
3. You can leave the generated main function as-is. You'll add your custom code inside this
function.
Python
# This part is automatically generated
def main(myblob: func.InputStream) :
logging. info(f"Python blob trigger function processed blob \n"
f"Name: {myblob.name}\n"
f"Blob Size: {myblob.length} bytes")
4. The following code block calls the Document Intelligence Analyze Layout API on the
uploaded document. Fill in your endpoint and key values.
Python
# This is the call to the Document Intelligence endpoint
endpoint = r"Your Document Intelligence Endpoint"
apim_key = "Your Document Intelligence Key"
post_url = endpoint + "/formrecognizer/v2.1/layout/analyze"
source = myblob.read()
headers = {
# Request headers
'Content-Type' : 'application/pdf',
'Ocp-Apim-Subscription-Key': apim_key,
}
text1=os.path.basename(myblob.name)
1 Important
Remember to remove the key from your code when you're done, and never post it
publicly. For production, use a secure way of storing and accessing your credentials
like Azure Key Vault. For more information, see Azure AI services security.
5. Next, add code to query the service and get the returned data.
Python


<!---- Page 592 ---------------------------------------------------------------------------------------------------------------------------------->
resp = requests.post(url=post_url, data=source, headers=headers)
if resp.status_code != 202:
print("POST analyze failed: \n%s" % resp. text)
quit()
print("POST analyze succeeded: \n%s" % resp.headers)
get_url = resp. headers["operation-location"]
wait_sec = 25
time.sleep(wait_sec)
# The layout API is async therefore the wait statement
resp = requests. get(url=get_url, headers={"Ocp-Apim-Subscription-Key":
apim_key})
resp_json = json. loads (resp. text)
status = resp_json["status"]
if status == "succeeded":
print("POST Layout Analysis succeeded: \n%s")
results = resp_json
else:
print("GET Layout results failed: \n%s")
quit()
results = resp_json
6. Add the following code to connect to the Azure Storage output container. Fill in your
own values for the storage account name and key. You can get the key on the Access
keys tab of your storage resource in the Azure portal.
Python
# This is the connection to the blob storage, with the Azure Python SDK
blob_service_client =
BlobServiceClient. from_connection_string("DefaultEndpointsProtocol=https; Acco
untName="Storage Account Name"; AccountKey="storage account
key";EndpointSuffix=core. windows. net")
container_client=blob_service_client. get_container_client("output")
The following code parses the returned Document Intelligence response, constructs a .csv
file, and uploads it to the output container.
1 Important


<!---- Page 593 ---------------------------------------------------------------------------------------------------------------------------------->
You will likely need to edit this code to match the structure of your own documents.
Python
# The code below extracts the json format into tabular data.
# Please note that you need to adjust the code below to your form
structure.
# It probably won't work out-of-the-box for your specific form.
pages = results["analyzeResult"][ "pageResults"]
def make_page (p):
res=[]
res_table=[]
y=0
page = pages [p]
for tab in page["tables"]:
for cell in tab["cells"]:
res. append (cell)
res_table. append(y)
y=y+1
res_table=pd. DataFrame (res_table)
res=pd. DataFrame(res)
res["table_num"]=res_table[0]
h=res.drop(columns=["boundingBox","elements"])
h.loc[: , "rownum"]=range(0, len(h))
num_table=max(h["table_num"])
return h, num_table, p
h, num_table, p= make_page(0)
for k in range(num_table+1):
new_table=h[h.table_num == k]
new_table. loc[ : , "rownum"]=range(0,len(new_table))
row_table=pages [p]["tables"][k]["rows"]
col_table=pages [p]["tables"][k]["columns"]
b=np.zeros((row_table,col_table))
b=pd.DataFrame(b)
s=0
for i,j in zip(new_table["rowIndex"],new_table["columnIndex"]):
b. loc[i, j]=new_table. loc[new_table. loc[s, "rownum"],"text"]
S=S+1
7. Finally, the last block of code uploads the extracted table and text data to your blob
storage element.
Python
# Here is the upload to the blob storage
tab1_csv=b.to_csv(header=False, index=False, mode='w')


<!---- Page 594 ---------------------------------------------------------------------------------------------------------------------------------->
name1=(os.path.splitext(text1)[0]) +'.csv'
container_client. upload_blob(name=name1, data=tab1_csv)
Run the function
1. Press F5 to run the function again.
2. Use Azure Storage Explorer to upload a sample PDF form to the input storage container.
This action should trigger the script to run, and you should then see the resulting .csv file
(displayed as a table) in the output container.
You can connect this container to Power BI to create rich visualizations of the data it contains.
Next steps
In this tutorial, you learned how to use an Azure Function written in Python to automatically
process uploaded PDF documents and output their contents in a more data-friendly format.
Next, learn how to use Power BI to display the data.
Microsoft Power BI
· What is Document Intelligence?
· Learn more about the layout model


<!---- Page 595 ---------------------------------------------------------------------------------------------------------------------------------->
Document Models - Analyze Document
Reference
Service: Azure AI Services
API Version: 2024-11-30
Analyzes document with document model.
HTTP
POST {endpoint}/documentintelligence/documentModels/{modelId} : analyze?
_overload=analyzeDocument&api-version=2024-11-30
With optional parameters:
HTTP
POST {endpoint}/documentintelligence/documentModels/{modelId} :analyze?
_overload=analyzeDocument&api-version=2024-11-30&pages={pages}&locale=
{locale}&stringIndexType={stringIndexType}&features={features}&queryFields=
{queryFields}&outputContentFormat={outputContentFormat}&output={output}
URI Parameters
[ Expand table
Name
In
Required
Type
Description
endpoint
path
True
string (uri)
The Document Intelligence service endpoint.
modelId
path
True
string
maxLength: 64
pattern: ^[a-zA-
Z0-9][a-zA-Z0-
9 ._~- }{1,63}$
Unique document model name.
api-
version
query
True
string
minLength: 1
The API version to use for this operation.
features
query
Document
Analysis
List of optional analysis features.
Feature[]
locale
query
string
Locale hint for text recognition and document
analysis. Value may contain only the language

| Name | In | Required | Type | Description |
| --- | --- | --- | --- | --- |
| endpoint | path | True | string (uri) | The Document Intelligence service endpoint. |
| modelId | path | True | string maxLength: 64 pattern: ^[a-zA- Z0-9][a-zA-Z0- 9 ._~- }{1,63}$ | Unique document model name. |
| api- version | query | True | string minLength: 1 | The API version to use for this operation. |
| features | query |  | Document Analysis | List of optional analysis features. |
|  |  |  | Feature[] |  |
| locale | query |  | string | Locale hint for text recognition and document analysis. Value may contain only the language |


<!---- Page 596 ---------------------------------------------------------------------------------------------------------------------------------->
code (ex. "en", "fr") or BCP 47 language tag (ex.
"en-US").
output
query
AnalyzeOutput
Option[]
Additional outputs to generate during analysis.
output
Content
Format
query
Document
ContentFormat
Format of the analyze result top-level content.
pages
query
string
pattern: ^(\d+(-
\d+)?)(\s*(\d+(-
\d+)?))*$
1-based page numbers to analyze. Ex. "1-3,5,7-9"
query
Fields
query
string[]
List of additional fields to extract. Ex.
"NumberOfGuests,StoreNumber"
string
IndexType
query
StringIndexType
Method used to compute string offset and length.
Request Body
0
Expand table
Name
Type
Description
base64Source
string
(byte)
Base64 encoding of the document to analyze. Either urlSource or
base64Source must be specified.
urlSource
string (uri)
Document URL to analyze. Either urlSource or base64Source must be
specified.
Responses
0
Expand table
Name
Type
Description
202
The request has been accepted for processing, but processing has not
Accepted
yet completed.
Headers
· Operation-Location: string
· Retry-After: integer

| output | query | AnalyzeOutput Option[] | Additional outputs to generate during analysis. |
| --- | --- | --- | --- |
| output Content Format | query | Document ContentFormat | Format of the analyze result top-level content. |
| pages | query | string pattern: ^(\d+(- \d+)?)(\s*(\d+(- \d+)?))*$ | 1-based page numbers to analyze. Ex. "1-3,5,7-9" |
| query Fields | query | string[] | List of additional fields to extract. Ex. "NumberOfGuests,StoreNumber" |
| string IndexType | query | StringIndexType | Method used to compute string offset and length. |

| Name | Type | Description |
| --- | --- | --- |
| base64Source | string (byte) | Base64 encoding of the document to analyze. Either urlSource or base64Source must be specified. |
| urlSource | string (uri) | Document URL to analyze. Either urlSource or base64Source must be specified. |

| Name | Type | Description |
| --- | --- | --- |
| 202 |  | The request has been accepted for processing, but processing has not |
| Accepted |  | yet completed. Headers · Operation-Location: string · Retry-After: integer |


<!---- Page 597 ---------------------------------------------------------------------------------------------------------------------------------->
Other Status
Document
An unexpected error response.
Codes
Intelligence
Error
Response
Security
Ocp-Apim-Subscription-Key
Type: apiKey
In: header
OAuth2Auth
Type: oauth2
Flow: accessCode
Authorization URL: https://login.microsoftonline.com/common/oauth2/authorize
Token URL: https://login.microsoftonline.com/common/oauth2/token
Scopes
0
] Expand table
Name
Description
https://cognitiveservices.azure.com/.default
Examples
[ Expand table
Analyze Document from Base64
Analyze Document from Url
Analyze Document from Base64
Sample request

| Other Status | Document | An unexpected error response. |
| --- | --- | --- |
| Codes | Intelligence |  |
|  | Error |  |
|  | Response |  |

| Name | Description |
| --- | --- |
| https://cognitiveservices.azure.com/.default |  |


<!---- Page 598 ---------------------------------------------------------------------------------------------------------------------------------->
HTTP
HTTP
POST
https://myendpoint.cognitiveservices.azure.com/documentintelligence/documentMo
dels/prebuilt-layout : analyze ?_ overload=analyzeDocument&api-version=2024-11-
30&pages=1-2,4&locale=en-US&stringIndexType=textElements
{
"base64Source": "e2Jhc2U2NEVuY29kZWRQZGZ9"
}
Sample response
Status code: 202
HTTP
Operation-Location:
https://myendpoint.cognitiveservices.azure.com/documentintelligence/documentModels
/prebuilt-layout/analyzeResults/3b31320d-8bab-4f88-b19c-2322a7f11034?api-
version=2024-11-30
Analyze Document from Url
Sample request
HTTP
HTTP
POST
https://myendpoint.cognitiveservices.azure.com/documentintelligence/documentMo
dels/customModel:analyze ?_ overload=analyzeDocument&api-version=2024-11-
30&pages=1-2,4&locale=en-US&stringIndexType=textElements
{
"urlSource": "http://host.com/doc.pdf"
}
Sample response


<!---- Page 599 ---------------------------------------------------------------------------------------------------------------------------------->
Status code: 202
HTTP
Operation-Location:
https://myendpoint.cognitiveservices.azure.com/documentintelligence/documentModels
/customModel/analyzeResults/3b31320d-8bab-4f88-b19c-2322a7f11034?api-version=2024-
11-30
Definitions
0
Expand table
Name
Description
AnalyzeDocumentRequest
Document analysis parameters.
AnalyzeOutputOption
Additional outputs to generate during analysis.
DocumentAnalysisFeature
Document analysis features to enable.
DocumentContentFormat
Format of the content in analyzed result.
DocumentIntelligenceError
The error object.
DocumentIntelligenceErrorResponse
Error response object.
DocumentIntelligenceInnerError
An object containing more specific information about the error.
StringIndexType
Method used to compute string offset and length.
AnalyzeDocumentRequest
Object
Document analysis parameters.
0
Expand table
Name
Type
Description
base64Source
string (byte)
Base64 encoding of the document to analyze. Either urlSource or
base64Source must be specified.
urlSource
string (uri)
Document URL to analyze. Either urlSource or base64Source must be
specified.

| Name | Description |
| --- | --- |
| AnalyzeDocumentRequest | Document analysis parameters. |
| AnalyzeOutputOption | Additional outputs to generate during analysis. |
| DocumentAnalysisFeature | Document analysis features to enable. |
| DocumentContentFormat | Format of the content in analyzed result. |
| DocumentIntelligenceError | The error object. |
| DocumentIntelligenceErrorResponse | Error response object. |
| DocumentIntelligenceInnerError | An object containing more specific information about the error. |
| StringIndexType | Method used to compute string offset and length. |

| Name | Type | Description |
| --- | --- | --- |
| base64Source | string (byte) | Base64 encoding of the document to analyze. Either urlSource or base64Source must be specified. |
| urlSource | string (uri) | Document URL to analyze. Either urlSource or base64Source must be specified. |


<!---- Page 600 ---------------------------------------------------------------------------------------------------------------------------------->
AnalyzeOutputOption
Enumeration
Additional outputs to generate during analysis.
[] Expand table
Value
Description
figures
Generate cropped images of detected figures.
pdf
Generate searchable PDF output.
DocumentAnalysisFeature
Enumeration
Document analysis features to enable.
@ Expand table
Value
Description
barcodes
Enable the detection of barcodes in the document.
formulas
Enable the detection of mathematical expressions in the document.
keyValuePairs
Enable the detection of general key value pairs (form fields) in the document.
languages
Enable the detection of the text content language.
ocrHighResolution
Perform OCR at a higher resolution to handle documents with fine print.
queryFields
Enable the extraction of additional fields via the queryFields query parameter.
styleFont
Enable the recognition of various font styles.
DocumentContentFormat
Enumeration
Format of the content in analyzed result.
[ Expand table
Value
Description
markdown
Markdown representation of the document content with section headings, tables, etc.

| Value | Description |
| --- | --- |
| figures | Generate cropped images of detected figures. |
| pdf | Generate searchable PDF output. |

| Value | Description |
| --- | --- |
| barcodes | Enable the detection of barcodes in the document. |
| formulas | Enable the detection of mathematical expressions in the document. |
| keyValuePairs | Enable the detection of general key value pairs (form fields) in the document. |
| languages | Enable the detection of the text content language. |
| ocrHighResolution | Perform OCR at a higher resolution to handle documents with fine print. |
| queryFields | Enable the extraction of additional fields via the queryFields query parameter. |
| styleFont | Enable the recognition of various font styles. |

| Value | Description |
| --- | --- |
| markdown | Markdown representation of the document content with section headings, tables, etc. |


<!---- Page 601 ---------------------------------------------------------------------------------------------------------------------------------->
text
Plain text representation of the document content without any formatting.
DocumentIntelligenceError
Object
The error object.
Expand table
Name
Type
Description
code
string
One of a server-defined set of error codes.
details
Document
Intelligence
Error[]
An array of details about specific errors that led to this reported error.
innererror
Document
Intelligence
InnerError
An object containing more specific information than the current object
about the error.
message
string
A human-readable representation of the error.
target
string
The target of the error.
DocumentIntelligenceErrorResponse
Object
Error response object.
0
Expand table
Name
Type
Description
error
Document
Error info.
Intelligence
Error
DocumentIntelligenceInnerError
Object
An object containing more specific information about the error.
0
Expand table

| Name | Type | Description |
| --- | --- | --- |
| code | string | One of a server-defined set of error codes. |
| details | Document Intelligence Error[] | An array of details about specific errors that led to this reported error. |
| innererror | Document Intelligence InnerError | An object containing more specific information than the current object about the error. |
| message | string | A human-readable representation of the error. |
| target | string | The target of the error. |

| Name | Type | Description |
| --- | --- | --- |
| error | Document | Error info. |
|  | Intelligence Error |  |
|  |  |  |


<!---- Page 602 ---------------------------------------------------------------------------------------------------------------------------------->
Name
Type
Description
code
string
One of a server-defined set of error codes.
innererror
Document
Intelligence
InnerError
Inner error.
message
string
A human-readable representation of the error.
StringIndexType
Enumeration
Method used to compute string offset and length.
Expand table
Value
Description
textElements
User-perceived display character, or grapheme cluster, as defined by Unicode 8.0.0.
unicodeCodePoint
Character unit represented by a single unicode code point. Used by Python 3.
utf16CodeUnit
Character unit represented by a 16-bit Unicode code unit. Used by JavaScript, Java,
and .NET.

| Name | Type | Description |
| --- | --- | --- |
| code | string | One of a server-defined set of error codes. |
| innererror | Document Intelligence InnerError | Inner error. |
| message | string | A human-readable representation of the error. |

| Value | Description |
| --- | --- |
| textElements | User-perceived display character, or grapheme cluster, as defined by Unicode 8.0.0. |
| unicodeCodePoint | Character unit represented by a single unicode code point. Used by Python 3. |
| utf16CodeUnit | Character unit represented by a 16-bit Unicode code unit. Used by JavaScript, Java, and .NET. |


<!---- Page 603 ---------------------------------------------------------------------------------------------------------------------------------->
Error guide v4.0, v3.1, and v3.0
Article · 11/19/2024
Document Intelligence uses a unified design to represent all errors encountered in the
REST APIs. Whenever an API operation returns a 4xx or 5xx status code, additional
information about the error is returned in the response JSON body as follows:
JSON
{
"error": {
"code": "InvalidRequest",
"message": "Invalid request.",
"innererror": {
"code": "InvalidContent",
"message": "The file format is unsupported or corrupted. Refer to
documentation for the list of supported formats."
}
}
}
For long-running operations where multiple errors are encountered, the top-level error
code is set to the most severe error, with the individual errors listed under the
error.details property. In such scenarios, the target property of each individual error
specifies the trigger of the error.
JSON
{
"status": "failed",
"createdDateTime": "2021-07-14T10:17:51Z",
"lastUpdatedDateTime": "2021-07-14T10:17:51Z",
"error": {
"code": "InternalServerError",
"message": "An unexpected error occurred.",
"details": [
{
"code": "InternalServerError",
"message": "An unexpected error occurred."
},
{
"code": "InvalidContentDimensions",
"message": "The input image dimensions are out of range.
Refer to documentation for supported image dimensions.",
"target": "2"
}
]


<!---- Page 604 ---------------------------------------------------------------------------------------------------------------------------------->
}
}
The top-level error.code property can be one of the following error code messages:
0
Expand table
Error Code
Message
Http Status
InvalidRequest
Invalid request.
400
InvalidArgument
Invalid argument.
400
Forbidden
Access forbidden due to policy or other configuration.
403
NotFound
Resource not found.
404
MethodNotAllowed
The requested HTTP method isn't allowed.
405
Conflict
The request couldn't be completed due to a conflict.
409
UnsupportedMedia Type
Request content type isn't supported.
415
InternalServerError
An unexpected error occurred.
500
ServiceUnavailable
A transient error occurred. Try again.
503
When possible, more details are specified in the inner error property.
0
Expand table
Top Error Code
Inner Error Code
Message
Conflict
ModelExists
A model with the provided name already
exists.
Forbidden
AuthorizationFailed
Authorization failed: {details}
Forbidden
InvalidDataProtectionKey
Data protection key is invalid: {details}
Forbidden
OutboundAccessForbidden
The request contains a disallowed
domain name or violates the current
access control policy.
InternalServerError
Unknown
Unknown error.
InvalidArgument
InvalidContentSourceFormat
Invalid content source: {details}
InvalidArgument
InvalidParameter
The parameter {parameterName} is
invalid: {details}

| Error Code | Message | Http Status |
| --- | --- | --- |
| InvalidRequest | Invalid request. | 400 |
| InvalidArgument | Invalid argument. | 400 |
| Forbidden | Access forbidden due to policy or other configuration. | 403 |
| NotFound | Resource not found. | 404 |
| MethodNotAllowed | The requested HTTP method isn't allowed. | 405 |
| Conflict | The request couldn't be completed due to a conflict. | 409 |
| UnsupportedMedia Type | Request content type isn't supported. | 415 |
| InternalServerError | An unexpected error occurred. | 500 |
| ServiceUnavailable | A transient error occurred. Try again. | 503 |

| Top Error Code | Inner Error Code | Message |
| --- | --- | --- |
| Conflict | ModelExists | A model with the provided name already exists. |
| Forbidden | AuthorizationFailed | Authorization failed: {details} |
| Forbidden | InvalidDataProtectionKey | Data protection key is invalid: {details} |
| Forbidden | OutboundAccessForbidden | The request contains a disallowed domain name or violates the current access control policy. |
| InternalServerError | Unknown | Unknown error. |
| InvalidArgument | InvalidContentSourceFormat | Invalid content source: {details} |
| InvalidArgument | InvalidParameter | The parameter {parameterName} is invalid: {details} |


<!---- Page 605 ---------------------------------------------------------------------------------------------------------------------------------->
Top Error Code
Inner Error Code
Message
InvalidArgument
InvalidParameterLength
Parameter {parameterName} length
must not exceed {maxChars} characters.
InvalidArgument
InvalidSasToken
The shared access signature (SAS) is
invalid: {details}
InvalidArgument
ParameterMissing
The parameter {parameterName} is
required.
InvalidRequest
ContentSourceNotAccessible
Content isn't accessible: {details}
InvalidRequest
ContentSourceTimeout
Timeout while receiving the file from
client.
InvalidRequest
DocumentModelLimit
Account can't create more than
{maximumModels} models.
InvalidRequest
DocumentModelLimitNeural
Account can't create more than 10
custom neural models per month.
Contact support to request more
capacity.
InvalidRequest
DocumentModelLimitComposed
Account can't create a model with more
than {details} component models.
InvalidRequest
InvalidContent
The file is corrupted or format is
unsupported. Refer to documentation
for the list of supported formats.
InvalidRequest
InvalidContentDimensions
The input image dimensions are out of
range. Refer to documentation for
supported image dimensions.
InvalidRequest
InvalidContentLength
The input image is too large. Refer to
documentation for the maximum file
size.
InvalidRequest
InvalidFieldsDefinition
Invalid fields: {details}
InvalidRequest
InvalidTrainingContentLength
Training content contains {bytes} bytes.
Training is limited to {maxBytes} bytes.
InvalidRequest
InvalidTrainingContentPageCount
Training content contains {pages} pages.
Training is limited to {pages} pages.
InvalidRequest
ModelAnalyzeError
Couldn't analyze using a custom model:
{details}
InvalidRequest
ModelBuildError
Couldn't build the model: {details}

| Top Error Code | Inner Error Code | Message |
| --- | --- | --- |
| InvalidArgument | InvalidParameterLength | Parameter {parameterName} length must not exceed {maxChars} characters. |
| InvalidArgument | InvalidSasToken | The shared access signature (SAS) is invalid: {details} |
| InvalidArgument | ParameterMissing | The parameter {parameterName} is required. |
| InvalidRequest | ContentSourceNotAccessible | Content isn't accessible: {details} |
| InvalidRequest | ContentSourceTimeout | Timeout while receiving the file from client. |
| InvalidRequest | DocumentModelLimit | Account can't create more than {maximumModels} models. |
| InvalidRequest | DocumentModelLimitNeural | Account can't create more than 10 custom neural models per month. Contact support to request more capacity. |
| InvalidRequest | DocumentModelLimitComposed | Account can't create a model with more than {details} component models. |
| InvalidRequest | InvalidContent | The file is corrupted or format is unsupported. Refer to documentation for the list of supported formats. |
| InvalidRequest | InvalidContentDimensions | The input image dimensions are out of range. Refer to documentation for supported image dimensions. |
| InvalidRequest | InvalidContentLength | The input image is too large. Refer to documentation for the maximum file size. |
| InvalidRequest | InvalidFieldsDefinition | Invalid fields: {details} |
| InvalidRequest | InvalidTrainingContentLength | Training content contains {bytes} bytes. Training is limited to {maxBytes} bytes. |
| InvalidRequest | InvalidTrainingContentPageCount | Training content contains {pages} pages. Training is limited to {pages} pages. |
| InvalidRequest | ModelAnalyzeError | Couldn't analyze using a custom model: {details} |
| InvalidRequest | ModelBuildError | Couldn't build the model: {details} |


<!---- Page 606 ---------------------------------------------------------------------------------------------------------------------------------->
Top Error Code
Inner Error Code
Message
InvalidRequest
ModelComposeError
Couldn't compose the model: {details}
InvalidRequest
ModelNotReady
Model isn't ready for the requested
operation. Wait for training to complete
or check for operation errors.
InvalidRequest
ModelReadOnly
The requested model is read-only.
InvalidRequest
NotSupportedApiVersion
The requested operation requires
{minimumApiVersion} or later.
InvalidRequest
OperationNotCancellable
The operation can no longer be
canceled.
InvalidRequest
TrainingContentMissing
Training data is missing: {details}
InvalidRequest
UnsupportedContent
Content isn't supported: {details}
NotFound
ModelNotFound
The requested model wasn't found. It
was deleted or still building.
NotFound
OperationNotFound
The requested operation wasn't found.
The identifier is invalid or the operation
is expired.
Feedback
Was this page helpful?
Yes
No
Provide product feedback 7 | Get help at Microsoft Q&A

| Top Error Code | Inner Error Code | Message |
| --- | --- | --- |
| InvalidRequest | ModelComposeError | Couldn't compose the model: {details} |
| InvalidRequest | ModelNotReady | Model isn't ready for the requested operation. Wait for training to complete or check for operation errors. |
| InvalidRequest | ModelReadOnly | The requested model is read-only. |
| InvalidRequest | NotSupportedApiVersion | The requested operation requires {minimumApiVersion} or later. |
| InvalidRequest | OperationNotCancellable | The operation can no longer be canceled. |
| InvalidRequest | TrainingContentMissing | Training data is missing: {details} |
| InvalidRequest | UnsupportedContent | Content isn't supported: {details} |
| NotFound | ModelNotFound | The requested model wasn't found. It was deleted or still building. |
| NotFound | OperationNotFound | The requested operation wasn't found. The identifier is invalid or the operation is expired. |


<!---- Page 607 ---------------------------------------------------------------------------------------------------------------------------------->
Analyzer
Reference
Service: Azure AI Services
API Version: 2.1
Operations
[] Expand table
Analyze Business
Card
Extract field text and semantic values from a given business card document.
The input document must be of one of the supported content types -
'application/pdf' ...
Analyze Id
Document
Extract field text and semantic values from a given ID document. The input
document must be of one of the supported content types -
'application/pdf', 'image/jp ...
Analyze Invoice
Extract field text and semantic values from a given invoice document. The
input document must be of one of the supported content types -
'application/pdf', 'ima ...
Analyze Layout
Extract text and layout information from a given document. The input
document must be of one of the supported content types -
'application/pdf', 'image/jpeg', ' ...
Analyze Receipt
Extract field text and semantic values from a given receipt document. The
input document must be of one of the supported content types -
'application/pdf', 'ima ...
Get Analyze
Business Card
Result
Track the progress and obtain the result of the analyze business card
operation.
Get Analyze Id
Document Result
Track the progress and obtain the result of the analyze ID operation.
Get Analyze
Invoice Result
Track the progress and obtain the result of the analyze invoice operation.
Get Analyze Layout
Result
Track the progress and obtain the result of the analyze layout operation
Get Analyze
Receipt Result
Track the progress and obtain the result of the analyze receipt operation.

| Analyze Business Card | Extract field text and semantic values from a given business card document. The input document must be of one of the supported content types - 'application/pdf' ... |
| --- | --- |
| Analyze Id Document | Extract field text and semantic values from a given ID document. The input document must be of one of the supported content types - 'application/pdf', 'image/jp ... |
| Analyze Invoice | Extract field text and semantic values from a given invoice document. The input document must be of one of the supported content types - 'application/pdf', 'ima ... |
| Analyze Layout | Extract text and layout information from a given document. The input document must be of one of the supported content types - 'application/pdf', 'image/jpeg', ' ... |
| Analyze Receipt | Extract field text and semantic values from a given receipt document. The input document must be of one of the supported content types - 'application/pdf', 'ima ... |
| Get Analyze Business Card Result | Track the progress and obtain the result of the analyze business card operation. |
| Get Analyze Id Document Result | Track the progress and obtain the result of the analyze ID operation. |
| Get Analyze Invoice Result | Track the progress and obtain the result of the analyze invoice operation. |
| Get Analyze Layout Result | Track the progress and obtain the result of the analyze layout operation |
| Get Analyze Receipt Result | Track the progress and obtain the result of the analyze receipt operation. |


<!---- Page 608 ---------------------------------------------------------------------------------------------------------------------------------->
Azure Document Intelligence client
library for .NET - version 1.0.0-beta.3
Article · 08/15/2024
Note: on July 2023, the Azure Cognitive Services Form Recognizer service was
renamed to Azure AI Document Intelligence. Any mentions of Form Recognizer or
Document Intelligence in documentation refer to the same Azure service.
Azure AI Document Intelligence is a cloud service that uses machine learning to analyze
text and structured data from your documents. It includes the following main features:
· Layout - Extract text, selection marks, table structures, styles, and paragraphs,
along with their bounding region coordinates from documents.
· Read - Read information about textual elements, such as page words and lines in
addition to text language information.
· Prebuilt - Analyze data from certain types of common documents using prebuilt
models. Supported documents include receipts, invoices, business cards, identity
documents, US W2 tax forms, and more.
. Custom analysis - Build custom document models to analyze text, field values,
selection marks, table structures, styles, and paragraphs from documents. Custom
models are built with your own data, so they're tailored to your documents.
· Custom classification - Build custom classifier models that combine layout and
language features to accurately detect and identify documents you process within
your application.
Source code [7 | Package (NuGet) [] | API reference documentation [] | Product
documentation | Samples ₹
Getting started
This section should include everything a developer needs to do to install and create
their first client connection very quickly.
Install the package
Install the Azure Document Intelligence client library for .NET with NuGet :
.NET CLI


<!---- Page 609 ---------------------------------------------------------------------------------------------------------------------------------->
dotnet add package Azure.AI.DocumentIntelligence -- prerelease
Note: This version of the client library defaults to the 2024-07-31-preview version of
the service.
Prerequisites
. An Azure subscription 2.
· A Cognitive Services or Document Intelligence resource to use this package.
Create a Cognitive Services or Document Intelligence resource
Document Intelligence supports both multi-service and single-service access. Create a
Cognitive Services resource if you plan to access multiple cognitive services under a
single endpoint and key. For Document Intelligence access only, create a Document
Intelligence resource. Please note that you will need a single-service resource if you
intend to use Azure Active Directory authentication.
You can create either resource using:
· Option 1: Azure Portal.
· Option 2: Azure CLI.
Below is an example of how you can create a Document Intelligence resource using the
CLI:
PowerShell
# Create a new resource group to hold the Document Intelligence resource
# If using an existing resource group, skip this step
az group create -- name <your-resource-name> -- location <location>
PowerShell
# Create the Form Recognizer resource
az cognitiveservices account create \
-- name <resource-name> \
-- resource-group <resource-group-name> \
-- kind FormRecognizer |
-- sku <sku> \
-- location <location> \
-- yes


<!---- Page 610 ---------------------------------------------------------------------------------------------------------------------------------->
For more information about creating the resource or how to get the location and sku
information see here.
Authenticate the client
In order to interact with the Document Intelligence service, you'll need to create an
instance of the DocumentIntelligenceClient [ class. An endpoint and a credential are
necessary to instantiate the client object.
Get the endpoint
You can find the endpoint for your Document Intelligence resource using the Azure
Portal or the Azure CLI:
PowerShell
# Get the endpoint for the Document Intelligence resource
az cognitiveservices account show -- name "<resource-name>" -- resource-group
"<resource-group-name>" -- query "properties. endpoint"
Either a regional endpoint or a custom subdomain can be used for authentication. They
are formatted as follows:
Regional endpoint: https://<region>.api.cognitive.microsoft.com/
Custom subdomain: https://<resource-name>.cognitiveservices.azure.com/
A regional endpoint is the same for every resource in a region. A complete list of
supported regional endpoints can be consulted here. Please note that regional
endpoints do not support AAD authentication.
A custom subdomain, on the other hand, is a name that is unique to the Document
Intelligence resource. They can only be used by single-service resources.
Get the API Key
The API key can be found in the Azure Portal E or by running the following Azure CLI
command:
PowerShell


<!---- Page 611 ---------------------------------------------------------------------------------------------------------------------------------->
az cognitiveservices account keys list -- name "<resource-name>" -- resource-
group "<resource-group-name>"
Create DocumentIntelligenceClient with AzureKeyCredential
Once you have the value for the API key, create an AzureKeyCredential. With the
endpoint and key credential, you can create the DocumentIntelligenceClient :
C#
string endpoint = "<endpoint>";
string apiKey = "<apiKey>";
var client = new DocumentIntelligenceClient(new Uri(endpoint), new
AzureKeyCredential(apiKey));
Create DocumentIntelligenceClient with Azure Active Directory
Credential
AzureKeyCredential authentication is used in the examples in this getting started guide,
but you can also authenticate with Azure Active Directory using the Azure Identity
library . Note that regional endpoints do not support AAD authentication. Create a
custom subdomain for your resource in order to use this type of authentication.
To use the DefaultAzureCredential « provider shown below, or other credential
providers provided with the Azure SDK, please install the Azure. Identity package:
.NET CLI
dotnet add package Azure. Identity
You will also need to register a new AAD application and grant access to Document
Intelligence by assigning the "Cognitive Services User" role to your service principal.
Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET.
C#
string endpoint = "<endpoint>";
var client = new DocumentIntelligenceClient(new Uri(endpoint), new
DefaultAzureCredential());


<!---- Page 612 ---------------------------------------------------------------------------------------------------------------------------------->
Key concepts
DocumentIntelligenceClient
DocumentIntelligenceClient provides operations for:
· Analyzing input documents using prebuilt and custom models through the
AnalyzeDocument API.
· Detecting and identifying custom input documents with the ClassifyDocument API.
Sample code snippets are provided to illustrate using a DocumentIntelligenceClient
here.
More information about analyzing documents, including supported features, locales,
and document types can be found in the service documentation .
DocumentIntelligenceAdministrationClient
DocumentIntelligenceAdministrationClient provides operations for:
· Building custom models to analyze specific fields you specify by labeling your
custom documents.
· Compose a model from a collection of existing models.
· Managing models created in your account.
· Copying a custom model from one Document Intelligence resource to another.
· Getting or listing operations created within the last 24 hours.
· Building and managing document classification models to accurately detect and
identify documents you process within your application.
See examples for Build a Custom Model, Manage Models, and Build a Document
Classifier.
Please note that models and classifiers can also be built using a graphical user interface
such as the Document Intelligence Studio [].
Thread safety
We guarantee that all client instance methods are thread-safe and independent of each
other (guideline[). This ensures that the recommendation of reusing client instances is
always safe, even across threads.


<!---- Page 613 ---------------------------------------------------------------------------------------------------------------------------------->
Additional concepts
Client options | Accessing the response < | Long-running operations | Handling
failures | Diagnostics | Mocking | Client lifetime
Examples
The following section provides several code snippets illustrating common patterns used
in the Document Intelligence .NET API. Most of the snippets below make use of
asynchronous service calls, but keep in mind that the Azure.AI.DocumentIntelligence
package supports both synchronous and asynchronous APIs.
· Extract Layout
· Use Prebuilt Models
· Build a Custom Model
· Manage Models
· Build a Document Classifier
. Classify a Document
Extract Layout
Extract text, selection marks, table structures, styles, and paragraphs, along with their
bounding region coordinates from documents.
C#
Uri uriSource = new Uri("<uriSource>");
var content = new AnalyzeDocumentContent()
{
UrlSource = uriSource
};
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-layout",
content);
AnalyzeResult result = operation. Value;
foreach (DocumentPage page in result. Pages)
{
Console.WriteLine($"Document Page {page. PageNumber} has
{page. Lines.Count} line(s), {page.Words. Count} word(s)," +
$" and {page. SelectionMarks. Count} selection mark(s).");
for (int i = 0; i < page. Lines.Count; i++)
{


<!---- Page 614 ---------------------------------------------------------------------------------------------------------------------------------->
DocumentLine line = page. Lines[i];
Console. WriteLine($"
Line {i}:");
Console. WriteLine ($"
Content: '{line. Content} '");
Console.Write("
Bounding polygon, with points ordered
for (int j = 0; j < line. Polygon. Count; j += 2)
{
Console.Write($" ({line. Polygon[j]}, {line. Polygon[j + 1]})");
}
clockwise:");
Console.WriteLine();
}
for (int i = 0; i < page.SelectionMarks. Count; i++)
{
DocumentSelectionMark selectionMark = page.SelectionMarks[i];
Console.WriteLine($" Selection Mark {i} is
{selectionMark. State} . ");
Console. WriteLine($"
State: {selectionMark. State}");
clockwise:");
Console.Write("
Bounding polygon, with points ordered
for (int j = 0; j < selectionMark. Polygon. Count; j++)
{
Console.Write($" ({selectionMark . Polygon [j]},
{selectionMark. Polygon[j + 1]})");
}
Console.WriteLine();
}
}
for (int i = 0; i < result. Paragraphs. Count; i++)
{
DocumentParagraph paragraph = result. Paragraphs[i];
Console.WriteLine($"Paragraph {i}:");
Console. WriteLine($" Content: {paragraph. Content}") ;
if (paragraph. Role != null)
{
Console.WriteLine($" Role: {paragraph. Role}");
}
}
foreach (DocumentStyle style in result. Styles)
{
// Check the style and style confidence to see if text is handwritten.
// Note that value '0.8' is used as an example.
bool isHandwritten = style. IsHandwritten. HasValue && style. IsHandwritten
== true;


<!---- Page 615 ---------------------------------------------------------------------------------------------------------------------------------->
if (isHandwritten && style. Confidence > 0.8)
{
Console.WriteLine($"Handwritten content found:");
foreach (DocumentSpan span in style. Spans)
{
var handwrittenContent = result.Content.Substring(span.Offset,
span. Length) ;
Console.WriteLine($" {handwrittenContent}");
}
}
}
for (int i = 0; i < result. Tables.Count; i++)
{
DocumentTable table = result. Tables[i];
Console.WriteLine($"Table {i} has {table. RowCount} rows and
{table. ColumnCount} columns.");
foreach (DocumentTableCell cell in table. Cells)
{
Console.WriteLine($" Cell ({cell. RowIndex}, {cell. ColumnIndex} ) is
a '{cell. Kind}' with content: {cell.Content}");
}
}
For more information, see here .
Use Prebuilt Models
Analyze data from certain types of common documents using prebuilt models provided
by the Document Intelligence service.
For example, to analyze fields from an invoice, use the prebuilt Invoice model provided
by passing the prebuilt-invoice model ID to the AnalyzeDocumentAsync method:
C#
Uri uriSource = new Uri("<uriSource>");
var content = new AnalyzeDocumentContent()
{
UrlSource = uriSource
};
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-invoice",
content);
AnalyzeResult result = operation. Value;


<!---- Page 616 ---------------------------------------------------------------------------------------------------------------------------------->
// To see the list of all the supported fields returned by service and its
corresponding types for the
// prebuilt-invoice model, see:
// https://aka.ms/azsdk/formrecognizer/invoicefieldschema
for (int i = 0; i < result.Documents. Count; i++)
{
Console.WriteLine($"Document {i}:");
AnalyzedDocument document = result. Documents[i];
if (document. Fields. TryGetValue("VendorName", out DocumentField
vendorNameField)
&& vendorNameField. Type == DocumentFieldType. String)
{
string vendorName = vendorNameField. ValueString;
Console.WriteLine($"Vendor Name: '{vendorName}', with confidence
{vendorNameField.Confidence}");
}
if (document.Fields. TryGetValue("CustomerName", out DocumentField
customerNameField)
&& customerNameField. Type == DocumentFieldType. String)
{
string customerName = customerNameField. ValueString;
Console. WriteLine($"Customer Name: '{customerName}', with confidence
{customerNameField.Confidence}");
}
if (document. Fields. TryGetValue("Items", out DocumentField itemsField)
&& itemsField. Type == DocumentFieldType. List)
{
foreach (DocumentField itemField in itemsField. ValueList)
{
Console.WriteLine("Item:");
if (itemField. Type == DocumentFieldType. Dictionary)
{
IReadOnlyDictionary<string, DocumentField> itemFields =
itemField. ValueDictionary;
if (itemFields. TryGetValue("Description", out DocumentField
itemDescriptionField)
&& itemDescriptionField. Type ==
DocumentFieldType.String)
{
string itemDescription =
itemDescriptionField. ValueString;
Console.WriteLine($" Description: ' {itemDescription} ',
with confidence {itemDescriptionField. Confidence}");
}
if (itemFields. TryGetValue("Amount", out DocumentField
itemAmountField)


<!---- Page 617 ---------------------------------------------------------------------------------------------------------------------------------->
&& itemAmountField. Type == DocumentFieldType. Currency)
{
CurrencyValue itemAmount =
itemAmountField. ValueCurrency;
Console.WriteLine($" Amount:
'{itemAmount. CurrencySymbol} {itemAmount. Amount}', with confidence
{itemAmountField.Confidence}");
}
}
}
}
if (document.Fields. TryGetValue("SubTotal", out DocumentField
subTotalField)
&& subTotalField. Type == DocumentFieldType. Currency)
{
CurrencyValue subTotal = subTotalField. ValueCurrency;
Console.WriteLine($"Sub Total: '{subTotal. CurrencySymbol}
{subTotal.Amount} ', with confidence {subTotalField.Confidence}");
}
if (document. Fields. TryGetValue("TotalTax", out DocumentField
totalTaxField)
&& totalTaxField. Type == DocumentFieldType. Currency)
{
CurrencyValue totalTax = totalTaxField. ValueCurrency;
Console.WriteLine($"Total Tax: '{totalTax. CurrencySymbol}
{totalTax. Amount} ', with confidence {totalTaxField. Confidence}");
}
if (document. Fields. TryGetValue("InvoiceTotal", out DocumentField
invoiceTotalField)
&& invoiceTotalField. Type == DocumentFieldType. Currency)
{
CurrencyValue invoiceTotal = invoiceTotalField. ValueCurrency;
Console.WriteLine($"Invoice Total: '{invoiceTotal. CurrencySymbol}
{invoiceTotal.Amount} ', with confidence {invoiceTotalField. Confidence}");
}
}
You are not limited to invoices! There are a couple of prebuilt models to choose from,
each of which has its own set of supported fields. More information about the
supported document types can be found in the service documentation .
For more information, see here Z.
Build a Custom Model
Build a custom model on your own document type. The resulting model can be used to
analyze values from the types of documents it was built on.


<!---- Page 618 ---------------------------------------------------------------------------------------------------------------------------------->
C#
// For this sample, you can use the training documents found in the
`trainingFiles` folder.
// Upload the documents to your storage container and then generate a
container SAS URL. Note
// that a container URI without SAS is accepted only when the container is
public or has a
/ / managed identity configured.
// For instructions to set up documents for training in an Azure Blob
Storage Container, please see:
// https://aka.ms/azsdk/formrecognizer/buildcustommodel
string modelId = "<modelId>";
Uri blobContainerUri = new Uri("<blobContainerUri>");
// We are selecting the Template build mode in this sample. For more
information about the available
// build modes and their differences, see:
// https://aka.ms/azsdk/formrecognizer/buildmode
var content = new BuildDocumentModelContent(modelId,
DocumentBuildMode. Template)
{
AzureBlobSource = new AzureBlobContentSource(blobContainerUri)
};
Operation<DocumentModelDetails> operation = await
client. BuildDocumentModelAsync(WaitUntil.Completed, content);
DocumentModelDetails model = operation. Value;
Console.WriteLine($"Model ID: {model. ModelId}");
Console.WriteLine($"Created on: {model. CreatedOn}");
Console.WriteLine("Document types the model can recognize:");
foreach (KeyValuePair<string, DocumentTypeDetails> docType in
model.DocTypes)
{
Console.WriteLine($" Document type: '{docType. Key}', which has the
following fields:");
foreach (KeyValuePair<string, DocumentFieldSchema> schema in
docType. Value. FieldSchema)
{
Console.WriteLine($" Field: ' {schema. Key}', with confidence
{docType.Value. FieldConfidence[ schema. Key]}");
}
}
For more information, see here .
Manage Models


<!---- Page 619 ---------------------------------------------------------------------------------------------------------------------------------->
Manage the models stored in your account.
C#
// Check number of custom models in the Document Intelligence resource, and
the maximum number
// of custom models that can be stored.
ResourceDetails resourceDetails = await client. GetResourceInfoAsync();
Console.WriteLine($"Resource has
{resourceDetails. CustomDocumentModels. Count} custom models. ");
Console. WriteLine($"It can have at most
{resourceDetails. CustomDocumentModels. Limit} custom models.");
// Get a model by ID.
string modelId = "<modelId>";
DocumentModelDetails model = await client. GetModelAsync(modelId);
Console.WriteLine($"Details about model with ID '{model. ModelId} ' :");
Console.WriteLine($" Created on: {model. CreatedOn}");
Console.WriteLine($" Expires on: {model. ExpiresOn}");
// List up to 10 models currently stored in the resource.
int count = 0;
await foreach (DocumentModelDetails modelItem in client. GetModelsAsync())
{
Console.WriteLine($"Model details:");
Console.WriteLine($"
Model ID: {modelItem.ModelId}");
Console.WriteLine($"
Description: {modelItem. Description}");
Console.WriteLine($"
Created on: {modelItem.CreatedOn}");
Console.WriteLine($"
Expires on: {model. ExpiresOn}");
}
if (++count == 10)
{
break;
}
For more information, see herez.
Build a Document Classifier
Build a document classifier by uploading custom training documents.
C#
// For this sample, you can use the training documents found in the
`classifierTrainingFiles' folder.
// Upload the documents to your storage container and then generate a


<!---- Page 620 ---------------------------------------------------------------------------------------------------------------------------------->
container SAS URL. Note
// that a container URI without SAS is accepted only when the container is
public or has a
/ / managed identity configured.
// For instructions to set up documents for training in an Azure Blob
Storage Container, please see:
// https://aka.ms/azsdk/formrecognizer/buildclassifiermodel
string classifierId = "<classifierId>";
Uri blobContainerUri = new Uri("<blobContainerUri>");
var sourceA = new AzureBlobContentSource(blobContainerUri) { Prefix = "IRS-
1040-A/train" };
var sourceB = new AzureBlobContentSource(blobContainerUri) { Prefix = "IRS-
1040-B/train" };
var docTypeA = new ClassifierDocumentTypeDetails() { AzureBlobSource =
sourceA };
var docTypeB = new ClassifierDocumentTypeDetails() { AzureBlobSource =
sourceB };
var docTypes = new Dictionary<string, ClassifierDocumentTypeDetails>()
{
{ "IRS-1040-A", docTypeA },
{ "IRS-1040-B", docTypeB }
};
var content = new BuildDocumentClassifierContent(classifierId, docTypes);
Operation<DocumentClassifierDetails> operation = await
client. BuildClassifierAsync(WaitUntil.Completed, content);
DocumentClassifierDetails classifier = operation. Value;
Console.WriteLine($"Classifier ID: {classifier. ClassifierId}");
Console.WriteLine($"Created on: {classifier. CreatedOn}");
Console.WriteLine("Document types the classifier can recognize:");
foreach (KeyValuePair<string, ClassifierDocumentTypeDetails> docType in
classifier. DocTypes)
{
Console.WriteLine($" {docType. Key}");
}
For more information, see here .
Classify a Document
Use document classifiers to accurately detect and identify documents you process within
your application.
C#


<!---- Page 621 ---------------------------------------------------------------------------------------------------------------------------------->
string classifierId = "<classifierId>";
Uri uriSource = new Uri("<uriSource>");
var content = new ClassifyDocumentContent()
{
UrlSource = uriSource
};
Operation<AnalyzeResult> operation = await
client. ClassifyDocumentAsync(WaitUntil. Completed, classifierId, content);
AnalyzeResult result = operation. Value;
Console.WriteLine($"Input was classified by the classifier with ID
'{result. ModelId} '.");
foreach (AnalyzedDocument document in result. Documents)
{
Console.WriteLine($"Found a document of type: {document. DocType}");
}
For more information, see here z .
Troubleshooting
General
When you interact with the Document Intelligence client library using the .NET SDK,
errors returned by the service will result in a RequestFailedException with the same
HTTP status code returned by the REST API request.
For example, if you submit a receipt image with an invalid Uri, a 400 error is returned,
indicating "Bad Request".
C#
var content = new AnalyzeDocumentContent()
{
UrlSource = new Uri("http://invalid.uri")
};
try
{
Operation<AnalyzeResult> operation = await
client. AnalyzeDocumentAsync(WaitUntil. Completed, "prebuilt-receipt",
content);
}
catch (RequestFailedException e)
{


<!---- Page 622 ---------------------------------------------------------------------------------------------------------------------------------->
}
Console.WriteLine(e.ToString());
You will notice that additional information is logged, like the client request ID of the
operation.
Message:
Azure. RequestFailedException: Service request failed.
Status: 400 (Bad Request)
ErrorCode: InvalidRequest
Content:
{"error":{"code":"InvalidRequest", "message":"Invalid
request. ", "innererror": {"code":"InvalidContent", "message":"The file is
corrupted or format is unsupported. Refer to documentation for the list of
supported formats."} } }
Headers:
Transfer-Encoding: chunked
x-envoy-upstream-service-time: REDACTED
apim-request-id: REDACTED
Strict-Transport-Security: REDACTED
X-Content-Type-Options: REDACTED
Date: Fri, 01 Oct 2021 02:55:44 GMT
Content-Type: application/json; charset=utf-8
Error codes and messages raised by the Document Intelligence service can be found in
the service documentation Z.
Setting up console logging
The simplest way to see the logs is to enable the console logging.
To create an Azure SDK log listener that outputs messages to console use the
AzureEventSourceListener.CreateConsoleLogger method.
C#
// Setup a listener to monitor logged events.
using AzureEventSourceListener listener =
AzureEventSourceListener.CreateConsoleLogger();
To learn more about other logging mechanisms see Diagnostics Samples .


<!---- Page 623 ---------------------------------------------------------------------------------------------------------------------------------->
Next steps
Samples showing how to use the Document Intelligence library are available in this
GitHub repository. Samples are provided for each main functional area:
. Extract the layout of a document
· Analyze a document with a prebuilt model
. Build a custom model
. Manage models
. Classify a document
. Build a document classifier
. Get and List document model operations
· Compose a model
· Copy a custom model between Document Intelligence resources
· Analyze a document with add-on capabilities
. Extract the layout of a document as Markdown
Contributing
This project welcomes contributions and suggestions. Most contributions require you to
agree to a Contributor License Agreement (CLA) declaring that you have the right to,
and actually do, grant us the rights to use your contribution. For details, visit
cla.microsoft.comz.
When you submit a pull request, a CLA-bot will automatically determine whether you
need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply
follow the instructions provided by the bot. You will only need to do this once across all
repos using our CLA.
This project has adopted the Microsoft Open Source Code of Conduct 2. For more
information see the Code of Conduct FAQ [ or contact opencode@microsoft.com with
any additional questions or comments.


<!---- Page 624 ---------------------------------------------------------------------------------------------------------------------------------->
Azure DocumentIntelligence client
library for Java - version 1.0.0-beta.4
Article · 08/15/2024
Azure Document Intelligence (previously known as Form Recognizer ) is a cloud service
that uses machine learning to analyze text and structured data from your documents. It
includes the following main features:
. Layout - Analyze text, table structures, and selection marks, along with their
bounding region coordinates, from documents.
· Prebuilt - Analyze data from certain types of common documents (such as receipts,
invoices, identity documents or US W2 tax forms) using prebuilt models.
· Custom - Build custom models to extract text, field values, selection marks, and
table data from documents. Custom models are built with your own data, so
they're tailored to your documents.
· Read - Read information about textual elements, such as page words and lines in
addition to text language information.
· Classifiers - Build custom classifiers to categorize documents into predefined
classes.
Source code | Package (Maven) | API reference documentation | Product
Documentation | Samples [
Getting started
Prerequisites
· Java Development Kit (JDK) with version 8 or above
Here are details about Java 8 client compatibility with Azure Certificate
Authority.
· Azure Subscription 7
. Al Services or Document Intelligence account to use this package.
Adding the package to your product
XML
‹dependency>
<groupId>com. azure</groupId>
<artifactId>azure-ai-documentintelligence</artifactId>


<!---- Page 625 ---------------------------------------------------------------------------------------------------------------------------------->
<version>1.0.0-beta.4</version>
</ dependency>
Note: This version of the client library defaults to the "2024-07-31-preview" version
of the service.
This table shows the relationship between SDK versions and supported API versions of
the service:
Expand table
SDK version
Supported API version of service
1.0.0-beta.1
2023-10-31-preview
1.0.0-beta.2
2024-02-29-preview
1.0.0-beta.3
2024-02-29-preview
1.0.0-beta.4
2024-07-31-preview
Note: Please rely on the older azure-ai-formrecognizer library through the older
service API versions for retired models, such as "prebuilt-businessCard" and
"prebuilt-document". For more information, see Changelog . The below table
describes the relationship of each client and its supported API version(s):
" Expand table
API version
Supported clients
2023-10-31-preview, 2024-02-
29-preview, 2024-07-31-
preview
DocumentIntelligenceClient and
DocumentIntelligenceAsyncClient
2023-07-31
DocumentAnalysisClient and
DocumentModelAdministrationClient in azure-ai-
formrecognizer SDK
Please see the Migration Guide for more information about migrating from azure-ai-
formrecognizer to azure-ai-documentintelligence.
Authentication

| SDK version | Supported API version of service |
| --- | --- |
| 1.0.0-beta.1 | 2023-10-31-preview |
| 1.0.0-beta.2 | 2024-02-29-preview |
| 1.0.0-beta.3 | 2024-02-29-preview |
| 1.0.0-beta.4 | 2024-07-31-preview |

| API version | Supported clients |
| --- | --- |
| 2023-10-31-preview, 2024-02- 29-preview, 2024-07-31- preview | DocumentIntelligenceClient and DocumentIntelligenceAsyncClient |
| 2023-07-31 | DocumentAnalysisClient and |
|  | DocumentModelAdministrationClient in azure-ai- |
|  | formrecognizer SDK |


<!---- Page 626 ---------------------------------------------------------------------------------------------------------------------------------->
In order to interact with the Azure Document Intelligence Service you'll need to create
an instance of client class, DocumentIntelligenceAsyncClient & or
DocumentIntelligenceClient E by using DocumentIntelligenceClientBuilder 2. To
configure a client for use with Azure DocumentIntelligence, provide a valid endpoint URI
to an Azure DocumentIntelligence resource along with a corresponding key credential,
token credential, or Azure Identity [ credential that's authorized to use the Azure
DocumentIntelligence resource.
Create an Azure DocumentIntelligence client with key credential
Get Azure DocumentIntelligence key credential from the Azure Portal.
Java
DocumentIntelligenceClient documentIntelligenceClient = new
DocumentIntelligenceClientBuilder()
. credential(new AzureKeyCredential("{key}"))
. endpoint ("{endpoint}")
.buildClient();
or
Java
DocumentIntelligenceAdministrationClient client =
new DocumentIntelligenceAdministrationClientBuilder()
.credential(new AzureKeyCredential("{key}"))
. endpoint("{endpoint}")
.buildClient();
Create an Azure DocumentIntelligence client with Azure Active
Directory credential
Azure SDK for Java supports an Azure Identity package, making it easy to get credentials
from Microsoft identity platform.
Authentication with AAD requires some initial setup:
. Add the Azure Identity package
XML
<dependency>
<groupId>com.azure</groupId>
<artifactId>azure-identity</artifactId>


<!---- Page 627 ---------------------------------------------------------------------------------------------------------------------------------->
<version>1.13.2</version>
</ dependency>
After setup, you can choose which type of credential from azure-identity to use. As an
example, DefaultAzureCredential « can be used to authenticate the client: Set the values
of the client ID, tenant ID, and client secret of the AAD application as environment
variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET.
Authorization is easiest using DefaultAzureCredential 7. It finds the best credential to
use in its running environment. For more information about using Azure Active Directory
authorization with DocumentIntelligence service, please refer to the associated
documentation.
Java
DocumentIntelligenceAsyncClient documentIntelligenceAsyncClient = new
DocumentIntelligenceClientBuilder()
.credential(new DefaultAzureCredentialBuilder() . build())
. endpoint("{endpoint}")
.buildAsyncClient();
Key concepts
DocumentAnalysisClient
The DocumentAnalysisClient and DocumentAnalysisAsyncClient provide both
synchronous and asynchronous operations for analyzing input documents using custom
and prebuilt models through the beginAnalyzeDocument API. See a full list of supported
models herez .
Sample code snippets to illustrate using a DocumentAnalysisClient here . More
information about analyzing documents, including supported features, locales, and
document types can be found here .
DocumentModelAdministrationClient
The DocumentModelAdministrationClient and
DocumentModelAdministrationAsyncClient provide both synchronous and
asynchronous operations
. Build custom document analysis models to analyze text content, fields, and values
found in your custom documents. See example Build a document model. A


<!---- Page 628 ---------------------------------------------------------------------------------------------------------------------------------->
DocumentModelDetails is returned indicating the document types that the model
can analyze, along with the fields and schemas it will extract.
· Managing models created in your account by building, listing, deleting, and see
the limit of custom models your account. See example Manage models.
· Copying a custom model from one Document Intelligence resource to another.
· Creating a composed model from a collection of existing built models.
· Listing document model operations associated with the Document Intelligence
resource.
Sample code snippets are provided to illustrate using a
DocumentModelAdministrationClient here.
Long-running operations
Long-running operations are operations that consist of an initial request sent to the
service to start an operation, followed by polling the service at intervals to determine
whether the operation has completed or failed, and if it has succeeded, to get the result.
Methods that build models, analyze values from documents, or copy and compose
models are modeled as long-running operations. The client exposes a
begin<MethodName> method that returns a SyncPoller or PollerFlux instance. Callers
should wait for the operation to be completed by calling getFinalResult() on the
returned operation from the begin<MethodName> method. Sample code snippets are
provided to illustrate using long-running operations below.
Examples
The following section provides several code snippets covering some of the most
common Document Intelligence tasks, including:
· Analyze Layout
· Use Prebuilt Models
· Build a Document Model
· Analyze Documents using a Custom Model
· Manage Your Models
Analyze Layout
Analyze text, table structures, and selection marks like radio buttons and check boxes,
along with their bounding box coordinates from documents without the need to build a
model.


<!---- Page 629 ---------------------------------------------------------------------------------------------------------------------------------->
Java
File layoutDocument = new File("local/file_path/filename.png");
Path filePath = layoutDocument. toPath();
BinaryData layoutDocumentData = BinaryData. fromFile(filePath, (int)
layoutDocument.length());
SyncPoller<AnalyzeResultOperation, AnalyzeResult> analyzeLayoutResultPoller
=
documentIntelligenceClient. beginAnalyzeDocument( "prebuilt-layout",
null,
null,
null,
null,
null,
null,
null,
new
AnalyzeDocumentRequest().setBase64Source(Files.readAllBytes(layoutDocument.t
oPath())));
AnalyzeResult analyzeLayoutResult =
analyzeLayoutResultPoller.getFinalResult();
/ / pages
analyzeLayoutResult. getPages() . forEach(documentPage -> {
System. out. printf("Page has width: %.2f and height: %.2f, measured with
unit: %s%n",
documentPage. getWidth(),
documentPage.getHeight(),
documentPage.getUnit());
// lines
documentPage. getLines().forEach(documentLine ->
System.out.printf("Line '%s' is within a bounding box %s.%n",
documentLine.getContent(),
documentLine.getPolygon().toString()));
// selection marks
documentPage.getSelectionMarks().forEach(documentSelectionMark ->
System. out.printf("Selection mark is '%s' and is within a bounding
box %s with confidence %. 2f .%n",
documentSelectionMark. getState().toString(),
documentSelectionMark.getPolygon().toString(),
documentSelectionMark.getConfidence()));
});
// tables
List<DocumentTable> tables = analyzeLayoutResult. getTables();
for (int i = 0; i < tables.size(); i++) {
DocumentTable documentTable = tables.get(i);
System.out.printf("Table %d has %d rows and %d columns.%n", i,
documentTable.getRowCount(),
documentTable.getColumnCount());


<!---- Page 630 ---------------------------------------------------------------------------------------------------------------------------------->
documentTable.getCells().forEach(documentTableCell -> {
System.out.printf("Cell '%s', has row index %d and column index
%d.%n", documentTableCell. getContent(),
documentTableCell.getRowIndex(),
documentTableCell.getColumnIndex());
});
System.out.println();
}
Use Prebuilt Models
Extract fields from select document types such as receipts, invoices, and identity
documents using prebuilt models provided by the Document Intelligence service.
Supported prebuilt models are:
· Analyze receipts using the prebuilt-receipt model (fields recognized by the
service can be found here )
· Analyze invoices using the prebuilt-invoice model (fields recognized by the
service can be found here ).
· Analyze identity documents using the prebuilt-idDocuments model (fields
recognized by the service can be found here ).
. Analyze US W2 tax forms using the prebuilt-tax. us. w2 model. Supported fields [].
For example, to analyze fields from a sales receipt, into the beginAnalyzeDocumentFromUrl
method:
Java
File sourceFile = new File(" .. /documentintelligence/azure-ai-
documentintelligence/src/samples/resources/"
+ "sample-forms/receipts/contoso-allinone. jpg");
SyncPoller<AnalyzeResultOperation, AnalyzeResult> analyzeReceiptPoller =
documentIntelligenceClient.beginAnalyzeDocument("prebuilt-receipt",
null,
null,
null,
null,
null,
null,
null,
new
AnalyzeDocumentRequest().setBase64Source(Files.readAllBytes(sourceFile. toPat
h())));
AnalyzeResult receiptResults = analyzeReceiptPoller.getFinalResult();
for (int i = 0; i < receiptResults.getDocuments().size(); i++) {


<!---- Page 631 ---------------------------------------------------------------------------------------------------------------------------------->
Document analyzedReceipt = receiptResults.getDocuments().get(i);
Map<String, DocumentField> receiptFields = analyzedReceipt. getFields();
System.out. printf("
Analyzing receipt info %d
%n",
i);
DocumentField merchantNameField = receiptFields. get("MerchantName");
if (merchantNameField != null) {
if (DocumentFieldType.STRING == merchantNameField.getType()) {
String merchantName = merchantNameField.getValueString();
System. out.printf("Merchant Name: %s, confidence: %.2f%n",
merchantName, merchantNameField.getConfidence());
}
}
DocumentField merchantPhoneNumberField =
receiptFields. get ("MerchantPhoneNumber") ;
if (merchantPhoneNumberField != null) {
if (DocumentFieldType. PHONE_NUMBER ==
merchantPhoneNumberField.getType()) {
String merchantAddress =
merchantPhoneNumberField.getValuePhoneNumber();
System.out.printf("Merchant Phone number: %s, confidence:
%.2f%n",
merchantAddress, merchantPhoneNumberField.getConfidence());
}
}
DocumentField merchantAddressField =
receiptFields. get ( "MerchantAddress");
if (merchantAddressField != null) {
if (DocumentFieldType.STRING == merchantAddressField.getType()) {
String merchantAddress = merchantAddressField.getValueString();
System. out. printf("Merchant Address: %s, confidence: %.2f%n",
merchantAddress, merchantAddressField.getConfidence());
}
}
DocumentField transactionDateField =
receiptFields . get ("TransactionDate");
if (transactionDateField != null) {
if (DocumentFieldType.DATE == transactionDateField.getType()) {
LocalDate transactionDate = transactionDateField.getValueDate();
System.out. printf("Transaction Date: %s, confidence: %.2f%n",
transactionDate, transactionDateField.getConfidence());
}
}
}
For more information and samples using prebuilt models, see:
. Identity Documents
. Invoices
. Receipts sample z


<!---- Page 632 ---------------------------------------------------------------------------------------------------------------------------------->
Build a document model
Build a machine-learned model on your own document type. The resulting model will be
able to analyze values from the types of documents it was built on. Provide a container
SAS url to your Azure Storage Blob container where you're storing the training
documents. See details on setting this up in the service quickstart documentation.
Note
You can use the Document Intelligence Studio preview for creating a labeled file for
your training forms. More details on setting up a container and required file structure
can be found in here .
Java
// Build custom document analysis model
String blobContainerUrl = "{SAS_URL_of_your_container_in_blob_storage}";
// The shared access signature (SAS) Url of your Azure Blob Storage
container with your forms.
SyncPoller<DocumentModelBuildOperationDetails, DocumentModelDetails>
buildOperationPoller =
administrationClient.beginBuildDocumentModel(new
BuildDocumentModelRequest("modelID", DocumentBuildMode. TEMPLATE)
. setAzureBlobSource(new AzureBlobContentSource(blobContainerUrl) ));
DocumentModelDetails documentModelDetails =
buildOperationPoller.getFinalResult();
// Model Info
System.out.printf("Model ID: %s%n", documentModelDetails.getModelId());
System.out.printf("Model Description: %s%n",
documentModelDetails.getDescription());
System.out. printf("Model created on: %s%n%n",
documentModelDetails.getCreatedDateTime());
System.out.println("Document Fields:");
documentModelDetails.getDocTypes().forEach((key, documentTypeDetails) -> {
documentTypeDetails.getFieldSchema(). forEach((field,
documentFieldSchema) -> {
System.out.printf("Field: %s", field);
System.out.printf("Field type: %s", documentFieldSchema.getType());
System.out.printf("Field confidence: %.2f",
documentTypeDetails.getFieldConfidence().get(field));
});
});
Analyze Documents using a Custom Model


<!---- Page 633 ---------------------------------------------------------------------------------------------------------------------------------->
Analyze the key/value pairs and table data from documents. These models are built with
your own data, so they're tailored to your documents. You should only analyze
documents of the same doc type that the custom model was built on.
Java
String documentUrl = "{document-url}";
String modelId = "{custom-built-model-ID}";
SyncPoller<AnalyzeResultOperation, AnalyzeResult> analyzeDocumentPoller =
documentIntelligenceClient.beginAnalyzeDocument(modelId,
"1",
"en-US",
StringIndexType. TEXT_ELEMENTS,
Arrays. asList (DocumentAnalysisFeature. LANGUAGES),
null,
ContentFormat. TEXT,
null,
new AnalyzeDocumentRequest().setUrlSource(documentUrl));
AnalyzeResult analyzeResult = analyzeDocumentPoller.getFinalResult();
for (int i = 0; i < analyzeResult.getDocuments().size(); i++) {
final Document analyzedDocument = analyzeResult.getDocuments().get(i);
System. out. printf(" ----------- Analyzing custom document %d ----------
%n", i);
System. out.printf("Analyzed document has doc type %s with confidence :
%. 2f%n",
analyzedDocument.getDocType(), analyzedDocument.getConfidence());
}
analyzeResult. getPages().forEach(documentPage -> {
System.out.printf("Page has width: %.2f and height: %.2f, measured with
unit: %s%n",
documentPage.getWidth(),
documentPage. getHeight(),
documentPage.getUnit());
// lines
documentPage. getLines().forEach(documentLine ->
System.out.printf("Line '%s' is within a bounding polygon %s .%n",
documentLine.getContent(),
documentLine.getPolygon()));
// words
documentPage. getWords(). forEach(documentWord ->
System. out. printf("Word '%s' has a confidence score of %.2f.%n",
documentWord.getContent(),
documentWord.getConfidence()));
});
// tables
List<DocumentTable> tables = analyzeResult. getTables();
for (int i = 0; i < tables.size(); i++) {


<!---- Page 634 ---------------------------------------------------------------------------------------------------------------------------------->
DocumentTable documentTable = tables.get(i);
System.out.printf("Table %d has %d rows and %d columns.%n", i,
documentTable.getRowCount(),
documentTable.getColumnCount());
documentTable.getCells().forEach(documentTableCell -> {
System. out.printf("Cell '%s', has row index %d and column index
%d . %n",
documentTableCell.getContent(),
documentTableCell.getRowIndex(),
documentTableCell.getColumnIndex());
});
System.out.println();
}
Manage your models
Manage the models in your Document Intelligence account.
Java
ResourceDetails resourceDetails = administrationClient.getResourceInfo();
System. out. printf("The resource has %s models, and we can have at most %s
models .%n",
resourceDetails.getCustomDocumentModels().getCount(),
resourceDetails.getCustomDocumentModels().getLimit());
// Next, we get a paged list of all of our models
PagedIterable<DocumentModelDetails> customDocumentModels =
administrationClient.listModels();
System. out.println("We have following models in the account:");
customDocumentModels. forEach(documentModelInfo -> {
System.out.println();
// get custom document analysis model info
DocumentModelDetails documentModel =
administrationClient.getModel(documentModelInfo.getModelId());
System.out.printf("Model ID: %s%n", documentModel.getModelId());
System. out.printf("Model Description: %s%n",
documentModel.getDescription());
System.out.printf("Model created on: %s%n",
documentModel.getCreatedDateTime());
if (documentModel.getDocTypes() != null) {
documentModel.getDocTypes().forEach((key, documentTypeDetails) -> {
documentTypeDetails.getFieldSchema() . forEach( (field,
documentFieldSchema) -> {
System.out.printf("Field: %s, ", field);
System.out.printf("Field type: %s, ",
documentFieldSchema.getType());
if (documentTypeDetails.getFieldConfidence() != null) {
System.out.printf("Field confidence: %.2f%n",
documentTypeDetails.getFieldConfidence().get(field));


<!---- Page 635 ---------------------------------------------------------------------------------------------------------------------------------->
}
});
});
}
});
For more detailed examples, refer to samples .
Troubleshooting
Enable client logging
You can set the AZURE_LOG_LEVEL environment variable to view logging statements made
in the client library. For example, setting AZURE_LOG_LEVEL=2 would show all
informational, warning, and error log messages. The log levels can be found here: log
levels [ .
Default HTTP Client
All client libraries by default use the Netty HTTP client. Adding the above dependency
will automatically configure the client library to use the Netty HTTP client. Configuring
or changing the HTTP client is detailed in the HTTP clients wiki .
Default SSL library
All client libraries, by default, use the Tomcat-native Boring SSL library to enable native-
level performance for SSL operations. The Boring SSL library is an uber jar containing
native libraries for Linux / macOS / Windows, and provides better performance
compared to the default SSL implementation within the JDK. For more information,
including how to reduce the dependency size, refer to the performance tuning < section
of the wiki.
Next steps
. Samples are explained in detail here .
Contributing
For details on contributing to this repository, see the contributing guide .


<!---- Page 636 ---------------------------------------------------------------------------------------------------------------------------------->
1. Fork it
2. Create your feature branch (git checkout -b my-new-feature)
3. Commit your changes ( git commit -am 'Add some feature' )
4. Push to the branch ( git push origin my-new-feature)
5. Create new Pull Request


<!---- Page 637 ---------------------------------------------------------------------------------------------------------------------------------->
Azure DocumentIntelligence (formerly
FormRecognizer) REST client library for
JavaScript - version 1.0.0-beta.3
Article · 08/21/2024
Extracts content, layout, and structured data from documents.
Please rely heavily on our REST client docs to use this library
NOTE: Form Recognizer has been rebranded to Document Intelligence. Please check the
Migration Guide from @azure/ai-form-recognizer to @azure-rest/ai-document-
intelligence .
Key links:
· Source code
· Package (NPM) z
· API reference documentation
· Samples zz
· Changelog
· Migration Guide from Form Recognizer
This version of the client library defaults to the "2024-07-31-preview" version of the service.
This table shows the relationship between SDK versions and supported API versions of the
service:
0
Expand table
SDK version
Supported API version of service
1.0.0-beta.3
2024-07-31-preview
1.0.0-beta.2
2024-02-29-preview
1.0.0-beta.1
2023-10-31-preview
Please rely on the older @azure/ai-form-recognizer library through the older service API
versions for retired models, such as "prebuilt-businessCard" and "prebuilt-document" .
For more information, see Changelog .

| SDK version | Supported API version of service |
| --- | --- |
| 1.0.0-beta.3 | 2024-07-31-preview |
| 1.0.0-beta.2 | 2024-02-29-preview |
| 1.0.0-beta.1 | 2023-10-31-preview |


<!---- Page 638 ---------------------------------------------------------------------------------------------------------------------------------->
The below table describes the relationship of each client and its supported API version(s):
" Expand table
Service API
version
Supported clients
Package
2024-07-31-
preview
DocumentIntelligenceClient
@azure-rest/ai-document-
intelligence version 1.0.0-beta.3
2024-02-29-
preview
DocumentIntelligenceClient
@azure-rest/ai-document-
intelligence version 1.0.0-beta.2
2023-10-31-
preview
DocumentIntelligenceClient
@azure-rest/ai-document-
intelligence version 1.0.0-beta.1
2023-07-31
DocumentAnalysisClient and
@azure/ai-form-recognizer version
DocumentModelAdministrationClient
^5.0.0
2022-08-01
DocumentAnalysisClient and
DocumentModelAdministrationClient
@azure/ai-form-recognizer version
^4.0.0
Getting started
Currently supported environments
· LTS versions of Node.js
Prerequisites
. You must have an Azure subscription to use this package.
Install the @azure-rest/ai-document-intelligence package
Install the Azure DocumentIntelligence(formerlyFormRecognizer) REST client REST client library
for JavaScript with npm:
Bash
npm install @azure-rest/ai-document-intelligence
Create and authenticate a DocumentIntelligenceClient

| Service API version | Supported clients | Package |
| --- | --- | --- |
| 2024-07-31- preview | DocumentIntelligenceClient | @azure-rest/ai-document- |
|  |  | intelligence version 1.0.0-beta.3 |
| 2024-02-29- preview | DocumentIntelligenceClient | @azure-rest/ai-document- |
|  |  | intelligence version 1.0.0-beta.2 |
| 2023-10-31- preview | DocumentIntelligenceClient | @azure-rest/ai-document- |
|  |  | intelligence version 1.0.0-beta.1 |
| 2023-07-31 | DocumentAnalysisClient and | @azure/ai-form-recognizer version |
|  | DocumentModelAdministrationClient | ^5.0.0 |
| 2022-08-01 | DocumentAnalysisClient and DocumentModelAdministrationClient | @azure/ai-form-recognizer version |
|  |  | ^4.0.0 |


<!---- Page 639 ---------------------------------------------------------------------------------------------------------------------------------->
To use an Azure Active Directory (AAD) token credential , provide an instance of the desired
credential type obtained from the @azure/identity library.
To authenticate with AAD, you must first npm install @azure/identity
After setup, you can choose which type of credential z from @azure/identity to use. As an
example, DefaultAzureCredential can be used to authenticate the client.
Set the values of the client ID, tenant ID, and client secret of the AAD application as
environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
Using a Token Credential
ts
import DocumentIntelligence from "@azure-rest/ai-document-intelligence";
const client = DocumentIntelligence(
process. env[ "DOCUMENT_INTELLIGENCE_ENDPOINT"],
new DefaultAzureCredential(),
);
Using an API KEY
ts
import DocumentIntelligence from "@azure-rest/ai-document-intelligence";
const client = DocumentIntelligence(process.env[ "DOCUMENT_INTELLIGENCE_ENDPOINT"],
{
key: process. env[ "DOCUMENT_INTELLIGENCE_API_KEY"],
});
Document Models
Analyze prebuilt-layout (urlSource)
ts
const initialResponse = await client
. path("/documentModels/ {modelId} : analyze", "prebuilt-layout")
. post ( {
contentType: "application/json",
body: {


<!---- Page 640 ---------------------------------------------------------------------------------------------------------------------------------->
urlSource:
"https: //raw. githubusercontent. com/Azure/azure-sdk-for-
js/6704eff082aaaf2d97c1371a28461f512f8d748a/sdk/formrecognizer/ai-form-
recognizer/assets/forms/Invoice_1.pdf",
},
queryParameters: { locale: "en-IN" },
});
Analyze prebuilt-layout (base64Source)
ts
import fs from "fs";
import path from "path";
const filePath = path. join(ASSET_PATH, "forms", "Invoice_1.pdf");
const base64Source = fs.readFileSync(filePath, { encoding: "base64" });
const initialResponse = await client
. path("/documentModels/{modelId} : analyze", "prebuilt-layout")
. post ({
contentType: "application/json",
body: {
base64Source,
},
queryParameters: { locale: "en-IN" },
});
Continue creating the poller from initial response
ts
import {
getLongRunningPoller,
AnalyzeResultOperationOutput,
isUnexpected,
} from "@azure-rest/ai-document-intelligence";
if (isUnexpected(initialResponse)) {
throw initialResponse. body. error;
}
const poller = await getLongRunningPoller(client, initialResponse);
const result = (await poller.pollUntilDone()). body as
AnalyzeResultOperationOutput;
console. log(result);
// {
status: 'succeeded',
createdDateTime: '2023-11-10T13: 31:31Z',
lastUpdatedDateTime: '2023-11-10T13: 31: 34Z',
analyzeResult: {
apiVersion: '2023-10-31-preview',
.


<!---- Page 641 ---------------------------------------------------------------------------------------------------------------------------------->
.
.
contentFormat: 'text'
}
// }
Markdown content format
Supports output with Markdown content format along with the default plain text. For now, this
is only supported for "prebuilt-layout". Markdown content format is deemed a more friendly
format for LLM consumption in a chat or automation use scenario.
Service follows the GFM spec (GitHub Flavored Markdown ) for the Markdown format. Also
introduces a new contentFormat property with value "text" or "markdown" to indicate the result
content format.
ts
import DocumentIntelligence from "@azure-rest/ai-document-intelligence";
const client = DocumentIntelligence(process.env[ "DOCUMENT_INTELLIGENCE_ENDPOINT"],
{
key: process. env[ "DOCUMENT_INTELLIGENCE_API_KEY"],
});
const initialResponse = await client
. path("/documentModels/ {modelId} : analyze", "prebuilt-layout")
. post ({
contentType: "application/json",
body: {
urlSource:
"https://raw. githubusercontent. com/Azure/azure-sdk-for-
js/6704eff082aaaf2d97c1371a28461f512f8d748a/sdk/formrecognizer/ai-form-
recognizer/assets/forms/Invoice_1.pdf",
},
queryParameters: { outputContentFormat : "markdown" }, // <-- new query
parameter
});
Query Fields
When this feature flag is specified, the service will further extract the values of the fields
specified via the queryFields query parameter to supplement any existing fields defined by the
model as fallback.
ts


<!---- Page 642 ---------------------------------------------------------------------------------------------------------------------------------->
await client. path("/documentModels/{modelId} : analyze", "prebuilt-layout") .post({
contentType: "application/json",
body: { urlSource: " ... " },
queryParameters: {
features: ["queryFields"],
queryFields: [ "NumberOfGuests", "StoreNumber"],
}, // <-- new query parameter
});
Split Options
In the previous API versions supported by the older @azure/ai-form-recognizer library,
document splitting and classification operation
("/documentClassifiers/{classifierId} : analyze") always tried to split the input file into
multiple documents.
To enable a wider set of scenarios, service introduces a "split" query parameter with the new
"2023-10-31-preview" service version. The following values are supported:
· split: "auto"
Let service determine where to split.
· split: "none"
The entire file is treated as a single document. No splitting is performed.
· split: "perPage"
Each page is treated as a separate document. Each empty page is kept as its own
document.
Document Classifiers #Build
ts
import {
DocumentClassifierBuildOperationDetailsOutput,
getLongRunningPoller,
isUnexpected,
} from "@azure-rest/ai-document-intelligence";
const containerSasUrl = (): string =>
process. env[ "DOCUMENT_INTELLIGENCE_TRAINING_CONTAINER_SAS_URL"];
const initialResponse = await client. path("/documentClassifiers : build") . post({
body: {


<!---- Page 643 ---------------------------------------------------------------------------------------------------------------------------------->
classifierId: 'customClassifier${getRandomNumber()}`,
description: "Custom classifier description",
docTypes: {
foo: {
azureBlobSource: {
containerUrl: containerSasUrl(),
},
},
bar: {
azureBlobSource: {
containerUrl: containerSasUrl(),
},
},
},
},
});
if (isUnexpected(initialResponse)) {
throw initialResponse. body. error;
}
const poller = await getLongRunningPoller(client, initialResponse);
const response = (await poller.pollUntilDone())
.body as DocumentClassifierBuildOperationDetailsOutput;
console. log(response);
{
operationId: '31466834048_f3ee629e-73fb-48ab-993b-1d55d73ca460',
kind: 'documentClassifierBuild',
status: 'succeeded',
.
.
result: {
classifierId: 'customClassifier10978',
createdDateTime: '2023-11-09T12:45:56Z',
.
.
description: 'Custom classifier description'
},
apiVersion: '2023-10-31-preview'
}
Get Info
ts
const response = await client.path("/info").get();
if (isUnexpected(response)) {
throw response. body. error;
}
console. log(response.body.customDocumentModels. limit);
// 20000


<!---- Page 644 ---------------------------------------------------------------------------------------------------------------------------------->
List Document Models
ts
import { paginate } from "@azure-rest/ai-document-intelligence";
const response = await client.path("/documentModels").get();
if (isUnexpected(response)) {
throw response. body. error;
}
const modelsInAccount: string[] = [];
for await (const model of paginate(client, response)) {
console.log(model.modelId);
}
Troubleshooting
Logging
Enabling logging may help uncover useful information about failures. In order to see a log of
HTTP requests and responses, set the AZURE_LOG_LEVEL environment variable to info.
Alternatively, logging can be enabled at runtime by calling setLogLevel in the @azure/logger:
JavaScript
const { setLogLevel } = require("@azure/logger");
setLogLevel("info");
For more detailed instructions on how to enable logs, you can look at the @azure/logger
package docs [ .


<!---- Page 645 ---------------------------------------------------------------------------------------------------------------------------------->
Azure AI Document Intelligence client
library for Python - version 1.0.0b4
Article · 09/06/2024
Azure AI Document Intelligence (previously known as Form Recognizer ) is a cloud
service that uses machine learning to analyze text and structured data from your
documents. It includes the following main features:
. Layout - Extract content and structure (ex. words, selection marks, tables) from
documents.
· Document - Analyze key-value pairs in addition to general layout from documents.
· Read - Read page information from documents.
· Prebuilt - Extract common field values from select document types (ex. receipts,
invoices, business cards, ID documents, U.S. W-2 tax documents, among others)
using prebuilt models.
. Custom - Build custom models from your own data to extract tailored field values
in addition to general layout from documents.
· Classifiers - Build custom classification models that combine layout and language
features to accurately detect and identify documents you process within your
application.
· Add-on capabilities - Extract barcodes/QR codes, formulas, font/style, etc. or
enable high resolution mode for large documents with optional parameters.
Source code [ | Package (PyPl) [^ | API reference documentation z | Product
documentation | Samples ₹
Disclaimer
The latest service API is currently only available in some Azure regions, the available
regions can be found from here z.
Getting started
Installating the package
Bash
python -m pip install azure-ai-documentintelligence


<!---- Page 646 ---------------------------------------------------------------------------------------------------------------------------------->
This table shows the relationship between SDK versions and supported API service
versions:
[] Expand table
SDK version
Supported API service version
1.0.0b1
2023-10-31-preview
1.0.0b2
2024-02-29-preview
Older API versions are supported in azure-ai-formrecognizer, please see the Migration
Guide for detailed instructions on how to update application.
Prequisites
· Python 3.8 or later is required to use this package.
. You need an Azure subscription to use this package.
· An existing Azure Al Document Intelligence instance.
Create a Cognitive Services or Document Intelligence resource
Document Intelligence supports both multi-service and single-service access . Create a
Cognitive Services resource if you plan to access multiple cognitive services under a
single endpoint/key. For Document Intelligence access only, create a Document
Intelligence resource. Please note that you will need a single-service resource if you
intend to use Azure Active Directory authentication.
You can create either resource using:
· Option 1: Azure Portal .
· Option 2: Azure CLI.
Below is an example of how you can create a Document Intelligence resource using the
CLI:
PowerShell
# Create a new resource group to hold the Document Intelligence resource
# if using an existing resource group, skip this step
az group create -- name <your-resource-name> -- location <location>
PowerShell

| SDK version | Supported API service version |
| --- | --- |
| 1.0.0b1 | 2023-10-31-preview |
| 1.0.0b2 | 2024-02-29-preview |


<!---- Page 647 ---------------------------------------------------------------------------------------------------------------------------------->
# Create the Document Intelligence resource
az cognitiveservices account create \
-- name <your-resource-name> \
-- resource-group <your-resource-group-name> \
-- kind FormRecognizer \
-- sku ‹sku> \
-- location <location> \
-- yes
For more information about creating the resource or how to get the location and sku
information see here.
Authenticate the client
In order to interact with the Document Intelligence service, you will need to create an
instance of a client. An endpoint and credential are necessary to instantiate the client
object.
Get the endpoint
You can find the endpoint for your Document Intelligence resource using the Azure
Portal or Azure CLI:
Bash
# Get the endpoint for the Document Intelligence resource
az cognitiveservices account show -- name "resource-name" -- resource-group
"resource-group-name" -- query "properties. endpoint"
Either a regional endpoint or a custom subdomain can be used for authentication. They
are formatted as follows:
Regional endpoint: https://<region>.api.cognitive.microsoft.com/
Custom subdomain: https://<resource-name>.cognitiveservices.azure.com/
A regional endpoint is the same for every resource in a region. A complete list of
supported regional endpoints can be consulted here . Please note that regional
endpoints do not support AAD authentication.
A custom subdomain, on the other hand, is a name that is unique to the Document
Intelligence resource. They can only be used by single-service resources .


<!---- Page 648 ---------------------------------------------------------------------------------------------------------------------------------->
Get the API key
The API key can be found in the Azure Portal E or by running the following Azure CLI
command:
Bash
az cognitiveservices account keys list -- name "<resource-name>" -- resource-
group "<resource-group-name>"
Create the client with AzureKeyCredential
To use an API key as the credential parameter, pass the key as a string into an instance
of AzureKeyCredential .
Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
endpoint = "https://<my-custom-subdomain>.cognitiveservices.azure.com/"
credential = AzureKeyCredential("<api_key>")
document_intelligence_client = DocumentIntelligenceClient(endpoint,
credential)
Create the client with an Azure Active Directory credential
AzureKeyCredential authentication is used in the examples in this getting started guide,
but you can also authenticate with Azure Active Directory using the azure-identity
library. Note that regional endpoints do not support AAD authentication. Create a
custom subdomain name for your resource in order to use this type of authentication.
To use the DefaultAzureCredential E type shown below, or other credential types
provided with the Azure SDK, please install the azure-identity package:
pip install azure-identity
You will also need to register a new AAD application and grant access to Document
Intelligence by assigning the "Cognitive Services User" role to your service principal.
Once completed, set the values of the client ID, tenant ID, and client secret of the AAD
application as environment variables: AZURE_CLIENT_ID, AZURE_TENANT_ID,
AZURE_CLIENT_SECRET .


<!---- Page 649 ---------------------------------------------------------------------------------------------------------------------------------->
Python
"""DefaultAzureCredential will use the values from these environment
variables: AZURE_CLIENT_ID, AZURE_TENANT_ID, AZURE_CLIENT_SECRET
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.identity import DefaultAzureCredential
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
credential = DefaultAzureCredential()
document_intelligence_client = DocumentIntelligenceClient(endpoint,
credential)
Key concepts
DocumentIntelligenceClient
DocumentIntelligenceClient provides operations for analyzing input documents using
prebuilt and custom models through the begin_analyze_document API. Use the model_id
parameter to select the type of model for analysis. See a full list of supported models
here . The DocumentIntelligenceClient also provides operations for classifying
documents through the begin_classify_document API. Custom classification models can
classify each page in an input file to identify the document(s) within and can also
identify multiple documents or multiple instances of a single document within an input
file.
Sample code snippets are provided to illustrate using a DocumentIntelligenceClient
here. More information about analyzing documents, including supported features,
locales, and document types can be found in the service documentation .
DocumentIntelligenceAdministrationClient
DocumentIntelligenceAdministrationClient provides operations for:
· Building custom models to analyze specific fields you specify by labeling your
custom documents. A DocumentModelDetails is returned indicating the document
type(s) the model can analyze, as well as the estimated confidence for each field.
See the service documentation for a more detailed explanation.
· Creating a composed model from a collection of existing models.
· Managing models created in your account.


<!---- Page 650 ---------------------------------------------------------------------------------------------------------------------------------->
· Listing operations or getting a specific model operation created within the last 24
hours.
· Copying a custom model from one Document Intelligence resource to another.
. Build and manage a custom classification model to classify the documents you
process within your application.
Please note that models can also be built using a graphical user interface such as
Document Intelligence Studio 2.
Sample code snippets are provided to illustrate using a
DocumentIntelligenceAdministrationClient here.
Long-running operations
Long-running operations are operations which consist of an initial request sent to the
service to start an operation, followed by polling the service at intervals to determine
whether the operation has completed or failed, and if it has succeeded, to get the result.
Methods that analyze documents, build models, or copy/compose models are modeled
as long-running operations. The client exposes a begin _< method-name> method that
returns an LROPoller or AsyncLROPoller. Callers should wait for the operation to
complete by calling result() on the poller object returned from the begin _< method-
name> method. Sample code snippets are provided to illustrate using long-running
operations below.
Examples
The following section provides several code snippets covering some of the most
common Document Intelligence tasks, including:
· Extract Layout
. Extract Figures from Documents
· Analyze Documents Result in PDF
. Using the General Document Model
· Using Prebuilt Models
· Build a Custom Model
· Analyze Documents Using a Custom Model
· Manage Your Models
· Add-on capabilities
Extract Layout


<!---- Page 651 ---------------------------------------------------------------------------------------------------------------------------------->
Extract text, selection marks, text styles, and table structures, along with their bounding
region coordinates, from documents.
Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
def _in_span(word, spans) :
for span in spans:
if word.span.offset >= span.offset and (word.span.offset +
word.span.length) <= (span.offset + span. length) :
return True
return False
def _format_polygon (polygon) :
if not polygon:
return "N/A"
return ", ". join([f"[ {polygon[i]}, {polygon[i + 1]}]" for i in range(0,
len(polygon), 2)])
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
with open(path_to_sample_documents, "rb") as f:
poller = document_intelligence_client. begin_analyze_document (
"prebuilt-layout", analyze_request=f,
content_type="application/octet-stream"
)
result: AnalyzeResult = poller.result()
if result.styles and any([style.is_handwritten for style in result. styles]) :
print("Document contains handwritten content")
else:
print("Document does not contain handwritten content")
for page in result.pages:
print(f" ---- Analyzing layout from page # {page. page_number} ---- ")
print(f"Page has width: {page.width} and height: {page. height}, measured
with unit: {page. unit}")
if page. lines:
for line_idx, line in enumerate(page.lines) :
words = []
if page.words:
for word in page. words:
print(f" ..... . Word ' {word. content}' has a confidence of
{word. confidence}")
if _in_span(word, line. spans) :
words . append (word)
print (


<!---- Page 652 ---------------------------------------------------------------------------------------------------------------------------------->
f" ... Line # {line_idx} has word count {len(words)} and text
'{line. content} ' "
f"within bounding polygon '{_format_polygon (line . polygon) } '"
)
if page. selection_marks:
for selection_mark in page. selection_marks:
print(
f"Selection mark is ' {selection_mark. state}' within bounding
polygon "
f"'{_format_polygon(selection_mark. polygon)}' and has a
confidence of {selection_mark. confidence}"
)
if result.paragraphs:
print(f" ---- Detected #{len(result.paragraphs)} paragraphs in the
document ---- ")
# Sort all paragraphs by span's offset to read in the right order.
result.paragraphs.sort(key=lambda p: (p.spans.sort(key=lambda s:
s. offset), p. spans[0].offset))
print(" ----- Print sorted paragraphs ----- ")
for paragraph in result.paragraphs:
if not paragraph. bounding_regions:
print(f"Found paragraph with role: ' {paragraph.role}' within N/A
bounding region")
else:
print(f"Found paragraph with role: '{paragraph. role}' within")
print(
", ". join(
f" Page #{region. page_number} :
{_format_polygon(region.polygon)} bounding region"
for region in paragraph. bounding_regions
)
)
print(f" ... with content: ' {paragraph. content} '")
print(f" ... with offset: {paragraph. spans[0].offset} and length:
{paragraph . spans [0] . length} ")
if result. tables:
for table_idx, table in enumerate(result. tables) :
print(f"Table # {table_idx} has {table. row_count} rows and " f"
{table. column_count} columns")
if table. bounding_regions:
for region in table. bounding_regions:
print (
f"Table # {table_idx} location on page:
{region. page_number} is {_format_polygon(region.polygon) }"
)
for cell in table. cells:
print(f" . . . Cell[ {cell.row_index} ] [ {cell.column_index} ] has text
'{cell. content} '")
if cell. bounding_regions:
for region in cell. bounding_regions:
print(
f" ... content on page {region. page_number} is within


<!---- Page 653 ---------------------------------------------------------------------------------------------------------------------------------->
bounding polygon '{_format_polygon (region . polygon) } ' "
)
print("
")
Extract Figures from Documents
Extract figures from the document as cropped images.
Python
from azure. core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeOutputOption,
AnalyzeResult
endpoint = os.environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
with open(path_to_sample_documents, "rb") as f:
poller = document_intelligence_client. begin_analyze_document (
"prebuilt-layout",
analyze_request=f,
output=[AnalyzeOutputOption. FIGURES],
content_type="application/octet-stream",
)
result: AnalyzeResult = poller.result()
operation_id = poller.details["operation_id"]
if result. figures:
for figure in result.figures:
if figure.id:
response =
document_intelligence_client. get_analyze_result_figure(
model_id=result.model_id, result_id=operation_id,
figure_id=figure.id
)
with open(f"{figure. id}. png", "wb") as writer:
writer.writelines (response)
else:
print("No figures found.")
Analyze Documents Result in PDF
Convert an analog PDF into a PDF with embedded text. Such text can enable text search
within the PDF or allow the PDF to be used in LLM chat scenarios.


<!---- Page 654 ---------------------------------------------------------------------------------------------------------------------------------->
Note: For now, this feature is only supported by prebuilt-read. All other models will
return error.
Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeOutputOption,
AnalyzeResult
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
with open(path_to_sample_documents, "rb") as f:
poller = document_intelligence_client.begin_analyze_document (
"prebuilt-read",
analyze_request=f,
output=[AnalyzeOutputOption. PDF],
content_type="application/octet-stream",
)
result: AnalyzeResult = poller.result()
operation_id = poller.details["operation_id"]
response =
document_intelligence_client. get_analyze_result_pdf(model_id=result.model_id
, result_id=operation_id)
with open("analyze_result.pdf", "wb") as writer:
writer.writelines (response)
Using the General Document Model
Analyze key-value pairs, tables, styles, and selection marks from documents using the
general document model provided by the Document Intelligence service. Select the
General Document Model by passing model_id="prebuilt-document" into the
begin_analyze_document method:
Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure. ai.documentintelligence.models import DocumentAnalysisFeature,
AnalyzeResult
def _in_span(word, spans) :
for span in spans:
if word. span.offset >= span.offset and (word. span.offset +


<!---- Page 655 ---------------------------------------------------------------------------------------------------------------------------------->
word. span.length) <= (span.offset + span. length) :
return True
return False
def _format_bounding_region(bounding_regions) :
if not bounding_regions:
return "N/A"
return ", ". join(
f"Page #{region.page_number}: {_format_polygon(region. polygon)}" for
region in bounding_regions
)
def _format_polygon (polygon) :
if not polygon:
return "N/A"
return ", ". join([f"[ {polygon[i]}, {polygon[i + 1]}]" for i in range(0,
len(polygon), 2)])
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
with open(path_to_sample_documents, "rb") as f:
poller = document_intelligence_client. begin_analyze_document (
"prebuilt-layout",
analyze_request=f,
features=[DocumentAnalysisFeature. KEY_VALUE_PAIRS],
content_type="application/octet-stream",
)
result: AnalyzeResult = poller.result()
if result.styles:
for style in result.styles:
if style. is_handwritten:
print("Document contains handwritten content: ")
print(",".join([result. content[span.offset : span.offset +
span. length] for span in style. spans ]))
print(" ---- Key-value pairs found in document ---- ")
if result.key_value_pairs:
for kv_pair in result. key_value_pairs:
if kv_pair.key:
print(
f"Key '{kv_pair. key. content} ' found within "
f"'{_format_bounding_region(kv_pair.key.bounding_regions)} '
bounding regions"
)
if kv_pair. value:
print (
f"Value '{kv_pair. value. content} ' found within "
f"'{_format_bounding_region(kv_pair.value. bounding_regions)} ' bounding
regions\n"
)


<!---- Page 656 ---------------------------------------------------------------------------------------------------------------------------------->
for page in result.pages:
print(f" ---- Analyzing document from page # {page. page_number} ---- ")
print(f"Page has width: {page.width} and height: {page.height}, measured
with unit: {page.unit}")
if page. lines:
for line_idx, line in enumerate(page.lines) :
words = []
if page.words:
for word in page.words:
print(f" ...... Word '{word. content}' has a confidence of
{word. confidence}")
if _in_span(word, line.spans) :
words . append (word)
print(
f" ... Line #{line_idx} has {len(words)} words and text
'{line. content} ' within "
f"bounding polygon '{_format_polygon(line. polygon) } '"
)
if page. selection_marks:
for selection_mark in page. selection_marks:
print (
f"Selection mark is ' {selection_mark. state}' within bounding
polygon "
f"'{_format_polygon(selection_mark. polygon)}' and has a
confidence of "
f"{selection_mark.confidence}"
)
if result.tables:
for table_idx, table in enumerate(result. tables) :
print(f"Table # {table_idx} has {table. row_count } rows and
{table. column_count} columns")
if table. bounding_regions:
for region in table. bounding_regions:
print (
f"Table # {table_idx} location on page:
{region. page_number} is {_format_polygon(region.polygon)}"
)
for cell in table.cells:
print(f" . . . Cell[ {cell.row_index} ] [{cell. column_index} ] has text
'{cell. content} '")
if cell. bounding_regions:
for region in cell. bounding_regions:
print(
f" ... content on page {region. page_number} is within
bounding polygon '{_format_polygon(region. polygon) } '\n"
)
print("
)
U
. Read more about the features provided by the prebuilt-document model here.


<!---- Page 657 ---------------------------------------------------------------------------------------------------------------------------------->
Using Prebuilt Models
Extract fields from select document types such as receipts, invoices, business cards,
identity documents, and U.S. W-2 tax documents using prebuilt models provided by the
Document Intelligence service.
For example, to analyze fields from a sales receipt, use the prebuilt receipt model
provided by passing model_id="prebuilt-receipt" into the begin_analyze_document
method:
Python
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
def _format_price(price_dict) :
return "". join([f"{p}" for p in price_dict. values()])
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
with open(path_to_sample_documents, "rb") as f:
poller = document_intelligence_client. begin_analyze_document (
"prebuilt-receipt", analyze_request=f, locale="en-US",
content_type="application/octet-stream"
)
receipts: AnalyzeResult = poller.result()
if receipts. documents :
for idx, receipt in enumerate(receipts.documents) :
print(f" -------- Analysis of receipt #{idx + 1} -------- ")
print(f"Receipt type: {receipt. doc_type if receipt.doc_type else
'N/A'}")
if receipt.fields:
merchant_name = receipt. fields . get ( "MerchantName")
if merchant_name:
print (
f"Merchant Name: {merchant_name. get ( 'valueString' )} has
confidence: "
f"{merchant_name.confidence}"
)
transaction_date = receipt. fields . get ( "TransactionDate")
if transaction_date:
print(
f"Transaction Date: {transaction_date. get ( 'valueDate' ) }
has confidence: "
f"{transaction_date. confidence}"
)
items = receipt.fields. get("Items")


<!---- Page 658 ---------------------------------------------------------------------------------------------------------------------------------->
if items:
print ("Receipt items:")
for idx, item in enumerate(items.get("valueArray")):
print(f" ... Item #{idx + 1}")
item_description =
item.get ("valueObject") .get ("Description")
if item_description:
print (
f" ...... Item Description:
{item_description. get ('valueString' )} has confidence: "
f"{item_description. confidence}"
)
item_quantity = item. get("valueObject") . get ("Quantity")
if item_quantity:
print (
f" ...... Item Quantity:
{item_quantity . get ('valueString' )} has confidence: "
f"{item_quantity. confidence}"
"
)
item_total_price =
item.get("valueObject") . get("TotalPrice")
if item_total_price:
print (
f" ...... Total Item Price:
{_format_price(item_total_price.get('valueCurrency' ) )} has confidence: "
f"{item_total_price. confidence}"
)
subtotal = receipt. fields. get("Subtotal")
if subtotal:
print(
f"Subtotal:
{_format_price(subtotal. get('valueCurrency' ) )} has confidence:
{subtotal. confidence}"
)
tax = receipt. fields. get("TotalTax")
if tax:
print(f"Total tax: {_format_price(tax. get( 'valueCurrency' ) ) }
has confidence: {tax. confidence}")
tip = receipt.fields.get("Tip")
if tip:
print(f"Tip: {_format_price(tip. get('valueCurrency' ) )} has
confidence: {tip. confidence}")
total = receipt. fields . get("Total")
if total:
print(f"Total: {_format_price(total. get('valueCurrency' ) ) }
has confidence: {total. confidence}")
print("
)
You are not limited to receipts! There are a few prebuilt models to choose from, each of
which has its own set of supported fields. See other supported prebuilt models here .
Build a Custom Model


<!---- Page 659 ---------------------------------------------------------------------------------------------------------------------------------->
Build a custom model on your own document type. The resulting model can be used to
analyze values from the types of documents it was trained on. Provide a container SAS
URL to your Azure Storage Blob container where you're storing the training documents.
More details on setting up a container and required file structure can be found in the
service documentation .
Python
# Let's build a model to use for this sample
import uuid
from azure.ai.documentintelligence import
DocumentIntelligenceAdministrationClient
from azure.ai.documentintelligence.models import (
DocumentBuildMode,
BuildDocumentModelRequest,
AzureBlobContentSource,
DocumentModelDetails,
from azure.core.credentials import AzureKeyCredential
)
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
container_sas_url =
os . environ [ "DOCUMENTINTELLIGENCE_STORAGE_CONTAINER_SAS_URL"]
document_intelligence_admin_client =
DocumentIntelligenceAdministrationClient(endpoint, AzureKeyCredential(key))
poller = document_intelligence_admin_client. begin_build_document_model(
BuildDocumentModelRequest (
model_id=str(uuid.uuid4()),
build_mode=DocumentBuildMode. TEMPLATE,
azure_blob_source=AzureBlobContentSource(container_url=container_sas_url),
description="my model description",
)
)
model: DocumentModelDetails = poller.result()
print(f"Model ID: {model.model_id}")
print(f"Description: {model. description}")
print(f"Model created on: {model. created_date_time}")
print(f"Model expires on: {model. expiration_date_time}")
if model.doc_types:
print("Doc types the model can recognize:")
for name, doc_type in model.doc_types.items():
print(f"Doc Type: '{name}' built with ' {doc_type. build_mode} ' mode
which has the following fields:")
if doc_type. field_schema:
for field_name, field in doc_type.field_schema. items():
if doc_type.field_confidence:
print(
f"Field: '{field_name} ' has type '{field[ 'type' ]} '


<!---- Page 660 ---------------------------------------------------------------------------------------------------------------------------------->
and confidence score "
f"{doc_type. field_confidence[field_name]}"
)
Analyze Documents Using a Custom Model
Analyze document fields, tables, selection marks, and more. These models are trained
with your own data, so they're tailored to your documents. For best results, you should
only analyze documents of the same document type that the custom model was built
with.
Python
from azure.core.credentials import AzureKeyCredential
from azure. ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
def _print_table(header_names, table_data) :
# Print a two-dimensional array like a table.
max_len_list = []
for i in range(len(header_names)):
col_values = list(map(lambda row: len(str(row[i])), table_data))
col_values.append(len(str(header_names[i])))
max_len_list. append (max(col_values))
row_format_str = "". join(map (lambda len: f"{{ :< {len + 4}} }",
max_len_list))
print(row_format_str. format(*header_names))
for row in table_data:
print(row_format_str.format(*row))
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
model_id = os. getenv("CUSTOM_BUILT_MODEL_ID", custom_model_id)
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
# Make sure your document's type is included in the list of document types
the custom model can analyze
with open(path_to_sample_documents, "rb") as f:
poller = document_intelligence_client.begin_analyze_document (
model_id=model_id, analyze_request=f,
content_type="application/octet-stream"
)
result: AnalyzeResult = poller.result()
if result.documents:
for idx, document in enumerate(result.documents) :
print(f" -------- Analyzing document #{idx + 1} -------- ")


<!---- Page 661 ---------------------------------------------------------------------------------------------------------------------------------->
print(f"Document has type {document. doc_type}")
print(f"Document has document type confidence
{document. confidence}")
print(f"Document was analyzed with model with ID {result.model_id}")
if document.fields:
for name, field in document.fields.items():
field_value = field.get("valueString") if
field. get("valueString") else field. content
print(
f" ...... found field of type ' {field. type}' with value
'{field_value}' and with confidence {field. confidence}"
)
# Extract table cell values
SYMBOL_OF_TABLE_TYPE = "array"
SYMBOL_OF_OBJECT_TYPE = "object"
KEY_OF_VALUE_OBJECT = "valueObject"
KEY_OF_CELL_CONTENT = "content"
for doc in result. documents:
if not doc.fields is None:
for field_name, field_value in doc.fields.items() :
# Dynamic Table cell information store as array in document
field.
if field_value. type == SYMBOL_OF_TABLE_TYPE and
field_value. value_array:
col_names = [ ]
sample_obj = field_value.value_array[0]
if KEY_OF_VALUE_OBJECT in sample_obj:
col_names =
list(sample_obj[KEY_OF_VALUE_OBJECT]. keys())
print(" ---- Extracting Dynamic Table Cell Values ---- ")
table_rows = [ ]
for obj in field_value.value_array:
if KEY_OF_VALUE_OBJECT in obj:
value_obj = obj[KEY_OF_VALUE_OBJECT]
extract_value_by_col_name = lambda key: (
value_obj[key]. get (KEY_OF_CELL_CONTENT)
if key in value_obj and KEY_OF_CELL_CONTENT
in value_obj[key]
else "None"
)
row_data = list(map(extract_value_by_col_name,
col_names))
table_rows . append (row_data)
_print_table(col_names, table_rows)
elif (
field_value. type == SYMBOL_OF_OBJECT_TYPE
and KEY_OF_VALUE_OBJECT in field_value
and field_value [KEY_OF_VALUE_OBJECT] is not None
):
rows_by_columns =
list(field_value[KEY_OF_VALUE_OBJECT].values())
is_fixed_table = all(


<!---- Page 662 ---------------------------------------------------------------------------------------------------------------------------------->
(
rows_of_column["type"] == SYMBOL_OF_OBJECT_TYPE
and Counter(list(rows_by_columns [0]
[KEY_OF_VALUE_OBJECT]. keys()))
==
Counter(list(rows_of_column[KEY_OF_VALUE_OBJECT]. keys()))
)
for rows_of_column in rows_by_columns
)
# Fixed Table cell information store as object in
document field.
if is_fixed_table:
print(" ---- Extracting Fixed Table Cell Values ---- ")
col_names =
list(field_value[KEY_OF_VALUE_OBJECT]. keys())
row_dict: dict = {}
for rows_of_column in rows_by_columns:
rows = rows_of_column [KEY_OF_VALUE_OBJECT]
for row_key in list(rows. keys()):
if row_key in row_dict:
row_dict[row_key].append(rows[row_key]. get (KEY_OF_CELL_CONTENT))
else:
row_dict[row_key] = [
row_key,
rows [row_key]. get (KEY_OF_CELL_CONTENT),
]
col_names.insert(0, "")
_print_table(col_names, list(row_dict. values()))
print("
")
Additionally, a document URL can also be used to analyze documents using the
begin_analyze_document method.
Python
from azure.core. credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeDocumentRequest,
AnalyzeResult
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint,
credential=AzureKeyCredential(key))
url = "https://raw. githubusercontent. com/Azure/azure-sdk-for-
python/main/sdk/documentintelligence/azure-ai-
documentintelligence/samples/sample_forms/receipt/contoso-receipt.png"


<!---- Page 663 ---------------------------------------------------------------------------------------------------------------------------------->
poller = document_intelligence_client. begin_analyze_document (
"prebuilt-receipt", AnalyzeDocumentRequest(url_source=url)
)
receipts: AnalyzeResult = poller.result()
Manage Your Models
Manage the custom models attached to your account.
Python
# Let's build a model to use for this sample
import uuid
from azure. ai. documentintelligence import
DocumentIntelligenceAdministrationClient
from azure.ai.documentintelligence.models import (
DocumentBuildMode,
BuildDocumentModelRequest,
AzureBlobContentSource,
DocumentModelDetails,
)
from azure.core.credentials import AzureKeyCredential
endpoint = os. environ[ "DOCUMENTINTELLIGENCE_ENDPOINT"]
key = os. environ[ "DOCUMENTINTELLIGENCE_API_KEY"]
container_sas_url =
os. environ[ "DOCUMENTINTELLIGENCE_STORAGE_CONTAINER_SAS_URL"]
document_intelligence_admin_client =
DocumentIntelligenceAdministrationClient(endpoint, AzureKeyCredential(key))
poller = document_intelligence_admin_client.begin_build_document_model(
BuildDocumentModelRequest (
model_id=str(uuid.uuid4()),
build_mode=DocumentBuildMode. TEMPLATE,
azure_blob_source=AzureBlobContentSource(container_url=container_sas_url),
description="my model description",
)
)
model: DocumentModelDetails = poller.result()
print(f"Model ID: {model.model_id}")
print(f"Description: {model. description}")
print(f"Model created on: {model.created_date_time}")
print(f"Model expires on: {model. expiration_date_time}")
if model.doc_types:
print("Doc types the model can recognize:")
for name, doc_type in model.doc_types.items():
print(f"Doc Type: '{name}' built with ' {doc_type. build_mode}' mode
which has the following fields:")
if doc_type. field_schema:
for field_name, field in doc_type.field_schema.items():


<!---- Page 664 ---------------------------------------------------------------------------------------------------------------------------------->
if doc_type. field_confidence:
print (
f"Field: '{field_name}' has type '{field['type' ]}'
and confidence score "
f"{doc_type.field_confidence[field_name]}"
)
Python
account_details = document_intelligence_admin_client. get_resource_info()
print(
f"Our resource has {account_details. custom_document_models. count} custom
models, "
f"and we can have at most {account_details. custom_document_models. limit}
custom models"
)
Python
# Next, we get a paged list of all of our custom models
models = document_intelligence_admin_client. list_models()
print("We have the following 'ready' models with IDs and descriptions:")
for model in models:
print(f"{model.model_id} | {model.description}")
Python
my_model =
document_intelligence_admin_client. get_model(model_id=model.model_id)
print(f"\nModel ID: {my_model. model_id}")
print(f"Description: {my_model.description}")
print(f"Model created on: {my_model. created_date_time}")
print(f"Model expires on: {my_model. expiration_date_time}")
if my_model.warnings:
print("Warnings encountered while building the model:")
for warning in my_model. warnings:
print (f"warning code: {warning. code}, message: {warning. message},
target of the error: {warning. target}")
Python
# Finally, we will delete this model by ID
document_intelligence_admin_client. delete_model(model_id=my_model.model_id)
from azure. core.exceptions import ResourceNotFoundError
try:
document_intelligence_admin_client.get_model(model_id=my_model.model_id)


<!---- Page 665 ---------------------------------------------------------------------------------------------------------------------------------->
except ResourceNotFoundError :
print(f"Successfully deleted model with ID {my_model. model_id}")
Add-on Capabilities
Document Intelligence supports more sophisticated analysis capabilities. These optional
features can be enabled and disabled depending on the scenario of the document
extraction.
The following add-on capabilities are available in this SDK:
· barcode/QR code
· formula z
· font/style
· high resolution mode z
· languagez
· query fields [
Note that some add-on capabilities will incur additional charges. See pricing:
https://azure.microsoft.com/pricing/details/ai-document-intelligence/ 2.
Troubleshooting
General
Document Intelligence client library will raise exceptions defined in Azure Core . Error
codes and messages raised by the Document Intelligence service can be found in the
service documentation .
Logging
This library uses the standard logging library for logging.
Basic information about HTTP sessions (URLs, headers, etc.) is logged at INFO level.
Detailed DEBUG level logging, including request/response bodies and unredacted
headers, can be enabled on the client or per-operation with the logging_enable
keyword argument.
See full SDK logging documentation with examples here.


<!---- Page 666 ---------------------------------------------------------------------------------------------------------------------------------->
Optional Configuration
Optional keyword arguments can be passed in at the client and per-operation level. The
azure-core reference documentation describes available configurations for retries,
logging, transport protocols, and more.
Next steps
More sample code
See the Sample README for several code snippets illustrating common patterns used
in the Document Intelligence Python API.
Additional documentation
For more extensive documentation on Azure AI Document Intelligence, see the
Document Intelligence documentation on docs.microsoft.com.
Contributing
This project welcomes contributions and suggestions. Most contributions require you to
agree to a Contributor License Agreement (CLA) declaring that you have the right to,
and actually do, grant us the rights to use your contribution. For details, visit
https://cla.microsoft.com.
When you submit a pull request, a CLA-bot will automatically determine whether you
need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply
follow the instructions provided by the bot. You will only need to do this once across all
repos using our CLA.
This project has adopted the Microsoft Open Source Code of Conduct . For more
information, see the Code of Conduct FAQ or contact opencode@microsoft.com with
any additional questions or comments.


<!---- Page 667 ---------------------------------------------------------------------------------------------------------------------------------->
Azure AI services support and help options
Article · 05/02/2025
Here are the options for getting support, staying up to date, giving feedback, and reporting
bugs for Azure AI services.
Get solutions to common issues
In the Azure portal, you can find answers to common AI service issues.
1. Go to your Azure AI services resource in the Azure portal. You can find it on the list on
this page: Azure Al services [2]. If you're a United States government customer, use the
Azure portal for the United States government.
2. In the left pane, under Help, select Support + Troubleshooting.
3. Describe your issue in the text box, and answer the remaining questions in the form.
4. You'll find Learn articles and other resources that might help you resolve your issue.
Create an Azure support request
A
Explore the range of Azure support options and choose the plant that best fits, whether
you're a developer just starting your cloud journey or a large organization deploying business-
critical, strategic applications. Azure customers can create and manage support requests in the
Azure portal.
To submit a support request for Azure AI services, follow the instructions on the New support
request page in the Azure portal. After choosing your Issue type, select Cognitive Services in
the Service type dropdown field.
Post a question on Microsoft Q&A
For quick and reliable answers on your technical product questions from Microsoft Engineers,
Azure Most Valuable Professionals (MVPs), or our expert community, engage with us on
Microsoft Q&A, Azure's preferred destination for community support.
If you can't find an answer to your problem using search, submit a new question to Microsoft
Q&A. Use one of the following tags when you ask your question:
· Azure Al services
Azure OpenAI


<!---- Page 668 ---------------------------------------------------------------------------------------------------------------------------------->
· Azure OpenAI
Vision
· Azure Al Vision
· Azure Al Custom Vision
· Azure Face
· Azure Al Document Intelligence
· Video Indexer
Language
· Azure Al Immersive Reader
· Language Understanding (LUIS)
· Azure QnA Maker
· Azure Al Language
· Azure Translator
Speech
· Azure Al Speech
Decision
· Azure Al Anomaly Detector
. Content Moderator
· Azure Al Metrics Advisor
· Azure Al Personalizer
Post a question to Stack Overflow
For answers to your developer questions from the largest community developer ecosystem, ask
your question on Stack Overflow.
If you submit a new question to Stack Overflow, use one or more of the following tags when
you create the question:
· Azure Al services
Azure OpenAI
· Azure OpenAI EZ
Vision


<!---- Page 669 ---------------------------------------------------------------------------------------------------------------------------------->
· Azure Al Vision
. Azure Al Custom Vision
. Azure Face
· Azure Al Document Intelligence
· Video Indexer Z
Language
. Azure Al Immersive Reader
· Language Understanding (LUIS) [
· Azure QnA Maker z
· Azure Al Language service
. Azure Translator Z
Speech
· Azure Al Speech service
Decision
. Azure Al Anomaly Detector
. Content Moderator
. Azure Al Metrics Advisor z
· Azure Al Personalizer
Submit feedback
To request new features, post them on https://feedback. azure. com. Share your ideas for
making Azure AI services and its APIs work better for the applications you develop.
. Azure Al services
Vision
· Azure Al Vision
. Azure Al Custom Vision
. Azure Face
. Azure Al Document Intelligence
· Video Indexer
Language
. Azure Al Immersive Reader
· Language Understanding (LUIS) [


<!---- Page 670 ---------------------------------------------------------------------------------------------------------------------------------->
· Azure QnA Maker
. Azure Al Language
. Azure Translator
Speech
· Azure Al Speech service
Decision
. Azure Al Anomaly Detector
. Content Moderator
. Azure Al Metrics Advisor
· Azure Al Personalizer
Stay informed
You can learn about the features in a new release or get the latest news on the Azure blog.
Staying informed can help you find the difference between a programming error, a service bug,
or a feature not yet available in Azure AI services.
· Learn more about product updates, roadmap, and announcements in Azure Updates .
. News about Azure Al services is shared in the Azure Al blog.
. Join the conversation on Reddit 4 about Azure Al services.
Next step
What are Azure AI services?


<!---- Page 671 ---------------------------------------------------------------------------------------------------------------------------------->
Azure AI Document Intelligence release
history
Article · 03/10/2025
Azure AI Document Intelligence is an innovative a cloud-based service that utilizes
machine learning to streamline data processing within applications and workflows. This
service is essential for boosting data-driven strategies and improving document search
functionalities. Here, discover key milestones and enhancements that have contributed
to the evolution of Azure AI Document Intelligence. For more information on recent
advances, see What's new ?.
July 2023
1 Note
Form Recognizer is now Azure AI Document Intelligence!
· No changes to pricing.
· The names Cognitive Services and Azure Applied Al continue to be used in
Azure billing, cost analysis, price list, and price APIs.
· No breaking changes to application programming interfaces (APIs) or client
libraries.
· Some platforms are still awaiting the renaming update. All mention of Form
Recognizer or Document Intelligence in our documentation refers to the same
Azure service.
Document Intelligence v3.1 (GA)
The Document Intelligence version 3.1 API is now generally available (GA)! The API
version corresponds to 2023-07-31. The v3.1 API introduces new and updated
capabilities:
· Document Intelligence APIs are now more modular and with support for optional
features. You can now customize the output to specifically include the features you
need. Learn more about the optional parameters.
· Document classification API for splitting a single file into individual documents.
Learn more about document classification.
. Prebuilt contract model.


<!---- Page 672 ---------------------------------------------------------------------------------------------------------------------------------->
· Prebuilt US tax form 1098 model.
· Support for Office file types with Read API.
· Barcode recognition in documents.
· Formula recognition add-on capability.
· Font recognition add-on capability.
· Support for high resolution documents.
· Custom neural models now require a single labeled sample to train.
· Custom neural models language expansion. Train a neural model for documents in
30 languages. See language support for the complete list of supported languages.
NEW
Prebuilt health insurance card model.
· Prebuilt invoice model locale expansion.
· Prebuilt receipt model language and locale expansion with more than 100
languages supported.
· Prebuilt ID model now supports European IDs.
Document Intelligence Studio UX Updates
V Analyze Options
· Document Intelligence now supports more sophisticated analysis capabilities and
the Studio allows one entry point (Analyze options button) for configuring the
add-on capabilities with ease.
· Depending on the document extraction scenario, configure the analysis range,
document page range, optional detection, and premium detection features.
Azure Al | Document Intelligence Studio
®
®
?
₹
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
×
Form Recognizer Studio > Layout
Layout
API version: 2023-07-31 (3.1 General Availability)
Service resource: forms-recognizer-eastus2-dev
0
CP
Run analysis
Analyze options
Content
Result
Code
Drag & drop file
here or
Browse for files
or
Fetch from URL
NEWS TODAY
-
Text
Selection marks
Tables
D
Latest news and bulletin updates
The scoop of the day
The hunt updates
Sample
layout-news.png
The scoop of the day
Sample
Run analysis on this document to see the results
$11
Run analysis
layout-report.png
Sample
The scoop of the day
The scoop of the day
The weop of the day
layout-checklist.jpg
<
of 1
Q Q # 2
Privacy & Cookies @ Microsoft 2022
!
Note


<!---- Page 673 ---------------------------------------------------------------------------------------------------------------------------------->
Font extraction isn't visualized in Document Intelligence Studio. However, you
can check the styles section of the JSON output for the font detection results.
V Auto labeling documents with prebuilt models or one of your own models
· In custom extraction model labeling page, you can now auto label your documents
using one of Document Intelligent Service prebuilt models or models you
previously trained.
Azure Al | Document Intelligence Studio
-
®
®
?
1
Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
₡
Form Recognizer Studio
Custom extraction model
Hello-Custom
Label data
Custom extraction model
Label data
Train
Hello-Custom
21 Run layout
V
Auto label
v
D
Draw region
O
0
0
+
Add a field
0
Label data
Drag & drop file
here or
Browse for files
Current document
+
Models
Unlabeled documents
LTD.
INVOICE
Test
P8TL
All documents
Contoso Headquarters
123 456th St
New York, NY, 10001
INVOICE: INV-100
#
Settings
invoice_2.pdf
INVOICE DATE: 11/15/2019
DUE DATE: 12/15/2019
sample.pdf
CUSTOMER NAME: MICROSOFT CORPORATION
SERVICE PERIOD: 10/14/2019 - 11/14/2019
read-ba.de pal
CUSTOMER ID: CID-12345
invoice.pdf
Microsoft Corp
123 Other St,
Redmond WA, 98052
layout.png
BILL TO:
Microsoft Finance
123 Bill St,
Redmond WA, 98052
SHIP TO:
Microsoft Delivery
123 Ship St,
Redmond WA, 98052
SERVICE ADDRESS:
Microsoft Services
123 Service St,
Redmond WA, 98052
THANK YOU FOR YOUR BUSINESS!
REMIT TO:
Contoso Billing
123 Remit St
New York, NY, 10001
< 1 of 1 >
Q Q . Q
Privacy & Cookies
Microsoft 202
ELE
SALESPERSON
P.O. NUMBER
REQUISITIONER
SHIPPED VIA
F.O.B. POINT
TERMS
PO-3333
III
DATE
ITEM CODE
DESCRIPTION
QTY
UM
PRICE
TAX
AMOUNT
3/4/2021
A123
Consulting Services
2
hours
$30.00
$6.00
$60.00
3/5/2021
B456
Document Fee
3
$10.00
$3.00
$30.00
3/6/2021
C789
Printing Fee
10
pages
$1.00
$1.00
$10.00
SUBTOTAL
$100.00
SALES TAX
$10.00
TOTAL
$110.00
PREVIOUS UNPAID BALANCE
$500.00
AMOUNT DUE
$610.00
· For some documents, there can be duplicate labels after running auto label. Make
sure to modify the labels so that there are no duplicate labels in the labeling page
afterwards.

| ELE | SALESPERSON | P.O. NUMBER | REQUISITIONER | SHIPPED VIA | F.O.B. POINT | TERMS |
| --- | --- | --- | --- | --- | --- | --- |
|  |  | PO-3333 |  |  |  |  |
|  |  |  |  |  |  |  |

| III DATE | ITEM CODE | DESCRIPTION | QTY | UM | PRICE | TAX | AMOUNT |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 3/4/2021 | A123 | Consulting Services | 2 | hours | $30.00 | $6.00 | $60.00 |
| 3/5/2021 | B456 | Document Fee | 3 |  | $10.00 | $3.00 | $30.00 |
| 3/6/2021 | C789 | Printing Fee | 10 | pages | $1.00 | $1.00 | $10.00 |

|  | SUBTOTAL | $100.00 |  |
| --- | --- | --- | --- |
|  | SALES TAX | $10.00 |  |
|  | TOTAL | $110.00 |  |
|  | PREVIOUS UNPAID BALANCE | $500.00 |  |
|  | AMOUNT DUE | $610.00 |  |


<!---- Page 674 ---------------------------------------------------------------------------------------------------------------------------------->
X
SHIP TO:
SOLD TO:
Duplicated labels
We found some labels present in multiple fields, which may lead to training failure.
Please review the labels listed below and remove any duplicates.
Labels
Related fields
New Belgium Brewery
CustomerAddressRecipient
CustomerName
Close
V Auto labeling tables
. In custom extraction model labeling page, you can now auto label the tables in the
document without having to label the tables manually.
Azure Al | Document Intelligence Studio
-
4
@
?
៛
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
×
ỐC
Form Recognizer Studio > Custom extraction model > Hello-Custom > Label data
Custom extraction model
Label data
Train
Hello-Custom
215 Run layout
V
Auto label
V
Draw region
O
0
ov
Add a field
Label data
Drag & drop file
here or
Browse for files
$55
Models
Liberty's Delightful Sinful Bakery & Café
765 Halifax St. Clearwater, FL 33756
3
Test
Our reference: 3456623
Your reference: 2334566
℮
Settings
invoice_2.pdf
Received from:
8556 Indian Summer Ave.
New Haven, CT 06511
Liberty booking contact
40 River Street
East Northport, NY 11731
sample.pdf
read-ba ... de.pdf
Name Summer River
Tel. 34456632
invoice pdf
E-mail email(@@@bertydelightfulsinful.com
layout.png
Booking Confirmation - ORIGINAL
Our reference:
3456623
Your reference:
2334566
05-Dec-2018
Booking date:
Contract No:
334566
BLINO:
EURH234
Summary:
45x72
Opt. A
Opt. B
Export:
Common
Export empty pick up depot(s)
Rows 3, Columns: 5
s Lane
Heights, MI 48310
TO
By
ETD
18 Queen Street
-2018
ETA
09-Dec-2020
Hoboken, NJ 07030
52 West Trenton St.
Harleysville, PA
cause science slow
19438
19:00
11:00
9 Ketch Harbour
75 Fawn Street
tone late spoken
12-Dec-2018
19-Dec-2020
Ave
Peabody, MA 01960
10:00
Vincentown, NJ
E
eadline
able
Location
Date/Time (local)
08-Dec-2019
Required action
Nobody loves a pig
Flight
Harleysville
(PA)
08-Dec-2019
Two more days and all his
Round
Harleysville
PA)
09-Dec-2019
problems would be solved
Accent
Harleysville
10-Dec-2019
Monkey
Harleysville
11-Dec-2019
Route
Harleysville
(PA)
11-Dec-2019
Peanuts don't grow on
trees
<
1
of 1 >
Q Q + Q
Privacy & Cookies @ Microsoft 2022
V Add test files directly to your training dataset
. Once you train a custom extraction model, make use of the test page to improve
your model quality by uploading test documents to training dataset if needed.

| Labels | Related fields |
| --- | --- |
| New Belgium Brewery | :selected: CustomerAddressRecipient :selected: CustomerName |


<!---- Page 675 ---------------------------------------------------------------------------------------------------------------------------------->
. If a low confidence score is returned for some labels, make sure your labels are
correct. If not, add them to the training dataset and relabel to improve the model
quality.
Azure Al | Document Intelligence Studio
®
@
?
2
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
×
«
Form Recognizer Studio
Custom extraction model
Hello-Custom
Test
Custom extraction model
Test model test-model v
Upload files
Hello-Custom
Run analysis
Analyze options
Fields
Result
Code
S
Label data
Drag & drop file
here or
Browse for files
or
Fetch from URL
receipt.png
Models
Contoso
DocType: test-model
Test
address
#1
€
Settings
PYN =B
24.80%
123 Main Street Redmond, WA 98052
receipt.png
123 Main Street
Redmond, WA 98052
name
#1
58.10%
Contoso
987-654-3210
6/10/2019 13:59
Sales Associate: Paul
2 mSurface Pro 6
$1,998.00
3 Surface Pen
$299.97
Sub-Total
$2,297.97
Tax
$218.31
Total
$2,516.28
‹
1
of 1 >
Q Q + 2
https://formrecognizer-dogfood.appliedai.azure.com/studio/custommodel/projects/e6166335-40fb-4474-b91e-ebd4ca54a3a7/model-test
Privacy & Cookies @ Microsoft 2022
V Make use of the document list options and filters in custom projects
. Use the custom extraction model labeling page. You can now navigate through
your training documents with ease by making use of the search, filter, and sort by
feature.
. Utilize the grid view to preview documents or use the list view to scroll through the
documents more easily.


<!---- Page 676 ---------------------------------------------------------------------------------------------------------------------------------->
Azure Al | Document Intelligence Studio
៛
@ Azure Form Recognizer is now Azure Al Document Intelligence. Learn more about the latest updates to the service and the Studio experience.
×
oC
Form Recognizer Studio > Custom extraction model > Hello-Custom > Label data
= Custom extraction model
Label data
Train
Hello-Custom
23 Run layout
Auto label
V
D
Draw region
O
0
+
Add a field
0
Label data
Drag & drop file
here or
Browse for files
=
1
address
Contoso Headquarters 123 456th St
New York, NY, 10001
:
1
Models
CONTOSO LTD.
INVOICE
L
Test
= 2
name
CONTOSO LTD.
:
Contost Headquarters
INVOICE: INV-100
@
Settings
123 456 57
INVOICE DATE: 11/15/2019
New York NY. 10001
DUE DATE: 12/15/2019
CUSTOMER NAME: MICROSOFT CORPORATION
SERVICE PERIOD: 10/14/2019 - 11/14/2019
CUSTOMER ID: CID-12345
Microsoft Corp
123 Other St,
Redmond WA, 98052
receipt.png
BILL TO:
Microsoft Finance
123 Bill St,
Redmond WA, 98052
SHIP TO:
Microsoft Delivery
123 Ship St,
Redmond WA, 98052
SERVICE ADDRESS:
Microsoft Services
123 Service St,
Redmond WA, 98052
Invoice_2.pdf
sample pdf
WVOICE
road-barcode.pdf
THANK YOU FOR YOUR BUSINESS!
REMIT TO:
Contoso Billing
123 Remit St
New York, NY, 10001
9
Invoice pdf
< 1 of1 >
Q Q q Q
Privacy & Cookies @ Microsoft 2022
E
SALESPERSON
P.O. NUMBER
REQUISITIONER
SHIPPED VIA
F.O.B. POINT
TERMS
PO-3333
EFI
DATE
ITEM CODE
DESCRIPTION
QTY
UM
PRICE
TAX
AMOUNT
3/4/2021
A123
Consulting Services
2
hours
$30.00
$5.00
$60.00
3/5/2021
B456
Document Fee
3
$10.00
$3.00
$30.00
3/6/2021
C789
Printing Fee
10
pages
$1.00
$1.00
$10.00
SUBTOTAL
$100.00
SALES TAX
$10.00
TOTAL
$110.00
PREVIOUS UNPAID BALANCE
$500.00
AMOUNT DUE
$610.00
V Project sharing
· Share custom extraction projects with ease. For more information, see Project
sharing with custom models.
May 2023
Introducing refreshed documentation for Build 2023
NEW
Document Intelligence Overview enhanced navigation, structured access
points, and enriched images.
NEW
Choose a Document Intelligence model provides guidance for choosing the
best Document Intelligence solution for your projects and workflows.
April 2023
Announcing the latest Document Intelligence client-library public preview release
· Document Intelligence REST API Version 2023-02-28-preview supports the public
preview release client libraries. This release includes the following new features and
capabilities available for .NET/C# (4.1.0-beta-1), Java (4.1.0-beta-1), JavaScript
(4.1.0-beta-1), and Python (3.3.0b.1) client libraries:
o Custom classification model

| E | SALESPERSON | P.O. NUMBER | REQUISITIONER | SHIPPED VIA | F.O.B. POINT | TERMS |
| --- | --- | --- | --- | --- | --- | --- |
|  |  | PO-3333 |  |  |  |  |
|  |  |  |  |  |  |  |

| EFI DATE | ITEM CODE | DESCRIPTION | QTY | UM | PRICE | TAX | AMOUNT |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 3/4/2021 | A123 | Consulting Services | 2 | hours | $30.00 | $5.00 | $60.00 |
| 3/5/2021 | B456 | Document Fee | 3 |  | $10.00 | $3.00 | $30.00 |
| 3/6/2021 | C789 | Printing Fee | 10 | pages | $1.00 | $1.00 | $10.00 |

| SUBTOTAL | $100.00 |
| --- | --- |
| SALES TAX | $10.00 |
| TOTAL | $110.00 |
| PREVIOUS UNPAID BALANCE | $500.00 |
| AMOUNT DUE | $610.00 |


<!---- Page 677 ---------------------------------------------------------------------------------------------------------------------------------->
o Query fields extraction
o Add-on capabilities
· For more information, see Document Intelligence SDK ( .. /public preview) and
March 2023 release notes
March 2023
1 Important
2023-02-28-preview capabilities are currently only available in the following
regions:
· West Europe
· West US2
· East US
· Custom classification model is a new capability within Document Intelligence
starting with the 2023-02-28-preview API.
· Query fields capabilities added to the General Document model, use Azure
OpenAI models to extract specific fields from documents. Try the General
documents with query fields feature using the Document Intelligence Studio2.
Query fields are currently only active for resources in the East us region.
· Add-on capabilities:
o Font extraction is now recognized with the 2023-02-28-preview API.
o Formula extraction is now recognized with the 2023-02-28-preview API.
o High resolution extraction is now recognized with the 2023-02-28-preview API.
· Custom extraction model updates:
o Custom neural model now supports added languages for training and analysis.
Train neural models for Dutch, French, German, Italian, and Spanish.
o Custom template model now has an improved signature detection capability.
· Document Intelligence Studio updates:
o In addition to support for all the new features like classification and query fields,
the Studio now enables project sharing for custom model projects.
o New model additions in gated preview: Vaccination cards, Contracts, US Tax
1098, US Tax 1098-E, and US Tax 1098-T. To request access to gated preview
models, complete and submit the Document Intelligence private preview
request form.
· Receipt model updates:


<!---- Page 678 ---------------------------------------------------------------------------------------------------------------------------------->
o Receipt model adds support for thermal receipts.
· Receipt model now adds language support for 18 languages and three regional
languages (English, French, Portuguese).
o Receipt model now supports TaxDetails extraction.
· Layout model now improves table recognition.
. Read model now adds improvement for single-digit character recognition.
February 2023
. Select Document Intelligence containers for v3.0 are now available for use!
. Currently Read v3.0 and Layout v3.0 containers are available.
For more information, see Install and run Document Intelligence containers.
January 2023
· Prebuilt receipt model - added languages supported. The receipt model now
supports these added languages and locales
· Japanese - Japan (ja-JP)
· French - Canada (fr-CA)
o Dutch - Netherlands (nl-NL)
o English - United Arab Emirates (en-AE)
· Portuguese - Brazil (pt-BR)
· Prebuilt invoice model - added languages supported. The invoice model now
supports these added languages and locales
o English - United States (en-US), Australia (en-AU), Canada (en-CA), United
Kingdom (en-UK), India (en-IN)
o Spanish - Spain (es-ES)
o French - France (fr-FR)
o Italian - Italy (it-IT)
· Portuguese - Portugal (pt-PT)
o Dutch - Netherlands (nl-NL)
· Prebuilt invoice model - added fields recognized. The invoice model now
recognizes these added fields
o Currency code
o Payment options


<!---- Page 679 ---------------------------------------------------------------------------------------------------------------------------------->
o Total discount
o Tax items (en-IN only)
· Prebuilt ID model - added document types supported. The ID model now supports
these added document types
o US Military ID
? Tip
All January 2023 updates are available with REST API version 2022-08-31 (GA).
· Prebuilt receipt model-additional language support:
The prebuilt receipt model adds support for the following languages:
o English - United Arab Emirates (en-AE)
o Dutch - Netherlands (nl-NL)
· French - Canada (fr-CA)
· German - (de-DE)
o Italian - (it-IT)
· Japanese - Japan (ja-JP)
· Portuguese - Brazil (pt-BR)
· Prebuilt invoice model-additional language support and field extractions
The prebuilt invoice model adds support for the following languages:
· English - Australia (en-AU), Canada (en-CA), United Kingdom (en-UK), India (en-
IN)
· Portuguese - Brazil (pt-BR)
The prebuilt invoice model now adds support for the following field extractions:
o Currency code
o Payment options
o Total discount
o Tax items (en-IN only)
. Prebuilt ID document model-additional document types support
The prebuilt ID document model now adds support for the following document
types:
o Driver's license expansion supporting India, Canada, United Kingdom, and
Australia
o US military ID cards and documents
o India ID cards and documents (PAN and Aadhaar)


<!---- Page 680 ---------------------------------------------------------------------------------------------------------------------------------->
o Australia ID cards and documents (photo card, Key-pass ID)
o Canada ID cards and documents (identification card, Maple card)
o United Kingdom ID cards and documents (national/regional identity card)
December 2022
. Document Intelligence Studio updates
The December Document Intelligence Studio release includes the latest updates to
Document Intelligence Studio. There are significant improvements to user
experience, primarily with custom model labeling support.
o Page range. The Studio now supports analyzing specified pages from a
document.
o Custom model labeling:
o Run Layout API automatically. You can opt to run the Layout API for all
documents automatically in your blob storage during the setup process for
custom model.
o Search. The Studio now includes search functionality to locate words within a
document. This improvement allows for easier navigation while labeling.
o Navigation. You can select labels to target labeled words within a document.
o Auto table labeling. After you select the table icon within a document, you
can opt to autolabel the extracted table in the labeling view.
o Label subtypes and second-level subtypes The Studio now supports
subtypes for table columns, table rows, and second-level subtypes for types
such as dates and numbers.
· Building custom neural models is now supported in the US Gov Virginia region.
· Preview API versions 2022-01-30-preview and 2021-09-30-preview will be retired
January 31 2023. Update to the 2022-08-31 API version to avoid any service
disruptions.
November 2022


<!---- Page 681 ---------------------------------------------------------------------------------------------------------------------------------->
· Announcing the latest stable release of Azure Al Document Intelligence libraries
o This release includes important changes and updates for .NET, Java, JavaScript,
and Python client libraries. For more information, see Azure SDK DevBlog.
o The most significant enhancements are the introduction of two new clients, the
DocumentAnalysisClient and the DocumentModelAdministrationClient .
October 2022
. Document Intelligence versioned content
o Document Intelligence documentation is updated to present a versioned
experience. Now, you can choose to view content targeting the v3.0 GA
experience or the v2.1 GA experience. The v3.0 experience is the default.
Microsoft Ignite
Watch live now
October 12-14, 2022
Microsoft
Learn
Documentation
Training
Certifications
Q&A
Code Samples
Shows
Events
Azure Product documentation v Architecture v Learn Azure v Develop v Resources
Version
Learn / Azure / Applied Al Services /
0
:
Form Recognizer v3.0
A
Azure Form Recognizer documentation
v3.0
Azure Form Recognizer is a cloud-based Azure Applied Al Service that uses machine-learning models to extract key-value pairs, text, and
tables from your documents. Form Recognizer analyzes your forms and documents, extracts text and data, maps field relationships as key-
value pairs, and returns a structured JSON output. You quickly get accurate results that are tailored to your specific content without
excessive manual intervention or extensive data science expertise. Use Form Recognizer to automate your data processing in applications
and workflows, enhance data-driven strategies, and enrich document search capabilities.
v2.1
v Overview
What is Azure Form Recognizer?
Form Recognizer Studio
Form Recognizer SDK
Language support
About Azure Form Recognizer
Quickstarts and how-to-
guides
Form Recognizer Studio (v3.0)
Pricing 17
Service limits
OVERVIEW
OVERVIEW
QUICKSTART
What's new
What is Azure Form Recognizer?
Studio overview
Form Recognizer FAQ
Form Recognizer Studio
WHAT'S NEW
Form Recognizer Sample Labeling tool
[6]
GET STARTED
· Document Intelligence Studio Sample Code
o Sample code for the Document Intelligence Studio labeling experience is now
available on GitHub. Customers can develop and integrate Document
Intelligence into their own UX or build their own new UX using the Document
Intelligence Studio sample code.
· Language expansion
o With the latest preview release, Document Intelligence's Read (OCR), Layout,
and Custom template models support 134 new languages. These language
additions include Greek, Latvian, Serbian, Thai, Ukrainian, and Vietnamese, along
with several Latin, and Cyrillic languages. Document Intelligence now has a total
of 299 supported languages across the most recent GA and new preview
versions. Refer to the supported languages pages to see all supported
languages.


<!---- Page 682 ---------------------------------------------------------------------------------------------------------------------------------->
o Use the REST API parameter api-version=2022-06-30-preview when using the
API or the corresponding SDK to support the new languages in your
applications.
. New Prebuilt Contract model
o A new prebuilt that extracts information from contracts such as parties, title,
contract ID, execution date and more. the contracts model is currently in
preview, request access here z.
· Region expansion for training custom neural models
o Training custom neural models now supported in added regions.
V East US
V East US2
V US Gov Arizona
September 2022
1 Note
Starting with version 4.0.0, a new set of clients is introduced to apply the newest
features of the Document Intelligence service.
SDK version 4.0.0 GA release includes the following updates:
C#
· Version 4.0.0 GA (2022-09-08)
· Supports REST API v3.0 and v2.0 clients
Package (NuGet) z
Changelog/Release History [
Migration guide
ReadMe
Samples
· Region expansion for training custom neural models now supported in six new
regions


<!---- Page 683 ---------------------------------------------------------------------------------------------------------------------------------->
V Australia East
V Central US
V East Asia
V France Central
V UK South
V West US2
o For a complete list of regions where training is supported see custom neural
models.
· Document Intelligence SDK version 4.0.0 GA release:
o Document Intelligence client libraries version 4.0.0 (.NET/C#, Java,
JavaScript) and version 3.2.0 (Python) are generally available and ready for
use in production applications !.
o For more information on Document Intelligence client libraries, see the SDK
overview.
· Update your applications using your programming language's migration
guide.
August 2022
Document Intelligence SDK beta August 2022 preview release includes the following
updates:
C#
Version 4.0.0-beta.5 (2022-08-09)
Changelog/Release History
Package (NuGet) z
SDK reference documentation
· Document Intelligence v3.0 generally available
o Document Intelligence REST API v3.0 is now generally available and ready for
use in production applications! Update your applications with REST API
version 2022-08-31.
· Document Intelligence Studio updates


<!---- Page 684 ---------------------------------------------------------------------------------------------------------------------------------->
V Next steps. Under each model page, the Studio now has a next steps section.
Users can quickly reference sample code, troubleshooting guidelines, and
pricing information.
V Custom models. The Studio now includes the ability to reorder labels in
custom model projects to improve labeling efficiency.
V Copy Models Custom models can be copied across Document Intelligence
services from within the Studio. The operation enables the promotion of a
trained model to other environments and regions.
V Delete documents. The Studio now supports deleting documents from
labeled dataset within custom projects.
· Document Intelligence service updates
o prebuilt-read. Read OCR model is now also available in Document Intelligence
with paragraphs and language detection as the two new features. Document
Intelligence Read targets advanced document scenarios aligned with the
broader document intelligence capabilities in Document Intelligence.
o prebuilt-layout. The Layout model extracts paragraphs and whether the
extracted text is a paragraph, title, section heading, footnote, page header, page
footer, or page number.
o prebuilt-invoice. The TotalVAT and Line/VAT fields now resolves to the existing
fields TotalTax and Line/Tax respectively.
o prebuilt-idDocument. Data extraction support for US state ID, social security,
and green cards. Support for passport visa information.
o prebuilt-receipt. Expanded locale support for French (fr-FR), Spanish (es-ES),
Portuguese (pt-PT), Italian (it-IT) and German (de-DE).
o prebuilt-businessCard. Address parse support to extract subfields for address
components like address, city, state, country/region, and zip code.
. Al quality improvements
o prebuilt-read. Enhanced support for single characters, handwritten dates,
amounts, names, other key data commonly found in receipts and invoices and
improved processing of digital PDF documents.
o prebuilt-layout. Support for better detection of cropped tables, borderless
tables, and improved recognition of long spanning cells.
o prebuilt-document. Improved value and check box detection.
o custom-neural. Improved accuracy for table detection and extraction.
June 2022


<!---- Page 685 ---------------------------------------------------------------------------------------------------------------------------------->
· Document Intelligence SDK beta June 2022 preview release includes the following
updates:
C#
Version 4.0.0-beta.4 (2022-06-08)
Changelog/Release History E
Package (NuGet) [
SDK reference documentation
. Document Intelligence Studio June release is the latest update to the Document
Intelligence Studio. There are considerable user experience and accessibility
improvements addressed in this update:
o Code sample for JavaScript and C#. The Studio code tab now adds JavaScript
and C# code samples in addition to the existing Python one.
o New document upload UI. Studio now supports uploading a document with
drag & drop into the new upload user interface.
o New feature for custom projects. Custom projects now support creating
storage account and blobs when configuring the project. In addition, custom
project now supports uploading training files directly within the Studio and
copying the existing custom model.
· Document Intelligence v3.0 2022-06-30-preview release presents extensive
updates across the feature APIs:
o Layout extends structure extraction. Layout now includes added structure
elements including sections, section headers, and paragraphs. This update
enables finer grain document segmentation scenarios. For a complete list of
structure elements identified, see enhanced structure.
o Custom neural model tabular fields support. Custom document models now
support tabular fields. Tabular fields by default are also multi page. To learn
more about tabular fields in custom neural models, see tabular fields.
o Custom template model tabular fields support for cross page tables. Custom
form models now support tabular fields across pages. To learn more about
tabular fields in custom template models, see tabular fields.
o Invoice model output now includes general document key-value pairs. Where
invoices contain required fields beyond the fields included in the prebuilt
model, the general document model supplements the output with key-value
pairs. See key value pairs.


<!---- Page 686 ---------------------------------------------------------------------------------------------------------------------------------->
o Invoice language expansion. The invoice model includes expanded language
support. See supported languages.
o Prebuilt business card now includes Japanese language support. See supported
languages.
o Prebuilt ID document model. The ID document model now extracts
DateOfIssue, Height, Weight, EyeColor, HairColor, and DocumentDiscriminator
from US driver's licenses. See field extraction.
o Read model now supports common Microsoft Office document types.
Document types like Word (docx), Excel (xlsx), and PowerPoint (pptx) are now
supported with the Read API. See Read data extraction.
February 2022
C#
Version 4.0.0-beta.3 (2022-02-10)
Changelog/Release History
Package (NuGet) z
SDK reference documentation
· Document Intelligence v3.0 preview release introduces several new features,
capabilities, and enhancements:
o Custom neural model or custom document model is a new custom model to
extract text and selection marks from structured forms, semi-structured and
unstructured documents.
o W-2 prebuilt model is a new prebuilt model to extract fields from W-2 forms
for tax reporting and income verification scenarios.
o Read API extracts printed text lines, words, text locations, detected languages,
and handwritten text, if detected.
o General document pretrained model is now updated to support selection
marks in addition to API text, tables, structure, and key-value pairs from forms
and documents.
o Invoice API Invoice prebuilt model expands support to Spanish invoices.
o Document Intelligence Studio adds new demos for Read, W2, Hotel receipt
samples, and support for training the new custom neural models.
o Language Expansion Document Intelligence Read, Layout, and Custom Form
add support for 42 new languages including Arabic, Hindi, and other languages


<!---- Page 687 ---------------------------------------------------------------------------------------------------------------------------------->
using Arabic and Devanagari scripts to expand the coverage to 164 languages.
Handwritten language support expands to Japanese and Korean.
· Get started with the new v3.0 preview API.
· Document Intelligence model data extraction:
0
Expand table
Model
Text
extraction
Key-Value
pairs
Selection
Marks
Tables
Signatures
Read
✓
General
document
✓
✓
✓
✓
Layout
✓
✓
✓
Invoice
✓
✓
✓
✓
Receipt
✓
✓
✓
ID document
✓
✓
Business card
✓
✓
Custom template
✓
✓
✓
✓
✓
Custom neural
✓
✓
✓
✓
· Document Intelligence SDK beta preview release includes the following updates:
o Custom Document models and modes:
o Custom template (formerly custom form).
o Custom neural.
o Custom model-build mode.
o W-2 prebuilt model ( .. /prebuilt-tax.us.w2).
o Read prebuilt model ( .. /prebuilt-read).
o Invoice prebuilt model (Spanish) ( .. /prebuilt-invoice).
Feedback

| Model | Text extraction | Key-Value pairs | Selection Marks | Tables Signatures |
| --- | --- | --- | --- | --- |
| Read | ✓ :selected: | :unselected: |  | :unselected: |
| General document | :selected: ✓ | :selected: ✓ | ✓ :selected: | ✓ :selected: |
| Layout | ✓ :selected: |  | ✓ :selected: | ✓ :selected: |
| Invoice | :selected: ✓ | ✓ :selected: | ✓ :selected: | ✓ :selected: |
| Receipt | :selected: ✓ | :selected: ✓ |  | ✓ :selected: |
| ID document | :selected: ✓ | :selected: ✓ |  |  |
| Business card | :selected: ✓ | :selected: ✓ |  |  |
| Custom template | :selected: ✓ | :selected: ✓ | ✓ :selected: | ✓ :selected: :selected: ✓ |
| Custom neural | :selected: ✓ | ✓ :selected: | ✓ :selected: | :selected: ✓ :unselected: |


<!---- Page 688 ---------------------------------------------------------------------------------------------------------------------------------->
Was this page helpful?
Yes
No
Provide product feedback [ | Get help at Microsoft Q&A
